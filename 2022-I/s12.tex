\documentclass[apuntes]{subfiles}

\begin{document}

\section{Producto escalar (punto), norma, interpretación geométrica del producto escalar: proyecciones y ortogonalidad, producto vectorial (cruz)* y triple producto escalar*} \label{Sec:2}

\subsection{Producto escalar} \label{Subsec:Producto_escalar}

Algunos espacios vectoriales están dotados de una operación entre vectores que da como resultado un escalar conocida como \emph{producto escalar}. En estos espacios, dicha operación tiene una gran riqueza geométrica, la cual nos permite tener nociones como ortogonalidad y magnitud \textemdash y generalizarlas a espacios vectoriales más abstractos, como veremos a lo largo de esta sección\footnote{En el último módulo del curso regresaremos a este tipo de espacios para descubrir más secretos del producto escalar.}.

\subsubsection{Definición de producto escalar (punto)} \label{Def:Producto_escalar}

\begin{tcolorbox}
\underline{Def.} Sea $V$ sobre $K$ un espacio vectorial, con $\mathbf{u},\mathbf{v},\mathbf{w}\in V$ y $a\in K$. El \textit{producto escalar} $\langle\cdot\mathop ,\cdot\rangle:V\times V\rightarrow K$ le asocia a cualquier par ordenado de vectores en $V$ un escalar en $K$, y cumple las tres propiedades siguientes:

\begin{tabular}{l}
    \\
    $\langle\mathbf{u}+\mathbf{w},\mathbf{v}\rangle = \langle\mathbf{u},\mathbf{v}\rangle+\langle\mathbf{w},\mathbf{v}\rangle,$ \\ \\ $\langle a\mathbf{u},\mathbf{v}\rangle = a\langle\mathbf{u},\mathbf{v}\rangle,$ \\ \\
    $\langle\mathbf{u},\mathbf{v}\rangle=\overline{\langle\mathbf{v},\mathbf{u}\rangle},$ \\ \\
\end{tabular}

donde la barra $\overline{\langle\mathbf{v},\mathbf{u}\rangle}$ denota el complejo conjugado de $\langle\mathbf{v},\mathbf{u}\rangle$\footnote{Recordemos que, en general, nuestro campo $K$ puede ser complejo; en ese caso, el producto escalar en $V$ le asignará a cada par ordenado de dos vectores de $V$ un escalar complejo $c\in\mathbb{C}$.}. Si, además, se cumple la propiedad

\begin{tabular}{l}
    \\
    $\forall\hspace{1.5mm}\mathbf{v}\in V, \hspace{1.5mm} \langle\mathbf{v},\mathbf{v}\rangle\geq0; \hspace{1.5mm} \langle\mathbf{v},\mathbf{v}\rangle = 0 \iff \mathbf{v}=\mathbf{0}$, \\ \\
\end{tabular}

se dice que el producto escalar es \textit{positivo definido}.

\vspace{3mm}
\hspace{2.5mm} Para vectores que son $n-$tuplas (como aquellos de $\mathbb{R}^n$ o $\mathbb{C}^n$), es común que el producto escalar de $\mathbf{u}$ y $\mathbf{v}$ se denote por $\mathbf{u}\cdot\mathbf{v}$, por lo cual también se le conoce como \textit{producto punto}; sin embargo, también existen notaciones alternativas como $(\mathbf{u}, \mathbf{v})$\footnote{La notación principal que utilizaremos para el producto escalar $\langle \cdot,\cdot\rangle$ es similar a la utilizada en Mecánica Cuántica $\braket{\phi|\psi}$ \textemdash en donde el campo siempre es complejo\textemdash, y esconde un profundo significado, el cual veremos más adelante.}.

\end{tcolorbox}{}

A partir de la definición, observemos lo siguiente:

\begin{itemize}
    \item Las primeras dos propiedades juntas nos dicen que $\langle\mathbf{u}+a\mathbf{w},\mathbf{v}\rangle = \langle\mathbf{u},\mathbf{v}\rangle+a\langle\mathbf{w},\mathbf{v}\rangle$. Para referirnos a esta propiedad específica en lenguaje matemático, decimos que el producto escalar es una operación \textit{lineal\footnote{Como debes sospechar, el concepto de operación \emph{lineal} es fundamental para el álgebra \emph{lineal}, y lo veremos con más detalle en las secciones de transformaciones y operadores lineales.} en la primera entrada}.
    \item La tercera propiedad nos dice que, si el campo es real, el producto escalar es una operación conmutativa, es decir, que si $K=\mathbb{R}\implies\langle\mathbf{u},\mathbf{v}\rangle=\langle\mathbf{v},\mathbf{u}\rangle$. En cambio, si el campo es complejo, en general, el producto escalar es no conmutativo.
    \item Las primeras tres propiedades juntas nos dicen que, si el espacio vectorial está definido sobre un campo complejo, entonces $\langle\mathbf{u},\mathbf{v}+a\mathbf{w}\rangle = \langle\mathbf{u},\mathbf{v}\rangle+\overline{a}\langle\mathbf{u},\mathbf{w}\rangle$, donde $\overline{a}$ denota el complejo conjugado del escalar complejo $a$. Esto nos dice que, si el campo es complejo, el producto escalar es una operación \textit{antilineal\footnote{Este concepto también se verá con mayor detenimiento más adelante.} en la segunda entrada}. En cambio, si el campo es real, entonces el producto escalar es lineal en ambas entradas.
    \item Para un producto escalar positivo definido, el único vector que puede tener como resultado el escalar $0$ al hacer producto escalar consigo mismo es el vector nulo $\mathbf{0}$ (el neutro aditivo que vimos en las secciones \ref{Def:Espacio_vectorial} y \ref{Def:Subespacio_vectorial}).
\end{itemize}


\subsubsection{Ejemplos de producto escalar en espacios vectoriales} \label{Ejem:Producto_escalar}

En $\mathbb{R}^2$ el producto escalar se define como $\begin{pmatrix}u_1\\u_2\end{pmatrix}\cdot\begin{pmatrix}v_1\\v_2\end{pmatrix} \equiv u_1v_1+u_2v_2$, donde hemos utilizado la notación de punto, ya que los vectores son 2-tuplas. En general, en el espacio vectorial real $\mathbb{R}^n$ el producto escalar se define como

$$\mathbf{u}\cdot\mathbf{v} = \begin{pmatrix}u_1&u_2&...&u_n\end{pmatrix}\cdot\begin{pmatrix}v_1&v_2&...&v_n\end{pmatrix} \equiv u_1v_1+u_2v_2+...+u_nv_n=\sum_{i=1}^n u_i v_i\footnote{Para simplificar la notación, en varias áreas de la física se elimina el signo de suma ($\Sigma$) cuando aparecen índices repetidos, siguiendo la convención de que éstos implícitamente indican una suma sobre el índice. Así, $\sum_{i=1}^n u_i v_i$ se puede escribir simplemente como $u_iv_i$. A esto se le conoce como \textit{notación de Einstein} o \textit{convención de suma de Einstein}.}.$$

Para cumplir \emph{todas} las propiedades descritas en la sección \ref{Def:Producto_escalar}, el producto escalar en el espacio vectorial complejo $\mathbb{C}^n$ se debe definir de una forma ligeramente distinta. Sean $\mathbf{a},\mathbf{b}\in\mathbb{C}^n$ vectores con $n$ entradas complejas, entonces el producto escalar se define como

$$\langle\mathbf{a},\mathbf{b}\rangle\equiv\mathbf{a}\cdot\mathbf{b}\equiv \sum_{i=1}^n a_i \overline{b_i},$$

\noindent es decir, se realiza un producto entre la $i$-ésima entrada de $\mathbf{a}$ y el \emph{complejo conjugado} de la $i$-ésima entrada de $\mathbf{b}$ para cada $i$, y luego se suman dichos productos.

\vspace{3mm}

Sea $C^0([a,b])$ el conjunto de todas las funciones de variable real continuas en el intervalo cerrado $[a,b]$ con integral finita, entonces podemos definir un producto escalar en el espacio vectorial del conjunto $C^0([a,b])$ sobre el campo $\mathbb{R}$ como

$$\langle f,g\rangle = \int_{a}^{b} f(x)g(x)dx.$$

\vspace{3mm}

Observemos que, como muestra el último ejemplo, se puede definir un producto escalar en muchos tipos de espacios vectoriales diferentes, y no sólo en aquellos que tienen como vectores a $n-$tuplas\footnote{Veremos otros tipos de espacios vectoriales donde se pueden definir productos escalares más adelante.}. Nótese además que, en cada caso, el resultado del producto escalar es un escalar del campo sobre el cual está definido el espacio vectorial.

\vspace{3mm}

Para ver más ejemplos de productos escalares pueden revisar \emph{Linear Algebra} de Friedberg (págs. 330-331), \textit{Linear Algebra: A Modern Introduction} de Poole\footnote{Ten en cuenta que algunos libros introducen el producto escalar (al cual pueden llamar producto punto) únicamente con espacios vectoriales reales, y lo generalizan a espacios vectoriales complejos en secciones posteriores.} (págs. 531-534), etc.


\subsubsection{Propiedades del producto escalar} \label{Prop:Producto_escalar}

Como resumen, a partir de la definición dada en la sección \ref{Def:Producto_escalar}, podemos ver que las principales propiedades del producto escalar son:

\begin{center}
    \begin{tabular}{lr}
        $\langle\mathbf{u},\mathbf{v}\rangle = \overline{\langle\mathbf{v},\mathbf{u}\rangle}$ & Conmutar vectores resulta en la conjugación del escalar \\
        $\langle a\mathbf{u}+\mathbf{v},\mathbf{w}\rangle = a\langle\mathbf{u},\mathbf{w}\rangle + \langle\mathbf{v},\mathbf{w}\rangle$ & Linealidad en la primera entrada\\
        $\langle\mathbf{u},\mathbf{w}+b\mathbf{z}\rangle = \langle\mathbf{u},\mathbf{w}\rangle + \overline{b}\langle\mathbf{u},\mathbf{z}\rangle$ & Antilinealidad en la segunda entrada\\
    \end{tabular}{}
\end{center}{}

En particular, el producto escalar definido \underline{sobre un espacio vectorial real} es una operación lineal en ambas entradas (o \emph{bilineal}), es decir, que $$\langle a_1\mathbf{u_1}+...+a_n\mathbf{u_n},\mathbf{v}\rangle=a_1\langle\mathbf{u_1},\mathbf{v}\rangle+...+a_n\langle\mathbf{u_n},\mathbf{v}\rangle\hspace{1mm}$$ \noindent y $$\langle\mathbf{u},b_1\mathbf{v_1}+...+b_n\mathbf{v_n}\rangle=b_1\langle\mathbf{u},\mathbf{v_1}\rangle+...+b_n\langle\mathbf{u},\mathbf{v_n}\rangle.$$

\newpage
\subsection{Norma} \label{Subsec:Norma}

Otro tipo de operación que se puede definir sobre un espacio vectorial es la \emph{norma}, que toma un sólo vector del espacio y devuelve un escalar del campo. La norma, en general, dota a los vectores de un espacio de un cierto sentido de \emph{magnitud} mediante el cual se puede comparar a distintos vectores. Como veremos en esta sección, si un espacio vectorial tiene un producto escalar positivo definido (dotando al espacio de las nociones de proyecciones y ortogonalidad que ya vimos en la sec. \ref{Subsec:Interpretación geométrica del producto escalar}), entonces se puede definir una norma sobre el espacio a partir de dicho producto escalar; sin embargo, no es necesario que una norma provenga de un producto escalar, y es posible que existan normas (y, por tanto, ciertas nociones de magnitud) en espacios vectoriales \emph{sin} producto escalar.

\subsubsection{Definición de norma} \label{Def:Norma}

\begin{tcolorbox}
\underline{Def.} Una \textit{norma} es una operación $||\cdot||:V\rightarrow K$ que toma sólo un vector y devuelve un escalar, y que cumple las siguientes propiedades:

\begin{center}
    \begin{tabular}{lr}
        $||\mathbf{u}+\mathbf{v}|| \leq ||\mathbf{u}|| + ||\mathbf{v}||$ & Satisface la desigualdad del triángulo \\ \\
        $||a\mathbf{u}|| = |a|\hspace{0.5mm}||\mathbf{u}||$ & Es escalable de forma absoluta \\ \\
        $||\mathbf{u}||=0\iff \mathbf{u}=\mathbf{0}$ & Distingue al vector nulo.
    \end{tabular}
\end{center}

\end{tcolorbox}{}

\vspace{3mm}
\noindent A partir de la definición anterior podemos demostrar que la norma es positivo definida; es decir que, además de que el único vector con norma igual a $0$ es el vector nulo, todos los vectores no nulos tienen norma positiva.

\begin{corolario} {2.2.1.1}
    Sea $V$ un espacio vectorial con una norma $||\cdot ||$, entonces $\hspace{1.5mm}\forall\hspace{1.5mm} \mathbf{v}\in V$ con $\mathbf{v}\neq\mathbf{0}$ se cumple que $||\mathbf{v}||>0.$

\begin{proof}
    Por definición de norma tenemos que $||\mathbf{0}||=0$. Sea $\mathbf{v}\in V$ tal que $\mathbf{v}\neq\mathbf{0};$ ya que la norma distingue al vector nulo, entonces $||\mathbf{v}||\neq 0.$ Por definición de espacio vectorial, $\exists\hspace{1mm} \mathbf{-v}\in V$ tal que $\mathbf{v}+(-\mathbf{v})=\mathbf{0}\implies ||\mathbf{v}+(-\mathbf{v})||=||\mathbf{0}||=0$. Ya que la norma cumple la desigualdad del triángulo por definición, tenemos que $$0=||\mathbf{0}||=||\mathbf{v}+(-\mathbf{v})||\leq ||\mathbf{v}||+||\mathbf{-v}||\implies 0\leq ||\mathbf{v}||+||\mathbf{-v}||.$$

\noindent Además, por definición, la norma es escalable de forma absoluta, por lo cual $$0\leq ||\mathbf{v}||+||-\mathbf{v}||=||\mathbf{v}||+|-1|\hspace{0.5mm} ||\mathbf{v}||=||\mathbf{v}||+||\mathbf{v}||=2||\mathbf{v}||.$$

\noindent Ya que $||\mathbf{v}||\neq 0$, el resultado anterior $2||\mathbf{v}||\geq 0\implies ||\mathbf{v}||>0,$ como se quería demostrar.

\end{proof}

\end{corolario}

\subsubsection{Ejemplos de norma}

Tanto en el espacio vectorial real $\mathbb{R}$ como en el espacio vectorial complejo $\mathbb{C}$ se puede definir una norma como $$||x|| = |x|$$

\noindent para cualquier vector $x$ de dichos espacios, ya que el valor absoluto cumple trivialmente las propiedades de la sección \ref{Def:Norma} (lo cual quizá demostraste en tu curso de Cálculo I, por lo menos en el caso real). Como recordatorio, las definiciones del valor absoluto son $$|r| = +\sqrt{r^2} \hspace{3mm}\forall\hspace{0.5mm}r\in\mathbb{
R}; \hspace{3mm} |c| = +\sqrt{c\hspace{0.5mm}\overline{c}}\hspace{3mm}\forall\hspace{0.5mm}c\in\mathbb{C}.$$

\noindent A ésta se le conoce como la \emph{norma del valor absoluto}\footnote{En el caso de los números reales, el valor absoluto equivale a cambiar los signos de los números negativos por signos positivos y no hacerle nada a los número no negativos.}. En el caso real, esta norma se interpreta geométricamente como la distancia entre el origen de la recta real y el punto correspondiente al valor $r$ o, equivalentemente, como la longitud de la flecha que tiene cola en $0$ y punta en $r$; en el caso complejo, se interpreta como la distancia euclideana entre el origen del plano complejo y el punto correspondiente al valor complejo $c$ o, equivalentemente, como la longitud de la flecha que tiene cola en el origen del plano complejo y punta en $c$. 

\vspace{3mm}

Para los vectores que son $n-$tuplas, la norma más comunmente utilizada se define como $$||\mathbf{u}|| = +\sqrt{\langle\mathbf{u},\mathbf{u}\rangle} = +\sqrt{\mathbf{u}\cdot\mathbf{u}},$$ \noindent de acuerdo a las definiciones de productos escalares para $n-$tuplas dadas en la sección \ref{Ejem:Producto_escalar}. Esta norma se interpreta geométricamente como la longitud de la flecha que tiene cola en el origen del espacio vectorial y punta en la coordenada dada por las entradas del vector $\mathbf{u}$, lo cual es equivalente a la distancia euclideana entre estos dos puntos. Por ende, a esta norma se le conoce como \emph{norma euclideana}\footnote{Observemos que la norma del valor absoluto es simplemente un caso particular de la norma euclideana para los espacios vectoriales $\mathbb{R}$ y $\mathbb{C}$, tanto en su definición algebráica como en su interpretación geométrica.}. En particular, a los espacios vectoriales reales $\mathbb{R}^n$ con esta norma se les conoce como \emph{espacios vectoriales euclideanos} (o \emph{euclídeos})\footnote{Estos son los espacios vectoriales básicos que se utilizan en geometría analítica y cálculo diferencial e integral con funciones de una o más variables reales.}. Cuando la norma está asociada geométricamente a la longitud de la flecha que representa un vector, también se dice que está relacionada con la \emph{magnitud} de ese vector.

\vspace{3mm}

También, en un espacio vectorial real de funciones reales con integral finita en el intervalo $[a,b]$, podemos definir una norma a partir del producto escalar como $$||f|| = +\sqrt{\langle f,f\rangle} = +\sqrt{\int_a^b f(x)f(x) dx},$$ \noindent siguiendo el último ejemplo de la sección \ref{Ejem:Producto_escalar}. Es fácil demostrar que esta definición cumple con las propiedades de norma (ver sec. \ref{Def:Norma}; se siguen de las propiedades de la integral vistas en Cálculo II). 

\subsubsection{Desigualdad de Cauchy-Schwarz} \label{Teo:Cauchy-Schwarz} 

Como mencionamos al inicio de esta sección, el proceso anterior de obtener una norma a partir de un producto escalar se puede generalizar para cualquier espacio vectorial con un producto escalar positivo definido, sin importar si dichos espacios vectoriales son reales o complejos. En estos casos, se dice que la norma es \emph{inducida} por un producto escalar positivo definido. Las normas de este tipo serán las más recurrentes durante este curso. El primer paso necesario para demostrar este resultado general es la llamada \emph{desigualdad de Cauchy-Schwarz}, que enunciamos a continuación como teorema. El resto de la demostración se encuentra en los ejercicios de repaso al final de esta sección (ver sec. \ref{Ejer:Norma}).

\begin{teorema} {2.2.3.1} 

    Sea $\langle.\hspace{0.5mm},.\rangle:V\times V\rightarrow K$ un producto escalar positivo definido y sea $||\cdot ||:V\rightarrow K$ una función definida tal que $||\cdot||=+\sqrt{\langle\cdot,\cdot\rangle}$\footnote{Con la ayuda de este teorema, se puede demostrar que esta función $||\cdot ||$ cumple con todas las propiedades de una norma, dadas en la sección \ref{Def:Norma}. Ésta es la llamada \emph{norma euclideana generalizada}.}, entonces para todo $\mathbf{u},\mathbf{v}\in V$ se cumple que $|\langle\mathbf{u},\mathbf{v}\rangle|\leq ||\mathbf{u}||\hspace{0.5mm}||\mathbf{v}||$.

\begin{proof}
    Sean $\mathbf{u},\mathbf{v}\in V.$ Si $\mathbf{v}$ o $\mathbf{u}$ (o ambos) son iguales al vector nulo, la demostración se cumple trivialmente (ver el primer ejercicio de la subsec. \ref{Ejer:Producto_escalar}). Supongamos que $\mathbf{u},\mathbf{v}\neq\mathbf{0}$. 

    Sabemos que para cualquier $a\in K$ se cumple que $$0\leq ||\mathbf{u}-a\mathbf{v}||^2 = \langle\mathbf{u}-a\mathbf{v},\mathbf{u}-a\mathbf{v}\rangle= \langle\mathbf{u},\mathbf{u}-a\mathbf{v}\rangle-a\langle\mathbf{v},\mathbf{u}-a\mathbf{v}\rangle=$$ $$\langle\mathbf{u},\mathbf{u}\rangle-\overline{a}\langle\mathbf{u},\mathbf{v}\rangle-a(\langle\mathbf{v},\mathbf{u}\rangle-\overline{a}\langle\mathbf{v},\mathbf{v})\rangle=\langle\mathbf{u},\mathbf{u}\rangle-\overline{a}\langle\mathbf{u},\mathbf{v}\rangle-a\langle\mathbf{v},\mathbf{u}\rangle+a\overline{a}\langle\mathbf{v},\mathbf{v}\rangle.$$ \noindent En particular, tomando $a=\frac{\langle\mathbf{u},\mathbf{v}\rangle}{\langle\mathbf{v},\mathbf{v}\rangle}$ y sustituyendo en la desigualdad anterior, obtenemos que $$0\leq\langle\mathbf{u},\mathbf{u}\rangle-\frac{\langle\mathbf{v},\mathbf{u}\rangle}{\langle\mathbf{v},\mathbf{v}\rangle}\langle\mathbf{u},\mathbf{v}\rangle-\frac{\langle\mathbf{u},\mathbf{v}\rangle}{\langle\mathbf{v},\mathbf{v}\rangle}\langle\mathbf{v},\mathbf{u}\rangle+\frac{\langle\mathbf{u},\mathbf{v}\rangle}{\langle\mathbf{v},\mathbf{v}\rangle}\frac{\langle\mathbf{v},\mathbf{u}\rangle}{\langle\mathbf{v},\mathbf{v}\rangle}\langle\mathbf{v},\mathbf{v}\rangle=$$ $$\langle\mathbf{u},\mathbf{u}\rangle-2\frac{\langle\mathbf{u},\mathbf{v}\rangle\langle\mathbf{v},\mathbf{u}\rangle}{\langle\mathbf{v},\mathbf{v}\rangle}+\frac{\langle\mathbf{u},\mathbf{v}\rangle\langle\mathbf{v},\mathbf{u}\rangle}{\langle\mathbf{v},\mathbf{v}\rangle}=||\mathbf{u}||^2-\frac{|\langle\mathbf{u},\mathbf{v}\rangle|^2}{||\mathbf{v}||^2}\implies$$ $$0\leq ||\mathbf{u}||^2-\frac{|\langle\mathbf{u},\mathbf{v}\rangle|^2}{||\mathbf{v}||^2}\implies \frac{|\langle\mathbf{u},\mathbf{v}\rangle|^2}{||\mathbf{v}||^2}\leq ||\mathbf{u}||^2\implies|\langle\mathbf{u},\mathbf{v}\rangle|^2\leq ||\mathbf{u}||^2\hspace{0.5mm}||\mathbf{v}||^2\implies|\langle\mathbf{u},\mathbf{v}\rangle|\leq ||\mathbf{u}||\hspace{0.5mm}||\mathbf{v}||.$$

\end{proof}
\end{teorema}

\newpage
\subsection{Interpretación geométrica del producto escalar: proyecciones y ortogonalidad}\label{Subsec:Interpretación geométrica del producto escalar} 

\subsubsection{Producto escalar y proyecciones} \label{Subsec:Producto_escalar_y_proyecciones}

Como quizás aprendiste en tus cursos de Geometría Analítica y/o Mecánica Vectorial, el producto escalar entre vectores de $\mathbb{R}^2$ tiene una definición geométrica dada por $$\langle\mathbf{u},\mathbf{v}\rangle\equiv ||\mathbf{u}||\hspace{0.5mm}||\mathbf{v}||\cos{\theta}=||\mathbf{v}||\hspace{0.5mm} ||\mathbf{u}||\cos{\theta},$$ \noindent donde $\theta$ es el ángulo positivo más chico entre $\mathbf{u}$ y $\mathbf{v}$; esta definición es equivalente a la algebráica dada al principio de la sección \ref{Ejem:Producto_escalar}. El producto $||\mathbf{u}||\cos{\theta}$ es igual a la \emph{proyección escalar} del vector $\mathbf{u}$ sobre el vector $\mathbf{v}$. Si escribimos dicha proyección escalar como $P_{\mathbf{v}}(\mathbf{u})$, tenemos que

$$\langle\mathbf{u},\mathbf{v}\rangle=||\mathbf{v}||P_{\mathbf{v}}(\mathbf{u}) \implies P_{\mathbf{v}}(\mathbf{u})=\frac{\langle\mathbf{u},\mathbf{v}\rangle}{||\mathbf{v}||}.$$ 

\noindent Este es el escalar por el cual habría que multiplicar un vector de norma $1$ en la dirección y sentido de $\mathbf{v}$ para obtener el vector componente de $\mathbf{u}$ colineal a $\mathbf{v}$, también conocido como \emph{proyección vectorial}. Empleando esta notación, la proyección vectorial de $\mathbf{u}$ sobre $\mathbf{v}$ se puede escribir como $P_{\mathbf{v}}(\mathbf{u})\frac{\mathbf{v}}{||\mathbf{v}||} .$ En la Figura \ref{fig:Proyecciones_y_ortogonalidad} se muestran ejemplos de proyecciones vectoriales entre vectores de $\mathbb{R}^2$.

\begin{figure}[h!]
    \begin{subfigure}[b]{0.3\textwidth}
    \centering
    \begin{tikzpicture}[thick,scale=1, every node/.style={scale=1}]
        \draw[thick,<->] (0,0) -- (4,0);
        \draw[thick,<->] (0,0) -- (0,4);
        \draw[step=1cm,gray,very thin,dashed] (0,0) grid (3.9,3.9);
        \foreach \x in {1,2,3}
            \draw (\x cm, 1pt) -- (\x cm, -1pt) node[anchor=north] {$\x$};
        \foreach \y in {1,2,3}
            \draw (1pt, \y cm) -- (-1pt, \y cm) node[anchor=east] {$\y$};
        \draw[ROJO,very thick,->] (0,0) -- (1.5,2.5) node[left, xshift=-0.2cm] {$\mathbf{u}$};
        \draw[ROJO,dashed, thin] (0,0) -- (2.4,4);
        \draw[AZUL,very thick,->] (0,0) -- (3.5,2) node[right, xshift=0.1cm] {$\mathbf{v}$};
        \draw[magenta,->] (0,0) -- (1.809,3.015) node[left, xshift=0.05cm, yshift=0.3cm] {$P_{\mathbf{u}}(\mathbf{v})\frac{\mathbf{u}}{||\mathbf{u}||}$};
        \draw[magenta,dotted,thin] (1.809,3.015) -- (3.5,2);
    \end{tikzpicture}
    \caption{}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0,3\textwidth}
    \centering
    \begin{tikzpicture}[thick,scale=1, every node/.style={scale=1}]
        \draw[thick,<->] (0,0) -- (4,0);
        \draw[thick,<->] (0,0) -- (0,4);
        \draw[step=1cm,gray,very thin,dashed] (0,0) grid (3.9,3.9);
        \foreach \x in {1,2,3}
            \draw (\x cm, 1pt) -- (\x cm, -1pt) node[anchor=north] {$\x$};
        \foreach \y in {1,2,3}
            \draw (1pt, \y cm) -- (-1pt, \y cm) node[anchor=east] {$\y$};
        \draw[ROJO,very thick,->] (0,0) -- (1.5,2.5) node[left, xshift=-0.2cm] {$\mathbf{u}$};
        \draw[ROJO,->] (0,0) -- (0.7059,1.1765) node[left,xshift=0.1cm,yshift=0.3cm] {$\frac{\mathbf{u}}{||\mathbf{u}||}$};
        \draw[AZUL,very thick,->] (0,0) -- (3.5,2) node[right, xshift=0.1cm] {$\mathbf{v}$};
        \draw[AZUL,->] (0,0) -- (1.1846,0.677) node[right,xshift=-0.2cm,yshift=-0.3cm] {$\frac{\mathbf{v}}{||\mathbf{v}||}$};
    \end{tikzpicture}
    \caption{}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0,3\textwidth}
    \centering
    \begin{tikzpicture}[thick,scale=1, every node/.style={scale=1}]
        \draw[thick,<->] (0,0) -- (4,0);
        \draw[thick,<->] (0,0) -- (0,4);
        \draw[step=1cm,gray,very thin,dashed] (0,0) grid (3.9,3.9);
        \foreach \x in {1,2,3}
            \draw (\x cm, 1pt) -- (\x cm, -1pt) node[anchor=north] {$\x$};
        \foreach \y in {1,2,3}
            \draw (1pt, \y cm) -- (-1pt, \y cm) node[anchor=east] {$\y$};
        \draw[ROJO,->,very thick] (0,0) -- (1.5,2.5) node[left, xshift=-0.2cm] {$\mathbf{u}$};
        \draw[AZUL,->,very thick] (0,0) -- (3.5,2) node[right, xshift=0.1cm] {$\mathbf{v}$};
        \draw[magenta,->] (0,0) -- (2.2077,1.2615) node[right, xshift=-0.15cm, yshift=-0.3cm] {$P_{\mathbf{v}}(\mathbf{u})\frac{\mathbf{v}}{||\mathbf{v}||}$};
        \draw[magenta,dotted,thin] (1.5,2.5) -- (2.2077,1.2615);
    \end{tikzpicture}
    \caption{}
    \end{subfigure}
    \caption{Proyecciones vectoriales entre dos vectores $\mathbf{u}$ y $\mathbf{v}$ de $\mathbb{R}^2$. En la subfigura (a) tenemos la proyección vectorial del vector $\mathbf{v}$ sobre el vector $\mathbf{u}$, es decir, la componente de $\mathbf{v}$ en el eje en el cual se encuentra $\mathbf{u}$. En (b) se muestran los vectores de norma $1$ que tienen la misma dirección y sentido que $\mathbf{u}$ y $\mathbf{v}$, respectivamente. En (c) se muestra la proyeccíon vectorial de $\mathbf{u}$ sobre $\mathbf{v}$. Nótese en cada caso que la proyección vectorial se obtiene reescalando el vector sobre el que se proyecta para que tenga norma $1$ y después multiplicando dicho vector por la proyección escalar correspondiente.}
    \label{fig:Proyecciones_y_ortogonalidad}
\end{figure}

Observemos que, ya que tanto $\cos\theta$ como las normas $||\mathbf{u}||$ y $||\mathbf{v}||$ son números reales, entonces $$\langle\mathbf{u},\mathbf{v}\rangle=||\mathbf{v}||P_{\mathbf{v}}(\mathbf{u})=||\mathbf{v}||\hspace{0.5mm}||\mathbf{u}||\cos\theta = ||\mathbf{u}||\hspace{0.5mm}||\mathbf{v}||\cos\theta=||\mathbf{u}||P_{\mathbf{u}}(\mathbf{v})=\langle\mathbf{v},\mathbf{u}\rangle.$$\noindent Esto no es más que la propiedad conmutativa del producto escalar en un espacio vectorial real \textemdash visto desde un punto de vista geométrico.

Ahora imaginemos que tenemos dos vectores $\mathbf{u},\mathbf{v}\in\mathbb{R}^2$ de la misma magnitud (i.e., $||\mathbf{u}||=||\mathbf{v}||$) trazados en el plano cartesiano. En este caso, la bisectriz del ángulo que los separa traza un eje de simetría. Si trazamos la proyección de cada uno de los vectores sobre el otro, veremos de forma más intuitiva que $$||\mathbf{u}||P_{\mathbf{u}}(\mathbf{v})=||\mathbf{v}||P_{\mathbf{v}}(\mathbf{u}),$$ con lo cual reafirmamos que el producto escalar conmuta. Supongamos ahora que reescalamos alguno de los vectores, digamos $\mathbf{u}$, al doble de su longitud \textemdash efectivamente duplicando así su norma\textemdash\hspace{0.5mm}, y llamamos a este nuevo vector $\mathbf{u}'$. Entonces, algebráicamente vemos que $$\langle\mathbf{u}',\mathbf{v}\rangle=||\mathbf{u'}||\hspace{0.5mm}||\mathbf{v}||\cos\theta=||2\mathbf{u}||\hspace{0.5mm}||\mathbf{v}||\cos\theta=2||\mathbf{u}||\hspace{0.5mm}||\mathbf{v}||\cos{\theta}=2||\mathbf{u}||P_{\mathbf{u}}(\mathbf{v}).$$ \noindent Geométricamente, podemos ver esto como que $\langle\mathbf{u}',\mathbf{v}\rangle=(2||\mathbf{u}||)P_{\mathbf{u}}(\mathbf{v})$ o, equivalentemente, que $\langle\mathbf{u}',\mathbf{v}\rangle=||\mathbf{u}||(2P_{\mathbf{u}}(\mathbf{v}))\footnote{Este mismo argumento se expone de forma animada en el siguiente video: \url{https://www.youtube.com/watch?v=LyGKycYT2v0&list=PL_w8oSr1JpVCZ5pKXHKz6PkjGCbPbSBYv&index=11&t=255s}.}.$ Observemos que la elección del vector reescalado fue arbitraria \textemdash de haber reescalado $\mathbf{v}$ por $2$ en vez de $\mathbf{u}$, hubiéramos obtenido el mismo resultado. La discusión anterior es simplemente la interpretación geométrica del hecho de que $\langle2\mathbf{u},\mathbf{v}\rangle=2\langle\mathbf{u},\mathbf{v}\rangle=\langle\mathbf{u},2\mathbf{v}\rangle$ en espacios vectoriales reales.

Similarmente, la interpretación geométrica del hecho de que $\langle\mathbf{u}+\mathbf{v},\mathbf{w}\rangle=\langle\mathbf{u},\mathbf{w}\rangle+\langle\mathbf{v},\mathbf{w}\rangle$ ó $\langle\mathbf{w},\mathbf{u}+\mathbf{v}\rangle=\langle\mathbf{w},\mathbf{u}\rangle+\langle\mathbf{w},\mathbf{v}\rangle$ se puede ver directamente de la Ley del paralelogramo. 

\vspace{3mm}

Sin embargo, al generalizar esta operación a cualquier tipo de espacios vectoriales nos encontramos con un problema: en general, el producto escalar no es conmutativo, por lo cual debemos decidir de qué manera precisa definiremos a esta operación. Por razones que veremos más adelante, la definición que resulta más conveniente es la siguiente:

\begin{tcolorbox}
    \underline{Def.} Sean $\mathbf{u},\mathbf{v}$ vectores de un espacio vectorial $V$ arbitrario tal que $\mathbf{v}$ es distinto al vector nulo. Definimos a la operación $P_{\mathbf{v}}(\hspace{0.5mm}\cdot\hspace{0.5mm} ):V\to K$, conocida como la \emph{proyección escalar del vector} $\mathbf{u}$ \emph{sobre el vector} $\mathbf{v}$, como \[
        P_{\mathbf{v}}(\mathbf{u}) \equiv \frac{\langle\mathbf{u},\mathbf{v}\rangle}{||\mathbf{v}||}
    .\] Además, definimos la \emph{proyección vectorial} de $\mathbf{u}$ sobre $\mathbf{v}$ como $P_{\mathbf{v}}(\mathbf{u}) \frac{\mathbf{v}}{||\mathbf{v}||}$.
 
\end{tcolorbox}

\subsubsection{Ortogonalidad}

Por otro lado, ya que $\cos(\frac{\pi}{2})=0$, podemos ver directamente que, si dos vectores en $\mathbb{R}^2$ son perpendiculares entre sí en el plano, su producto escalar será cero. Esto significaría que la proyección de cualquiera de estos vectores sobre el otro sería nula \textemdash lo cual, en el plano cartesiano, es sinónimo de que los vectores sean perpendiculares\textemdash \hspace{0.5mm}; más generalmente, a esta propiedad se le conoce como \emph{ortogonalidad}, y siempre se asocia con un producto escalar nulo por razones que veremos más adelante.

\vspace{2mm}

\begin{tcolorbox} \label{Def:Vectores_ortogonales_y_subconjunto_ortogonal}
    \underline{Def.} Se dice que dos vectores $\mathbf{u}, \mathbf{v}\in V$ son \emph{ortogonales} si su producto escalar es nulo (i.e., si la proyección de cualquiera de los vectores sobre el otro es cero). Matemáticamente, esto se escribe como $\mathbf{u}\perp\mathbf{v}\iff\langle\mathbf{u},\mathbf{v}\rangle=0$ \hspace{0.5mm} ó, equivalentemente, $\mathbf{u}\perp\mathbf{v}\iff P_{\mathbf{u}}(\mathbf{v})=0=P_{\mathbf{v}}(\mathbf{u})$\footnote{Observemos que esta definición también aplica para espacios vectoriales complejos, ya que $\overline{0}=0$.}.

    \vspace{3mm} 

    \underline{Def.} Se dice que un subconjunto $S\subset V$ es \emph{ortogonal} si cualquier vector de $S$ es ortogonal al resto de los vectores del conjunto, es decir, si $\forall\hspace{1.5mm} \mathbf{u}_i, \mathbf{u}_j\in S, \mathbf{u}_i\neq\mathbf{u}_j\implies \langle\mathbf{u}_i,\mathbf{u}_j\rangle=0.$
\end{tcolorbox}

Por ejemplo, en $\mathbb{R}^3, \begin{pmatrix} 2 & 3 & 1 \end{pmatrix}\perp\begin{pmatrix} -4 & 2 & 2 \end{pmatrix}$, ó $\begin{pmatrix} 2 & 3 & 1 \end{pmatrix}\perp\begin{pmatrix} -8 & 4 & 4 \end{pmatrix}$; además, el subconjunto $\{\begin{pmatrix} 5  & 0 & 0 \end{pmatrix}, \begin{pmatrix} 0 & 7 & 0 \end{pmatrix}, \begin{pmatrix} 0 & 0 & -4 \end{pmatrix}\}$ es ortogonal. En cambio, en el espacio de funciones polinomiales reales $P^2$ restringido al intervalo $[-1,1]$, donde definimos el producto escalar como $\langle f,g\rangle\equiv \int_{-1}^{1}f(x)g(x)\hspace{1mm} dx$, el conjunto $\{1,x,\frac{1}{2}(3x^2-1)\}$ es ortogonal (¿podrás comprobarlo?).

\subsubsection{Vectores unitarios}

Nótese por la definición dada en \ref{Def:Vectores_ortogonales_y_subconjunto_ortogonal} y las propiedades del producto escalar que, si dos vectores son ortogonales, entonces reescalar cualquiera de ellos no afecta a su ortogonalidad. Además, notemos que la definición geométrica del producto escalar para vectores en $\mathbb{R}^2$ se simplifica cuando la norma de los vectores es uno. De hecho, como veremos más adelante, este tipo de vectores en general pueden simplificar muchas expresiones de operaciones realizadas en espacios vectoriales, por lo cual les damos un nombre especial.

\vspace{2mm}

\begin{tcolorbox} \label{Def:Vector_unitario_y_subconjunto_ortonormal} 
    \underline{Def.} Decimos que un vector $\mathbf{v}\in V$ es \emph{unitario} cuando su norma es igual a uno, i.e., si y sólo si $||\mathbf{v}||=1.$ Los vectores unitarios suelen denotarse con una cuña encima de la letra, e.g. $\hat{i}, \hat{j}, \hat{k},$ etc. 
    
    \vspace{3mm}
    
    \underline{Def.} Decimos que un subconjunto $S\subset V$ es \emph{ortonormal} si es un conjunto ortogonal donde todos los vectores son unitarios. Es decir, $S$ es \emph{ortonormal} si y sólo si $\langle\mathbf{v}_i,\mathbf{v}_j\rangle=\delta_{ij}\hspace{3mm} \forall\hspace{1.5mm} \mathbf{v}_i,\mathbf{v}_j\in S,$ donde $\delta_{ij}$ es la delta de Kronecker de dos entradas.
\end{tcolorbox}

Por ejemplo, el vector $\begin{pmatrix} \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} \end{pmatrix}$ es unitario en $\mathbb{R}^2 (\text{ó}\hspace{1mm} \mathbb{C}^2)$, mientras que los vectores $\begin{pmatrix} \frac{-1}{\sqrt{2}} & \frac{i}{\sqrt{2}} \end{pmatrix}$ y $\begin{pmatrix} \frac{-i}{\sqrt{2}} & \frac{1}{\sqrt{2}} \end{pmatrix}$ son unitarios en $\mathbb{C}^2.$

Por último, notemos que si $\mathbf{v}\in V$ es un vector arbitrario no unitario distinto al vector nulo, entonces el vector $$\hat{\mathbf{v}}=\frac{\mathbf{v}}{||\mathbf{v}||}$$ \emph{es} unitario, i.e., tiene norma igual a uno. Por ello, a este proceso de multiplicar un vector no nulo y no unitario por el inverso multiplicativo de su norma se le conoce como \emph{normalización}. Además, ya que por la definición de norma $||c\mathbf{v}||=|c|\hspace{0.5mm} ||\mathbf{v}||$ entonces, si hacemos el producto de un vector unitario por cualquier escalar de valor absoluto igual a uno (i.e., con $|c|=1$), obtendremos nuevamente un vector unitario.

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}[thick,scale=0.8, every node/.style={scale=1}]
        \draw[thick,<->] (0,0) -- (6,0);
        \draw[thick,<->] (0,0) -- (0,6);
        \draw[step=1cm,gray,very thin,dashed] (0,0) grid (5.9,5.9);
        \foreach \x in {1,2,3,4,5}
            \draw (\x cm,1pt) -- (\x cm,-1pt) node[anchor=north] {$\x$};
        \foreach \y in {1,2,3,4,5}
            \draw (1pt,\y cm) -- (-1pt,\y cm) node[anchor=east] {$\y$};
        \draw[ROJO,thin,dashed,->] (0,0) -- (3,3) node[] at (3.5,3.5) {$\begin{pmatrix} 3 & 3 \end{pmatrix}$};
    \draw[ROJO,very thick,->] (0,0) -- (0.71,0.71) node[] at (3.4,0.6) {$\frac{1}{\sqrt{18}}\begin{pmatrix}3&3\end{pmatrix}=\begin{pmatrix} \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} \end{pmatrix}$};
    \end{tikzpicture}
        \caption{Ejemplo de \emph{normalización} del vector $\protect\begin{pmatrix}3&3\protect\end{pmatrix}\in\mathbb{R}^2$. Ya que la norma de este vector es igual a $\sqrt{3^2+3^2}=\sqrt{18},$ hacemos su producto por el escalar $\frac{1}{\sqrt{18}}$ \textemdash el inverso multiplicativo de $\sqrt{18}$ en el campo real\textemdash\hspace{0.5mm}  y obtenemos al vector resultante $\protect\begin{pmatrix}\frac{1}{\sqrt{2}}&\frac{1}{\sqrt{2}}\protect\end{pmatrix}$, que tiene el mismo sentido y dirección que $\protect\begin{pmatrix}3&3\protect\end{pmatrix}$, pero con norma igual a uno.}
    \label{fig:Normalización_en_el_plano_cartesiano}
\end{figure}

Observemos que podemos utilizar la notación usual de vector unitario para reescribir la proyección \emph{vectorial} del vector $\mathbf{u}$ sobre el vector $\mathbf{v}$ como \[
    P_{\mathbf{v}}(\mathbf{u}) \ \hat{\mathbf{v}}
,\] \noindent donde $P_{\mathbf{v}}(\mathbf{u})$ es la proyección \emph{escalar}  de $\mathbf{u}$ sobre $\mathbf{v}$.

\subsection{Producto vectorial (cruz)*} 

El producto vectorial es una operación de gran utilidad, principalmente en algunas ramas de la física clásica (en las cuales se trabaja con vectores en $\mathbb{R}^3$), y tiene una interpretación geométrica bastante rica, por lo cual se menciona brevemente en estas notas. En esta sección también se mencionan por primera vez los temas de matrices, determinantes y vectores canónicos, los cuales asumo que ya debes conocer, aunque más adelante en el curso los repasaremos.

\subsubsection{Definición del producto vectorial (cruz)} \label{Def:Producto_vectorial}

En $\mathbb{R}^3$ se puede definir una operación entre dos vectores que da como resultado un tercer vector, el cual es ortogonal a los dos anteriores. A esta operación se le conoce como \emph{producto vectorial}.

\begin{tcolorbox}
\underline{Def.} En $\mathbb{R}^3$, el producto vectorial $\times:V\times V \rightarrow V$ entre dos vectores $\mathbf{r}=(r_1,r_2,r_3)$ y $\mathbf{s}=(s_1,s_2,s_3)$ se define como

$$\mathbf{r}\times\mathbf{s} \equiv (r_2s_3-r_3s_2,r_3s_1-r_1s_3,r_1s_2-r_2s_1).$$

\noindent Dado el símbolo ($\times$) utilizado para denotar esta operación, también se le conoce como \emph{producto cruz}.
\end{tcolorbox}{}

Algebráicamente, el producto cruz $\mathbf{r}\times\mathbf{s}$ también puede ser visto como el determinante de una matriz de $3\times3$ como sigue:

$$\mathbf{r}\times\mathbf{s} = \text{det} \begin{vmatrix} \mathbf{i}&\mathbf{j}&\mathbf{k} \\ r_1&r_2&r_3 \\ s_1&s_2&s_3 \\
\end{vmatrix} = \mathbf{i}(r_2s_3-r_3s_2)+\mathbf{j}(r_3s_1-r_1s_3)+\mathbf{k}(r_1s_2-r_2s_1),$$

\noindent donde $\mathbf{i},\mathbf{j}$ y $\mathbf{k}$ son los vectores canónicos de $\mathbb{R}^3$ $(1,0,0)$, $(0,1,0)$ y $(0,0,1)$, respectivamente.

Geométricamente, si nombramos como $\theta$ el menor ángulo de separación positivo entre dos vectores $\mathbf{r}$ y $\mathbf{s}\in\mathbb{R}^3$, entonces podemos hacer la definición equivalente $\mathbf{r}\times\mathbf{s}=||\mathbf{r}|| \hspace{0.5mm}  ||\mathbf{s}||\sin\theta$. En este caso, la magnitud del vector resultante de hacer el producto vectorial $\mathbf{r}\times\mathbf{s}$ se interpreta como la magnitud del área del paralelogramo formado por los vectores $\mathbf{r}$ y $\mathbf{s}$. De aquí se sigue que el producto vectorial de dos vectores ortogonales sea igual al producto de sus normas, mientras que el producto cruz de dos vectores colineales sea $0$ (en particular, el producto cruz de cualquier vector consigo mismo es igual a $0$). Además, de ambas definiciones (algebráica y geométrica) se sigue que el producto vectorial de cualquier vector en $\mathbb{R}^3$ con el vector nulo ($\mathbf{0}$) sea $\mathbf{0}$.

\subsubsection{Propiedades del producto vectorial (cruz)} \label{Prop:Producto_vectorial}

Las siguientes propiedades son fáciles de demostrar para cualesquiera $\mathbf{r},\mathbf{s},\mathbf{t}\in\mathbb{R}^3$ a partir de la definición de la sección \ref{Def:Producto_vectorial} (¡inténtalo!):

\begin{center}
\begin{tabular}{lr}
    $\mathbf{s}\times\mathbf{r} = -\mathbf{r}\times\mathbf{s}$ & Anticonmutatividad del producto vectorial \\
    $(\mathbf{r}\times\mathbf{s})\times\mathbf{t}\neq\mathbf{r}\times(\mathbf{s}\times\mathbf{t})$ & No asociatividad del producto vectorial \\
    $\mathbf{r}\times(\mathbf{s}+\mathbf{t}) = \mathbf{r}\times\mathbf{s}+\mathbf{r}\times\mathbf{t}$ & Distributividad bajo la suma vectorial \\
    $(a\mathbf{r}\times\mathbf{s}) = a(\mathbf{r}\times\mathbf{s})=(\mathbf{r}\times a\mathbf{s})$ & Compatibilidad con el producto de un vector por un escalar\\
    $\mathbf{r}\times(\mathbf{s}\times\mathbf{t})+\mathbf{s}\times(\mathbf{t}\times\mathbf{r})+\mathbf{t}\times(\mathbf{r}\times\mathbf{s}) = \mathbf{0}$ & Identidad de Jacobi para el producto vectorial.
\end{tabular}
\end{center}



\subsection{Triple producto escalar*}

Al unir el producto escalar con el producto vectorial, se obtiene una operación llamada \emph{triple producto escalar} entre tres vectores, que da como resultado un escalar. La magnitud del triple producto escalar se interpreta geométricamente como la magnitud del volumen del paralelepípedo formado por los tres vectores.

\subsubsection{Definición de triple producto escalar}
\begin{tcolorbox}
\underline{Def.} El \emph{triple producto escalar} entre tres vectores $\mathbf{r},\mathbf{s},\mathbf{t}\in\mathbb{R}^3$ se define como

$$\mathbf{r}\cdot\mathbf{s}\times\mathbf{t} \equiv r_1(s_2t_3-s_3t_2) + r_2(s_3t_1-s_1t_3) + r_3(s_1t_2-s_2t_1),$$

\noindent es decir, primero se realiza el producto vectorial $\mathbf{s}\times\mathbf{t}$ y luego se realiza el producto escalar entre el vector resultante de esa operación y $\mathbf{r}$.
\end{tcolorbox}

Recordemos que el producto escalar se realiza entre dos vectores y da como resultado un escalar, mientras que el producto vectorial se realiza entre dos vectores pero da como resultado un vector. Por lo tanto, no hay ambigüedad en la expresión $\mathbf{r}\cdot\mathbf{s}\times\mathbf{t}$, ya que la única forma lógica de juntar estas dos operaciones es realizando primero el producto vectorial y después el producto escalar; por ende, escribir esta operación como $\mathbf{r}\cdot(\mathbf{s}\times\mathbf{t})$ sería redundante.

Al igual que el producto vectorial, el triple producto escalar también puede ser visto algebráicamente como el determinante de una matriz:

$$\mathbf{r}\cdot\mathbf{s}\times\mathbf{t} = \begin{vmatrix} r_1&r_2&r_3 \\ s_1&s_2&s_3 \\ t_1&t_2&t_3 \end{vmatrix}.$$

    Además, recordando que el producto escalar de dos vectores ortogonales es igual a $0$ y que el vector resultante de la operación $\mathbf{r}\times\mathbf{s}$ es ortogonal tanto a $\mathbf{r}$ como a $\mathbf{s}$, se sigue directamente que $\mathbf{r}\cdot\mathbf{r}\times\mathbf{s}=0=\mathbf{r}\cdot\mathbf{s}\times\mathbf{r}$.



\subsubsection{Propiedades del triple producto escalar} \label{Prop:Triple_producto_escalar}

\begin{center}
\begin{tabular}{lr}
    $\mathbf{r}\cdot\mathbf{s}\times\mathbf{t} = \mathbf{s}\cdot\mathbf{t}\times\mathbf{r} = \mathbf{t}\cdot\mathbf{r}\times\mathbf{s}$ & Conmutatividad bajo permutaciones cíclicas \\
    $\mathbf{r}\cdot\mathbf{s}\times\mathbf{t} = \mathbf{r}\times\mathbf{s}\cdot\mathbf{t}$ & Invariancia bajo intercambio de operadores \\
    $\mathbf{r}\cdot\mathbf{s}\times\mathbf{t} = -\mathbf{r}\cdot\mathbf{t}\times\mathbf{s} = -\mathbf{s}\cdot\mathbf{r}\times\mathbf{t} = -\mathbf{t}\cdot\mathbf{s}\times\mathbf{r}$ & Anticonmutatividad bajo intercambio de dos vectores \\
\end{tabular}
\end{center}

Nota: también se puede definir un \emph{triple producto vectorial} entre tres vectores que da como resultado un vector como $\mathbf{r}\times\mathbf{s}\times\mathbf{t}$. Éste es de gran utilidad para hacer demostraciones de algunas identidades de cálculo vectorial en $\mathbb{R}^3$.

\subsection{Ejercicios de repaso}

\subsubsection{Producto escalar} \label{Ejer:Producto_escalar} 

\begin{enumerate}
    \item Sean las funciones $f(x)=\frac{x}{40.5}, g(x)=\frac{x^2}{243}$ y $h(x)=\frac{1}{9}$ definidas con dominio $[0,9]$. Calcula el producto escalar entre estas funciones (sin contar productos escalares de la forma $\langle f , f \rangle$) utilizando la definición vista en la sección \ref{Ejem:Producto_escalar}.
    \item Sea $V$ un espacio vectorial con producto escalar. Demuestra que para todo $\mathbf{v}\in V, \langle \mathbf{v},\mathbf{0}\rangle=\langle\mathbf{0},\mathbf{v}\rangle=0.$ Además, demuestra que si $\langle\mathbf{u},\mathbf{v}\rangle=\langle \mathbf{u},\mathbf{w}\rangle$ para todo $\mathbf{u}\in V$, entonces $\mathbf{v}=\mathbf{w}.$
    \item Explica por qué no se puede definir un producto escalar de la forma $\langle f,g\rangle=\int_a^b f(x)g(x)dx$ para funciones integrables $f$ y $g$ con integrales con valor infinito en $[a,b]$. 
    \item Sea $\mathbb{C}^2$ un espacio vectorial complejo con $\mathbf{q},\mathbf{r}\in\mathbb{C}^2$. Explica cuál(es) de las propiedades de la sección \ref{Prop:Producto_escalar} no se cumpliría(n) si definiéramos el producto escalar en este espacio vectorial ingenuamente como $\mathbf{q}\cdot\mathbf{r}=\sum_{i=1}^n q_ir_i$  con $n=2$ (nótese que esta misma definición de producto escalar nos generaría problemas al intentar usarla para definir una norma en este espacio). 
\end{enumerate}

\subsubsection{Norma} \label{Ejer:Norma}

\begin{enumerate}
    \item Calcula la norma de las funciones dadas en el primer ejercicio de la sección \ref{Ejer:Producto_escalar}. (Nota: utiliza la norma inducida por ese mismo producto escalar.) 
    \item ¿El conjunto $\{(x_1,x_2,...,x_n)\mathop|\mathop x_i\in\mathbb{R} \land ||(x_1,x_2,...,x_n)||\leq1\}$ con las operaciones entre $n$-tuplas vistas en la sección \ref{Ejem:Espacios_vectoriales} puede formar un espacio vectorial sobre el campo real? Argumenta. 
    \item Argumenta e ilustra la interpretación geométrica de la desigualdad de Cauchy-Schwarz para dos vectores cualesquiera de $\mathbb{R}^2$ (Nota: ver sec. \ref{Teo:Cauchy-Schwarz}). 
    \item Demuestra la desigualdad del triángulo $||\mathbf{a}+\mathbf{b}|| \leq ||\mathbf{a}||+||\mathbf{b}||$ para cualesquiera dos vectores en un espacio $V$ con producto escalar positivo definido donde la norma $||\mathbf{a}||\equiv+\sqrt{(\mathbf{a},\mathbf{a})}\hspace{3mm}\forall\hspace{1.5mm} \mathbf{a}\in V$ (pista: usa la desigualdad de Cauchy-Schwarz)\footnote{Con esto habrás demostrado que a partir de cualquier producto escalar positivo definido $(\mathbf{a},\mathbf{b})$ se puede definir una norma como $||\mathbf{a}|| = +\sqrt{(\mathbf{a},\mathbf{a})}.$}. Argumenta e ilustra su interpretación geométrica para dos vectores cualesquiera de $\mathbb{R}^2$. 
\end{enumerate}

\subsubsection{Interpretación geométrica del producto escalar: proyecciones y ortogonalidad}
\begin{enumerate}
\item Sea $\mathbf{v}\in\mathbb{R}^2$ no-nulo. Prueba que el conjunto $\{\mathbf{u}\in\mathbb{R}^2\hspace{0.5mm} |\hspace{0.5mm} \langle\mathbf{u},\mathbf{v}\rangle =0\}$ es una recta que pasa por el origen y llévala a una expresión de la forma $ax+by=c.$ 
    \item Sean $\mathbf{v}=\begin{pmatrix} v_1 & v_2 \end{pmatrix}, \mathbf{u}=\begin{pmatrix}u_1 & u_2 \end{pmatrix}$ vectores del espacio real $\mathbb{R}^2.$ Deduce que $\mathbf{u}\cdot\mathbf{v}=u_1v_1+u_2v_2=||\mathbf{u}||\hspace{0.5mm}||\mathbf{v}||\cos(\theta)$, donde $\theta$ es el ángulo mínimo entre ambos vectores en el plano cartesiano. (Nota: supon que ambos vectores son no nulos y encuentra al escalar $k\in\mathbb{R}$ tal que $\langle\mathbf{u},\mathbf{u}-k\mathbf{v}\rangle=0$. ¿Por qué es especial este escalar?) 
    \item Define a los vectores $\mathbf{u}=\begin{pmatrix} a_1 & a_2 \end{pmatrix}, \mathbf{v}=\begin{pmatrix} a_3 & a_4\end{pmatrix}$ y $\mathbf{w}=\begin{pmatrix} a_5 & a_6\end{pmatrix}\in\mathbb{R}^2,$ donde $a_i$ corresponde al $i$-ésimo dígito de tu número de cuenta. Explica gométricamente el hecho de que $\langle\mathbf{u}+\mathbf{v},\mathbf{w}\rangle=\langle\mathbf{u},\mathbf{w}\rangle+\langle\mathbf{v},\mathbf{w}\rangle$, en términos de las proyecciones discutidas en la sección \ref{Subsec:Interpretación geométrica del producto escalar} (Nota: recuerda la Ley del paralelogramo). 
\end{enumerate}

\subsubsection{Producto vectorial (cruz)*}

\begin{enumerate}
    \item Demuestra las propiedades del producto vectorial de la sección \ref{Prop:Producto_vectorial}. Además, da una interpretación geométrica para la primera, tercera y cuarta propiedad enlistadas. (Nota: si quieres hacer estas demostraciones utilizando una notación condensada, te recomiendo investigar acerca del símbolo de Levi-Civita el cual, junto con la delta de Kronecker, facilitan la escritura de muchas demostraciones de cálculo vectorial, entre otras áreas de las matemáticas.) 
%    \item Da una definición de un producto cruz entre vectores de $\mathbb{C}^3$ tal que se mantengan las propiedades vistas en la sección \ref{Prop:Producto_vectorial}.
\end{enumerate}

\subsubsection{Triple producto escalar*}

\begin{enumerate}
    \item Demuestra las propiedades del triple producto escalar de la sección \ref{Prop:Triple_producto_escalar}. 
\end{enumerate}{}

\begin{tcolorbox}
\begin{center}
    \textbf{Nota aclaratoria: \emph{Sobre nombres y traducciones...}}
\end{center}

\hspace{2.5mm}Como seguramente habrás notado al leer los libros recomendados, al producto escalar en inglés se le conoce como \emph{inner product}, y a los espacios vectoriales dotados de un producto escalar se les conoce como \emph{inner product spaces}. En español, al producto escalar también se le conoce como \emph{producto interior}; sin embargo, existe otro tipo de producto diferente al producto escalar al cual en inglés, desafortunadamente, le llaman \emph{interior product}.

\vspace{5mm}
\hspace{2.5mm}Esto significa que, en inglés, la convención es que \emph{scalar product} e \emph{interior product} sean operaciones diferentes mientras que, en español, la convención es que \emph{producto escalar} y \emph{producto interior} se refieran a la misma operación. Algunos textos en español utilizan \emph{producto interno} (en vez de producto interior) como sinónimo de \emph{producto escalar} para homologar los nombres con los utilizados en inglés pero, por ahora, las convenciones preponderantes en español e inglés no permiten una traducción directa.

\vspace{5mm}
\hspace{2.5mm}Por lo anterior, en estas notas decidí usar únicamente el nombre de \emph{producto escalar} para la operación entre dos vectores que da como resultado un escalar (la cual acostumbramos llamar \emph{producto punto} cuando esos vectores son $n$-tuplas), pero es importante que sepan que esta operación es equivalente al \emph{\underline{inner} product} de los textos en inglés.

\vspace{5mm}
\hspace{2.5mm} Para empeorar la situación, algunos textos en inglés se refieren a la operación de producto de un vector por un escalar como \emph{scalar multiplication} \textemdash por lo cual algunos textos en español pueden referirse a esta operación como \emph{multiplicación escalar} o, inclusive, \emph{producto escalar}\textemdash \hspace{0.5mm} mientras que otros textos utilizan el término \emph{scalar multiplication} para referirse a la multiplicación entre dos elementos del campo (escalares) que da como resultado otro elemento del campo (escalar). Por lo tanto, debemos recordar que el significado de estos términos depende del contexto en que se utilicen. 

\vspace{5mm}
\hspace{2.5mm} Finalmente, remarcamos que en la convención de este texto: 
    \begin{itemize}
        \item Los términos \emph{producto de un vector por un escalar} y \emph{reescalamiento} se emplean para referirnos a la operación realizada entre un vector del conjunto vectorial y un escalar del campo que da como resultado otro vector;
        \item el término \emph{producto escalar} se reserva para la operación realizada entre dos vectores que da como resultado un escalar;
        \item el término \emph{producto entre escalares} se utiliza para referirnos a la multiplicación entre dos escalares del campo que da como resultado otro escalar del campo.
    \end{itemize}

\end{tcolorbox}

\subsection{Ortogonalización y ortonormalización} \label{Subsec:Ortogonalización y ortonormalización}

Recordemos de la sección \ref{Subsec:Interpretación geométrica del producto escalar} que dos vectores $\mathbf{u}$ y $\mathbf{v}$ son ortogonales si $\langle\mathbf{u},\mathbf{v}\rangle=0$. Supongamos que este no es el caso, i.e., que $\langle\mathbf{u},\mathbf{v}\rangle\neq 0$\footnote{Aquí implícitamente estamos asumiendo que $\mathbf{u}$ y $\mathbf{v}$ son vectores no nulos ya que, por definición, el producto escalar \emph{distingue} al vector nulo (ver sec. \ref{Def:Producto_escalar}).}. Existe una manera de modificar cualquiera de los vectores de tal forma que se vuelva ortogonal al otro: simplemente definimos $\mathbf{u'}=\mathbf{u}-\langle\mathbf{u},\mathbf{v}\rangle\frac{\mathbf{v}}{||\mathbf{v}||^2}$ y comprobamos que $$\langle\mathbf{u'},\mathbf{v}\rangle=\big\langle\mathbf{u}-\langle\mathbf{u},\mathbf{v}\rangle\frac{\mathbf{v}}{||\mathbf{v}||^2},\mathbf{v}\big\rangle=\langle\mathbf{u},\mathbf{v}\rangle-\big\langle\frac{\langle\mathbf{u},\mathbf{v}\rangle}{||\mathbf{v}||^2}\mathbf{v},\mathbf{v}\big\rangle=\langle\mathbf{u},\mathbf{v}\rangle-\frac{\langle\mathbf{u},\mathbf{v}\rangle}{\langle\mathbf{v},\mathbf{v}\rangle}\langle\mathbf{v},\mathbf{v}\rangle=\langle\mathbf{u},\mathbf{v}\rangle-\langle\mathbf{u},\mathbf{v}\rangle\cdot 1=0.$$ 

A esto se le conoce como un proceso de \emph{ortogonalización}. Observemos que, si $\mathbf{v}$ es un vector unitario (i.e., si $||\mathbf{v}||=1$) entonces la definición de $\mathbf{u'}$ se reduce a $\mathbf{u'}=\mathbf{u}-\langle\mathbf{u},\mathbf{v}\rangle\mathbf{v}$, simplificando el proceso. Por razones que irán quedando más claras con la experiencia, a menudo es conveniente trabajar con bases de vectores que sean ortogonales entre sí y, en casos específicos, que además sean unitarios, por lo cual damos las siguientes definiciones..

\subsubsection{Definiciones}

\begin{tcolorbox}

    \underline{Def.} Sea $O=\{\mathbf{o}_1, \mathbf{o}_2, ..., \mathbf{o}_n\}$ una base de un espacio vectorial $V$. Decimos que $O$ es una \emph{base ortogonal} si cada uno de sus vectores es ortogonal a todos los demás, i.e., si $\langle\mathbf{o}_i,\mathbf{o}_j\rangle=0\hspace{3mm}\forall\hspace{1.5mm} \mathbf{o}_i, \mathbf{o}_j\in O, \hspace{1.5mm} i\neq j$.

\vspace{3mm}

    \underline{Def.} Sea $N=\{\mathbf{n}_1, \mathbf{n}_2, ..., \mathbf{n}_n\}$ una base de un espacio vectorial $V$. Decimos que $N$ es una base \emph{ortonormal} si es una base ortogonal y, además, todos sus vectores son unitarios, i.e., si $\langle\mathbf{n}_i,\mathbf{n}_j\rangle=\delta_{ij}\hspace{3mm}\forall\hspace{1.5mm} \mathbf{n}_i,\mathbf{n}_j\in N,$ donde $\delta_{ij}$ es la \emph{delta de Kronecker} de dos índices\footnote{En general, la delta de Kronecker de $n$ índices se define como $\delta_{ab...n}=1$ si $a=b=...=n$ y $\delta_{ab...n}=0$ en cualquier otro caso.}.

\end{tcolorbox}

Antes de ver cómo podemos construir bases ortogonales y ortonormales, el siguiente teorema y corolario nos ayudarán a comenzar a entender su utilidad.

\begin{teorema} {4.3.1.1} \label{Teo:4.3.1.1}
    Sea $V$ sobre $K$ un espacio vectorial con producto escalar y $S=\{\mathbf{v}_1, ..., \mathbf{v}_n\}$ un subconjunto ortogonal de $V$ con $\mathbf{v}_i\neq\mathbf{0}\hspace{3mm}\forall\hspace{1.5mm} 1\leq i\leq n.$ Si $\mathbf{u}\in \langle S \rangle \implies$ $$\mathbf{u} = \sum_{i=1}^n \frac{\langle\mathbf{u},\mathbf{v}_i\rangle}{||\mathbf{v}_i||^2}\mathbf{v}_i=\sum_{i=1}^n P_{\mathbf{v}_i}(\mathbf{u})\frac{\mathbf{v}_i}{||\mathbf{v}_i||}.$$

\begin{proof}
    Ya que $\mathbf{u}\in\langle S \rangle \implies \mathbf{u}=c_1\mathbf{v}_1+...+c_n\mathbf{v}_n=\sum_{i=1}^n c_i\mathbf{v}_i$ para algunos coeficientes $c_i\in K$. Para ver precisamente quiénes son esos coeficientes $c_i$, observemos que para $1\leq j\leq n$ $$\langle\mathbf{u},\mathbf{v}_j)=\big \langle \sum_{i=1}^n c_i\mathbf{v}_i, \mathbf{v}_j \big \rangle.$$ \noindent Aplicando las propiedades del producto escalar y la definición de conjunto ortogonal, tenemos que $$\langle\mathbf{u},\mathbf{v}_j\rangle=\sum_{i=1}^n \langle c_i\mathbf{v}_i, \mathbf{v}_j\rangle= \sum_{i=1}^n c_i\langle\mathbf{v}_i,\mathbf{v}_j\rangle=c_j\langle\mathbf{v}_j,\mathbf{v}_j\rangle=c_j||\mathbf{v}_j||^2\iff c_j||\mathbf{v}_j||^2=\langle\mathbf{u},\mathbf{v}_j\rangle\iff c_j=\frac{\langle\mathbf{u},\mathbf{v}_j\rangle}{||\mathbf{v}_j||^2}.$$ Sustituyendo, tenemos que \[
        \mathbf{u}=\sum_{i=1}^n c_i \mathbf{v}_i=\sum_{i=1}^n \frac{\langle\mathbf{u},\mathbf{v}_i\rangle}{||\mathbf{v}_i||^2}\mathbf{v}=\sum_{i=1}^n P_{\mathbf{v}_i}(\mathbf{u})\frac{\mathbf{v}_i}{||\mathbf{v}_i||}
    ,\] \noindent como se quería demostrar originalmente. En particular, observamos que $c_i= \frac{P_{\mathbf{v}_i}(\mathbf{u})}{||\mathbf{v}_i||}.$
\end{proof}

Habíamos visto con anterioridad que cualquier vector puede ser expresado como una combinación lineal única de elementos de su base; sin embargo, no habíamos entrado en detalles sobre cómo obtener los coeficientes necesarios para esto más allá de plantear y resolver un sistema de ecuaciones. Si aplicamos el teorema anterior a una base ortogonal $O$ de un espacio vectorial $V$, entonces $\forall\hspace{1.5mm} \mathbf{u}\in V $ tenemos una \emph{receta} para obtener directamente los coeficientes necesarios para expresar a $\mathbf{v}$ como combinación lineal de los vectores de $O$: de ahí viene la utilidad de las bases ortogonales.

    Nótese que, en particular, si el conjunto $S$ es \emph{ortonormal}, entonces $c_j=\langle\mathbf{u},\mathbf{v}_j\rangle$ en la demostración anterior y el resultado que obtuvimos se reduce a \[
        \mathbf{u} = \sum_{i=1}^n \langle\mathbf{u},\mathbf{v}_i\rangle\mathbf{v}_i
    .\] \noindent Por lo tanto, si aplicamos el Teorema 4.3.1.1 a una base ortonormal $N$, los coeficientes mencionados en el párrafo anterior son simplemente el producto escalar del vector $\mathbf{u}$ con cada vector de la base ortonormal.

\end{teorema}

\begin{corolario} {4.3.1.2}
Sea $V$ sobre $K$ un espacio vectorial con producto interior y $S$ un subconjunto ortogonal con vectores no nulos, entonces $S$ es linealmente independiente.

\begin{proof}
    Supongamos que $S=\{\mathbf{v}_1, ..., \mathbf{v}_n\}.$ Fijamos nuestra atención en la combinación lineal $\sum_{i=1}^n c_i\mathbf{v}_i=\mathbf{0}$, con $c_i\in K$. Aplicando el Teorema 4.3.1.1 con $\mathbf{u}=\mathbf{0}$ tenemos que $c_i=\frac{\langle\mathbf{0},\mathbf{v}_i\rangle}{||\mathbf{v}_i||^2}=0\hspace{3mm}\forall\hspace{1.5mm}  c_i\implies S$ es linealmente independiente.
\end{proof}

Observemos que este corolario es una genearlización del primer ejercicio de repaso de la sección 3. Además, este corolario y el teorema del cual se desprende nos dicen que los conjuntos ortogonales son buenos candidatos para bases, ya que son linealmente independientes y el cálculo de los coeficientes $c_i$ es sumamente sencillo. En particular, aplicando este corolario a los conjuntos ortogonales vemos que, si $V$ es un espacio vectorial de dimensión $n$, entonces cualquier conjunto ortogonal de $n$ vectores es una base de $V$ (ver teorema 4.2.3). 
\end{corolario}

Así como a partir de cualquier conjunto linealmente independiente de un espacio vectorial $V$ se puede construir una base para $V$ se puede, además, realizar un proceso de \emph{ortogonalización} u \emph{ortonormalización} de esa misma base. La demostración de este hecho\textemdash que también nos deletrea el proceso a seguir para lograrlo\textemdash \hspace{0.5mm} se conoce como el Teorema de Gram-Schmidt.

\subsubsection{Teorema de Gram-Schmidt} \label{Teo:Gram-Schmidt}

\begin{teorema} {4.3.2.1 (Gram-Schmidt)}
    Sea $V$ sobre $K$ un espacio vectorial y $S=\{\mathbf{u}_1, ..., \mathbf{u}_n\}$ un subconjunto linealmente independiente de $V$. Si definimos al conjunto $S'=\{\mathbf{v}_1, ..., \mathbf{v}_n\}$ de tal forma que $\mathbf{v}_1=\mathbf{u}_1$ y $$\mathbf{v}_k=\mathbf{u}_k-\sum_{j=1}^{k-1}\frac{\langle\mathbf{u}_k,\mathbf{v}_j\rangle}{||\mathbf{v}_j||^2} \mathbf{v}_j=\mathbf{u}_k-\sum_{j=1}^{k-1} P_{\mathbf{v}_j}(\mathbf{u}_k) \frac{\mathbf{v}_j}{||\mathbf{v}_j||},\hspace{3mm 2\leq k\leq n},$$ entonces $S'$ es un subconjunto ortogonal de $V$ tal que $\langle S' \rangle = \langle S \rangle.$

\begin{proof}
    Esta demostración se hará por inducción sobre $n$ y, para realizarla, definimos a $S_k\equiv\{\mathbf{u}_1, ..., \mathbf{u}_k\}$ para $k=1,2, ..., n.$

    \vspace{3mm}
\textbf{Base inductiva}
Si $n=1$, entonces el teorema se demuestra trivialmente, ya que $S'=S$, por lo cual $\langle S' \rangle =\langle S \rangle$ y, además,  $S'$ sería un subconjunto ortogonal de $V$ por vacuidad.

    \vspace{3mm}
\textbf{Hipótesis de inducción}
Supongamos que el conjunto $S'_{k-1}=\{\mathbf{v}_1, ..., \mathbf{v}_{k-1}\}$ ha sido construido siguiendo el proceso descrito en el planteamiento del teorema y que cumple las propiedades deseadas, es decir, que es un conjunto ortogonal tal que $\langle S'_{k-1} \rangle = \langle S_{k-1} \rangle,$ donde $S_{k-1}$ es linealmente independiente.

    \vspace{3mm}
\textbf{Paso inductivo}
    Sea $S_k$ un conjunto linealmente independiente y $S'_k=\{\mathbf{v}_1, ..., \mathbf{v}_{k-1}, \mathbf{v}_k\}$ tal que $$\mathbf{v}_k=\mathbf{u}_k-\sum_{j=1}^{k-1}\frac{\langle\mathbf{u}_k,\mathbf{v}_j\rangle}{||\mathbf{v}_j||^2}\mathbf{v}_j.$$

    Si $\mathbf{v}_k=\mathbf{0}$ entonces la ecuación anterior implicaría que $\mathbf{u}_k\in\langle S'_{k-1} \rangle = \langle S_{k-1} \rangle,$ lo cual contradice el hecho de que $S_k$ es un conjunto linealmente independiente, por lo cual forzozamente $\mathbf{v}_k\neq\mathbf{0}.$ 

    Para $1\leq i\leq k-1$ se sigue que $$\langle\mathbf{v}_k,\mathbf{v}_i\rangle=\big \langle\mathbf{u}_k-\sum_{j=1}^{k-1}\frac{\langle\mathbf{u}_k,\mathbf{v}_j\rangle}{||\mathbf{v}_j||^2}\mathbf{v}_j,\mathbf{v}_i \big \rangle=\langle\mathbf{u}_k,\mathbf{v}_i\rangle-\sum_{j=1}^{k-1}\frac{\langle\mathbf{u}_k,\mathbf{v}_j\rangle}{||\mathbf{v}_j||^2}\langle\mathbf{v}_j,\mathbf{v}_i\rangle=\langle\mathbf{u}_k, \mathbf{v}_i\rangle-\frac{\langle\mathbf{u}_k,\mathbf{v}_i\rangle}{||\mathbf{v}_i||^2}||\mathbf{v}_i||^2=\mathbf{0},$$ ya que en nuestra hipótesis inductiva supusimos que $S'_{k-1}$ es ortogonal. Por ende, $S'_k$ es un conjunto ortogonal de vectores no nulos.

Finalmente, por construcción $S'_k\subseteq \langle S_k \rangle$, lo cual también implica que $\langle S'_k \rangle \subseteq \langle S_k \rangle;$ análogamente, por la forma en que construimos $S'_k$, $S_k\subseteq \langle S'_k \rangle$, lo cual implica que $\langle S_k \rangle \subseteq \langle S'_k \rangle.$ Juntando ambos resultados, concluimos que $\langle S_k \rangle = \langle S'_k \rangle ,$ como queríamos demostrar.

\end{proof}
    Aplicando este teorema a bases, nos dice que a partir de una base arbitraria $B$ de un espacio vectorial $V$, se puede construir una base ortogonal $O$ para ese mismo espacio $V$: sólo hace falta escoger algún vector $\mathbf{b}_1\in B$ con el cual definir $\mathbf{o}_1\equiv\mathbf{b}_1$ y después seguir el procedimiento descrito en el teorema. 

    Además, observemos que se podría modificar ligeramente el procedimiento seguido en el teorema anterior definiendo al primer vector de la nueva base como un vector unitario. Es más, supongamos que definimos un nuevo conjunto generador $N$ con $\mathbf{n}_1=\frac{\mathbf{b}_1}{||\mathbf{b}_1||}$ y después, cada vez que obtenemos un nuevo vector ortogonal a los anteriores siguiendo el proceso del Teorema 4.3.2.1, lo normalizamos antes de añadirlo a $N$: en este caso, el conjunto generador resultante será una base \emph{ortonormal} de $V$. Es decir, podríamos hacer un Teorema de Gram-Scmidt \emph{modificado} de tal forma que a partir de cualquier subconjunto linealmente independiente podamos generar un subconjunto ortonormal que genere al mismo subespacio que el anterior. Aplicado a bases, estaríamos asegurando que a partir de cualquier base $B$ de un espacio vectorial $V$, se puede construir una base ortonormal para $V$, además de detallar el proceso mediante el cual se contstruye dicha base ortonormal.  

\end{teorema}

%\subsubsection{Ejemplos de ortogonalización y ortonormalización} 

\subsubsection{Ortogonalización y ortonormalización}
\begin{enumerate}
    \item Sea $B$ la base que obtuviste del ejercicio 4.4.1.2. Aplica el Teorema de Gram-Schmidt y construye una base ortogonal de $\mathbb{R}^3$ a partir de $B$. 
    \item Aplica el Teorema de Gram-Schmidt \emph{modificado} y construye una base ortonormal a partir de $B$. 
\item Sea $P_0(x)=1, P_1(x)=x, P_2(x)=\frac{3x^2-1}{2}, P_3(x)=\frac{5x^3-3x}{2}.$ Demuestra que el conjunto $\{P_0,P_1,P_2,P_3\}$ es una base del espacio vectorial $P^3$ y di si es ortogonal y/o ortonormal en el intervalo $[-1,1]$ (Nota: tendrás que utilizar la definición de producto escalar para funciones vista en la sec. \ref{Ejem:Producto_escalar}). 
\end{enumerate}

\end{document}
