\documentclass[apuntes]{subfiles}

\begin{document}

\section{Combinaciones lineales, subespacio generado y conjunto generador, dependencia e independencia lineal} \label{Sec:3}

\subsection{Combinaciones lineales} \label{Subsec:Combinaciones_lineales}

Como vimos en la sección \ref{Def:Espacio_vectorial}, las dos operaciones \emph{necesarias} para definir un espacio vectorial son la suma vectorial (realizada entre elementos del conjunto vectorial) y el producto de un vector por un escalar (realizada entre un elemento del conjunto vectorial y un elemento del campo), ambas dando como resultado un vector en el mismo espacio. La operación más general que podemos realizar a partir de dichas operaciones se define a continuación.

\begin{tcolorbox}
    \underline{Def.} Sea $V$ sobre $K$ un espacio vectorial y $L=\{\mathbf{v}_1, \mathbf{v}_2, ..., \mathbf{v}_n\}\subset V$ un conjunto con un número finito de vectores de ese espacio. Decimos que $\mathbf{u}$ es una \emph{combinación lineal} de los vectores de $L$ si y sólo si $$\mathbf{u} =c_1\mathbf{v}_1+c_2\mathbf{v}_2+...+c_n\mathbf{v}_n=\sum_{i=1}^n c_i\mathbf{v}_i,\hspace{3mm}c_i\in K, \mathbf{v}_i\in L, n\in \mathbb{N}.$$

    Observemos que, dadas las propiedades de cerradura de ambas operaciones vistas en la sección \ref{Def:Espacio_vectorial}, claramente $\mathbf{u} \in V$, es decir, \emph{el resultado de la combinación lineal de vectores de un espacio vectorial $V$ siempre es un vector del mismo espacio}. 
\end{tcolorbox}

Así, vemos que en un espacio vectorial $V$ arbitrario es posible expresar cualquiera de sus vectores como combinación lineal de otros vectores del mismo espacio. Por ejemplo, en $\mathbb{R}^2$ el vector $$\begin{pmatrix} 1 & 5 \end{pmatrix} = \begin{pmatrix} 1 & 0 \end{pmatrix} + 5\begin{pmatrix} 0 & 1 \end{pmatrix} = 2\begin{pmatrix} 1 & 1.5 \end{pmatrix} + (-0.5)\begin{pmatrix} 2 & -4 \end{pmatrix} $$ $$ = (-4)\begin{pmatrix} 0.5 & -3 \end{pmatrix} + 3\begin{pmatrix} 1 & 1 \end{pmatrix} + (-5)\begin{pmatrix} 0 & 2 \end{pmatrix} = ...$$ \noindent Observamos que, en cada caso, el valor de los coeficientes $c_i\in\mathbb{R}$ depende de los vectores $\mathbf{v}_i\in\mathbb{R}^2$ con los cuales se realiza la combinación lineal. Para dar otro ejemplo, en $P^2$, si definimos los vectores $f(x) = 7x^2 - 5x + 2, g(x) = x^2, h(x) = 9x, i(x)=7, j(x)=x^2 + x + 1$, podemos verificar que $$f(x) = 7g(x)-\frac{5}{9}h(x)+\frac{2}{7}i(x)=7j(x)-\frac{4}{3}h(x)+\frac{1}{7}i(x)=3j(x)+4g(x)+-\frac{8}{9}h(x)-\frac{1}{7}i(x)=...$$

Siguiendo la discusión de la sección \ref{Subsec:Interpretación_geométrica_de_las_operaciones_de_los_espacios_vectoriales}, donde se plantearon por separado las interpretaciones geométricas de las operaciones de suma entre vectores y producto de un vector por un escalar, podemos pensar que, si combinamos ambas operaciones, la operación resultante se puede interpretar como una combinación de flechas rectas reescaladas (y/o rotadas, si el espacio vectorial es complejo y el escalar tiene una parte imaginaria no nula), cada una con su propia longitud, dirección y sentido, a las cuales aplicamos la Ley del paralelogramo para obtener una nueva flecha (o línea) como resultado. Precisamente por ello es que a está operación general se le conoce como \emph{combinación lineal}.

\newpage
\subsection{Subspacio generado y conjunto generador} \label{Subsec:Espacio_generado_y_conjunto_generador}

Sea $V$ sobre $K$ un espacio vectorial y $\mathbf{v}_1, \mathbf{v}_2, ... , \mathbf{v}_n\in V$ vectores. Si tomamos un conjunto de estos vectores, digamos, $G=\{\mathbf{v}_1, \mathbf{v}_2\}$, podemos también definir el conjunto de todos los vectores que se pueden generar a través de combinaciones lineales de los vectores de $G$ como $\{a\mathbf{v}_1+b\mathbf{v}_2\hspace{0.5mm}|\hspace{0.5mm}a,b\in K\}$. Este nuevo conjunto cumple con todas las propiedades de un espacio vectorial, y este hecho es generalizable a cualquier conjunto $G$ con un número finito de elementos, lo cual motiva la definición siguiente. 

\begin{tcolorbox} \label{Def:Espacio_generado_y_conjunto_generador}

    \underline{Def.} Sea $V$ sobre $K$ un espacio vectorial y $G\subset V$. Entonces, definimos $$\langle G\rangle \equiv \{c_1\mathbf{v}_1+c_2\mathbf{v}_2+...+c_n\mathbf{v}_n\hspace{0.5mm}|\hspace{0.5mm}c_i\in K, \mathbf{v}_i\in G\}\subseteq V.$$ A $G$ se le conoce como el \emph{conjunto generador} y a $\langle G \rangle$, como el \emph{subespacio generado} por $G$ ya que, por definición, cumple con todas las propiedades de un espacio vectorial y además, por cerradura de $V$, cualquier vector generado por $G$ es elemento de $V$\footnote{Nótese por la definición que, dependiendo de cómo sea el conjunto $G$, es posible que $\langle G \rangle =V$.}. Por completez, definimos $\langle \emptyset \rangle = \{\mathbf{0}\}.$ Si $G$ consiste de un sólo vector $\mathbf{g}$, podemos escribir el generado de $G$ como $\langle \mathbf{g} \rangle$.

\end{tcolorbox}

Para dar algunos ejemplos, si elegimos cualquier vector $\mathbf{v}\in\mathbb{R}^2$, entonces el subespacio generado correspondiente $\langle \mathbf{v} \rangle = \{c\mathbf{v}\hspace{0.5mm}|\hspace{0.5mm}c\in \mathbb{R}\}$ se puede interpretar geométricamente en el plano cartesiano como el conjunto de todas las flechas posibles de obtener a partir de reescalamientos de $\mathbf{v}$. Por otro lado, si en $\mathbb{R}^3$ definimos a $N=\{\begin{pmatrix} 1 & 0 & 0 \end{pmatrix}, \begin{pmatrix} 0 & 1 & 0 \end{pmatrix}, \begin{pmatrix} 0 & 0 & 1 \end{pmatrix}\}$ entonces vemos que $$\langle N \rangle = \{c_1\begin{pmatrix} 1 & 0 & 0 \end{pmatrix} + c_2\begin{pmatrix} 0 & 1 & 0 \end{pmatrix} + c_3\begin{pmatrix} 0 & 0 & 1 \end{pmatrix}\hspace{0.5mm}|\hspace{0.5mm}c_1,c_2,c_3\in\mathbb{R}\},$$ pero esto es equivalente a la definición $\mathbb{R}^3 = \{\begin{pmatrix} c_1 & c_2 & c_3\end{pmatrix} \hspace{0.5mm}|\hspace{0.5mm}c_1,c_2,c_3\in\mathbb{R}\}$; es decir, en este caso \emph{el espacio generado por los vectores de $N$ es igual a $\mathbb{R}^3$}, i.e., $\langle N \rangle =\mathbb{R}^3$.

A continuación, veremos un teorema que será de gran importancia en las secciones posteriores.

\begin{teorema} {3.2.1} 
Sea $V$ un espacio vectorial, $S\subset V$ un conjunto de vectores de $V$ y $\mathbf{v}\in V$ un vector arbitrario. Si $S'=S\cup\{\mathbf{v}\}$, entonces $\langle S \rangle = \langle S' \rangle \iff v\in\langle S \rangle.$

\begin{proof}
Ya que $\mathbf{v}\in S'$ entonces trivialmente se cumple que $\mathbf{v}\in\langle S'\rangle;$ por lo tanto, si $\mathbf{v}\notin \langle S \rangle \implies \langle S \rangle \neq \langle S' \rangle.$ Por otro lado, si $\mathbf{v}\in\langle S \rangle$ entonces $S'\subset\langle S \rangle$, lo cual implica que $\langle S' \rangle \subset \langle S \rangle.$ Además, ya que $S\subset S'$, entonces trivialmente se cumple que $\langle S \rangle \subset \langle S' \rangle.$ En conclusión, $\langle S \rangle =\langle S' \rangle.$
\end{proof}
        
    Este teorema nos dice que agregar un vector a un conjunto generador no necesariamente cambiará el subespacio generado por ese conjunto generador. Para que este cambio realmente suceda, el vector añadido debe ser en algún sentido \emph{ajeno} a los del conjunto generador original. En la siguiente sección, veremos algunas definiciones necesarias para precisar esta idea.

\end{teorema}

\newpage
\subsection{Dependencia e independencia lineal} \label{Subsec:Dependencia_e_independencia_lineal}

Como se vio en la sección \ref{Subsec:Combinaciones_lineales}, un vector puede ser expresado como diferentes combinaciones lineales de otros vectores del mismo espacio. En particular, el vector nulo $\mathbf{0}$ de cualquier espacio vectorial $V$ puede ser obtenido a través de la \emph{combinación lineal trivial} de cualesquiera $n$ vectores del espacio: sólo basta con que todos los coeficientes sean cero, i.e. $$0\mathbf{v}_1+0\mathbf{v}_2+...+0\mathbf{v}_n=\mathbf{0}, \hspace{3mm}\forall\hspace{1.5mm}\mathbf{v}_1 \mathbf{v}_2, ...,\mathbf{v}_n \in V.$$ Sin embargo, también pueden existir combinaciones lineales entre $n$ vectores de un espacio vectorial $V$ que den como resultado $\mathbf{0}$ pero sean no triviales (es decir, tengan coeficientes distintos de cero), e.g., en $\mathbb{R}^2, 7\begin{pmatrix} 1 & 1 \end{pmatrix}+5\begin{pmatrix} -1 & 1 \end{pmatrix}+2\begin{pmatrix} -1 & -6 \end{pmatrix}=\begin{pmatrix} 0 & 0 \end{pmatrix}=\mathbf{0}$. Una consecuencia de este hecho es que podamos despejar a cualquiera de los vectores y expresarlo como combinación lineal de los demás; por ejemplo, $\begin{pmatrix} 1 & 1 \end{pmatrix}=-\frac{5}{7}\begin{pmatrix} -1 & 1 \end{pmatrix}-\frac{2}{7}\begin{pmatrix} -1 & -6 \end{pmatrix}$, ó $\begin{pmatrix} -1 & -6 \end{pmatrix} = -\frac{7}{2}\begin{pmatrix} 1 & 1 \end{pmatrix}-\frac{5}{2}\begin{pmatrix} -1 & 1 \end{pmatrix},$ etc. Este importante hecho motiva la siguiente definición.

\subsubsection{Definición de dependencia e independencia lineal} \label{Def:Dependencia_e_independencia_lineal}

\begin{tcolorbox}

    \underline{Def.} Sea $V$ un espacio vectorial y $\mathbf{v}_1,\mathbf{v}_2,...,\mathbf{v}_n\in V$. Decimos que los vectores $\mathbf{v}_1,\mathbf{v}_2, ...,\mathbf{v}_n$ son \emph{linealmente independientes} entre sí si la única combinación lineal de ellos que da como resultado el vector nulo es la trivial (i.e., en la cual todos los coeficientes son cero). Matemáticamente, $$\mathbf{v}_1,\mathbf{v}_2, ..., \mathbf{v}_n \hspace{1.5mm} \text{son} \hspace{1.5mm} l.i. \iff c_1\mathbf{v}_1+c_2\mathbf{v}_2+...+c_n\mathbf{v}_n=\mathbf{0}\implies c_1,c_2, ...,c_n=0.$$

    En cambio, decimos que $\mathbf{v}_1,\mathbf{v}_2, ..., \mathbf{v}_n$ son \emph{linealmente dependientes} si existe al menos una combinación lineal no trivial que dé como resultado el vector nulo o, equivalentemente, si cualquiera de los vectores $\mathbf{v}_i$ puede ser expresado como una combinación lineal de los demás\footnote{El vector nulo se excluye de estas definiciones ya que, de lo contrario, cumpliría ambas trivialmente para cualquier conjunto arbitrario de vectores dado.}.

    Si todos los vectores de un conjunto $S$ son linealmente dependientes (independientes) entre sí, se dice que el conjunto $S$ es linealmente dependiente (independiente).

\end{tcolorbox}

Por ejemplo, en $\mathbb{R}^2$ los vectores $\mathbf{u}_1=\begin{pmatrix} 1 & 5 \end{pmatrix}$ y $\mathbf{u}_2=\begin{pmatrix} -3 & -15 \end{pmatrix}$ son linealmente dependientes, ya que $\mathbf{u}_2=-3\mathbf{u}_1$, por lo cual $3\mathbf{u}_1+\mathbf{u}_2=\mathbf{0}$; por otro lado, los vectores $\mathbf{v}_1=\begin{pmatrix} 1 & 2 \end{pmatrix}$ y $\mathbf{v}_2=\begin{pmatrix} 1 & 3 \end{pmatrix}$ son linealmente independientes, ya que no existe un número $c\in\mathbb{R}$ tal que $\mathbf{v}_1=c\mathbf{v}_2.$. Notemos que, como nuestros vectores en este caso son $2-$tuplas, las ecuaciones $\mathbf{u}_2=-3\mathbf{u}_1$ y $\mathbf{v}_1=c\mathbf{v}_2$ son en realidad la notación compactada de un \emph{sistema de ecuaciones}, con una ecuación por cada entrada del vector. En particular, la ecuación $\mathbf{v}_1=c\mathbf{v}_2$ puede ser reescrita como $$\begin{pmatrix} 1 & 2 \end{pmatrix}=c\begin{pmatrix} 1 & 3 \end{pmatrix}\iff 1=c1 \hspace{1.5mm} \land\hspace{1.5mm} 2=c3,$$ de donde vemos directamente que no existe solución para $c$, por lo cual estos vectores son linealmente independientes.

    En general, cuando expresamos combinaciones lineales del tipo $\mathbf{v}=c_1\mathbf{u}_1+...+c_n\mathbf{u}_n$ en donde los vectores son dados pero los coeficientes son desconocidos, éstos útlimos se vuelven las \emph{incógnitas} del \emph{sistema de ecuaciones algebráicas} dado por la ecuación $\mathbf{v}=c_1\mathbf{u}_1+...+c_n\mathbf{u}_2$. El número de ecuaciones del sistema dependerá de la naturaleza de los vectores, mientras que el número de incógnitas es igual al número de coeficientes desconocidos. Por lo tanto, la pregunta de si un vector es linealmente independiente o dependiente de otro(s) se reduce a la de si el sistema de ecuaciones asociado a la combinación lineal de ellos tiene solución o no.

Para ver más ejemplos de conjuntos de vectores linealmente dependientes e independientes pueden consultar, e.g., el Friedberg (págs. 36-38), el Lang introductorio (págs. 104-109), etc. 

\subsubsection{Interpretación geométrica de la dependencia e independencia lineal}

Como ya mencionamos, si un vector $\mathbf{v}_n\in V$ es linealmente \emph{dependiente} de otros vectores $\mathbf{v}_1, \mathbf{v}_2, ..., \textbf{v}_m\in V$, entonces puede ser expresado como combinación lineal de esos vectores. Geométricamente, en los espacios vectoriales reales $\mathbb{R}^2$ y $\mathbb{R}^3$ esto quiere decir que es posible reescalar y combinar (mediante la Ley del paralelogramo) las flechas de los vectores $\mathbf{v}_1, \mathbf{v}_2, ..., \mathbf{v}_m$ y obtener, como resultado final, a $\mathbf{v}_n$. Adicionalmente, en el espacio vectorial complejo $\mathbb{C}$, también se podrían estar rotando los vectores $\mathbf{v}_1, \mathbf{v}_2, ..., \mathbf{v}_m$ \textemdash además de reescalarlos y combinarlos\textemdash \hspace{0.5mm} para formar, finalmente, a $\mathbf{v}_n$. Si son linealmente \emph{independientes}, entonces lo anterior no es posible.

\subsubsection{Algunos teoremas sobre dependencia e independencia lineal} \label{Teo:Dependencia_e_independencia_lineal} 

\begin{teorema} {3.3.3.1} 

    Sea $V$ un espacio vectorial y $\mathbf{v_1}, \mathbf{v_2}, ..., \mathbf{v}_n$ vectores linealmente independientes de $V$. Sean $c_1, c_2, ..., c_n\in K$ y $d_1, d_2, ..., d_n\in K$ tales que $$c_1\mathbf{v}_1+c_2\mathbf{v}_2+...+c_n\mathbf{v}_n=d_1\mathbf{v}_1+d_2\mathbf{v}_2+...+d_n\mathbf{v}_n,$$ entonces se tiene que $c_i=d_1\hspace{3mm}\forall\hspace{1.5mm}i\in\{1,2, ..., n\}$.

    \begin{proof}

        $$c_1\mathbf{v_1}+c_2\mathbf{v}_2+...+c_n\mathbf{v}_n=d_1\mathbf{v}_1+d_2\mathbf{v}_2+...+d_n\mathbf{v}_n\iff(c_1-d_1)\mathbf{v}_1+(c_2-d_2)\mathbf{v}_2+...+(c_n-d_n)\mathbf{v}_n=\mathbf{0}.$$ Pero, ya que por hipótesis estos vectores son linealmente independientes, entonces por definición $$c_i-d_i=0\hspace{3mm}\forall\hspace{1.5mm}i\in\{1,2, ...,n\}\iff c_i=d_i\hspace{3mm}\forall\hspace{1.5mm}i\in\{1,2, ...,n\}.$$

    \end{proof}

    Este teorema significa que si un vector es resultado de una combinación lineal de vectores linealmente independientes, entonces esa combinación lineal es \emph{la única} que da como resultado a ese vector. Es decir, que si $\mathbf{u}=c_1\mathbf{v}_1+c_2\mathbf{v}_2+...+c_n\mathbf{v}_n$ y $\{\mathbf{v}_1,\mathbf{v}_2, ...,\mathbf{v}_n\}\hspace{1.5mm}\text{es}\hspace{1.5mm} l.i.$ entonces no existe otra combinación de escalaras y vectores $c_i \mathbf{v}_i$ tal que la suma de todos ellos dé $\mathbf{u}$.

\end{teorema}

\begin{teorema} {3.3.3.2} 

    Sea $V$ un espacio vectorial y $S_1,S_2$ subespacios tales que $S_1\subseteq S_2\subseteq V$. Si $S_2$ es linealmente independiente, entonces $S_1$ también es linealmente independiente.

\begin{proof}

    Sean $S_1=\{\mathbf{v}_1,\mathbf{v}_2, ..., \mathbf{v}_k\}$ y $S_2=\{\mathbf{v}_1,\mathbf{v}_2, ..., \mathbf{v}_n\}$ con $k\leq n$. Ya que por hipótesis $S_2$ es $l.i.$, por definición se cumple que $$c_1\mathbf{v}_1+c_2\mathbf{v}_2+...+c_n\mathbf{v}_n=\mathbf{0}\iff c_i=0\hspace{3mm}\forall\hspace{1.5mm}  i\in\{1,2, ..., n\}.$$ Si $k=n$ entonces $S_1$ también es l.i. trivialmente. Supongamos que $k<n$. Entonces, por el Teorema 1.2.3.2 (ver sec. \ref{Teo:Espacios_vectoriales}), $0\mathbf{v}_{k+1}+...+0\mathbf{v}_n=\mathbf{0}$ por lo cual lo anterior implica que $$c_1\mathbf{v}_1+c_2\mathbf{v}_2+...+c_k\mathbf{v}_k=\mathbf{0}\iff c_i=0\hspace{3mm}\forall\hspace{1.5mm}  i\in\{1,2, ...,k\}.$$ Por lo tanto, por definición, $S_1$ también es $l.i$.

\end{proof}

Este teorema nos dice que si removemos un vector de un conjunto linealmente independiente, el conjunto resultante también es linealmente independiente.

\end{teorema}

\begin{teorema} {3.3.3.3}

Sea $V$ un espacio vectorial sobre un campo $K$ y $L\subset V$ un conjunto con $n$ elementos linealmente independientes entre sí. Entonces, para cualquier $\mathbf{v}\in V$, el conjunto $L'\equiv L\cup \{\mathbf{v}\}$ es $l.i. \iff \mathbf{v}\notin \langle L \rangle$.

\begin{proof}
Sea $L=\{\mathbf{u}_1, ... , \mathbf{u}_n\}.$ Supongamos que $\mathbf{v}\in\langle L \rangle$, entonces existen coeficientes $c_i\in K$ tales que $\mathbf{v}=c_1\mathbf{u}_1+...+c_n\mathbf{v}_n.$ Despejando esta ecuación, obtenemos que $c_1\mathbf{u}_1+...+c_n\mathbf{u}_n+(-1)\mathbf{v}=\mathbf{0},$ es decir, que existe una combinación lineal no trivial entre los vectores de $L'$ que dan como resultado al vector nulo, por lo cual $L'$ es un conjunto linealmente dependiente.

Por otro lado, supongamos que $L'$ es linealmente dependiente. Entonces, existe una combinación lineal no trivial de los vectores de $L'$ que resulta en el vector nulo, i.e., $c_1\mathbf{u}_2+...+c_n\mathbf{u}_n+b\mathbf{v}=\mathbf{0}$ con al menos un coeficiente distinto de cero. En este caso, el coeficiente $b\neq 0$: si $b$ fuera igual a cero, la ecuación restante sería $c_1\mathbf{u}_1+...+c_n\mathbf{u}_n=\mathbf{0}$; ya que $L$ es linealmente independiente, entonces todos los vectores $c_i$ deben ser iguales a cero pero, ya que estamos suponiendo que también $b=0$, entonces el conjunto $L'$ también sería linealmente independiente, contradiciendo la hipótesis. Así, sabiendo que $b\neq 0$ podemos despejar la ecuación $c_1\mathbf{u}_1+...+c_n\mathbf{u}_n+b\mathbf{v}=\mathbf{0}$ y obtener que $\mathbf{v}=\frac{-c_1}{b}\mathbf{u}_1+...+\frac{-c_n}{b}\mathbf{u}_n$, lo cual implica que $\mathbf{v}\in\langle L \rangle.$

    Por contraposición, tenemos que $L'\equiv L\cup\{\mathbf{v}\}\hspace{1.5mm}\text{es}\hspace{1.5mm} l.i.\iff \mathbf{v}\notin \langle L \rangle.$

\end{proof}

    La demostración de este teorema básicamente nos dice que si tenemos un conjunto linealmente independiente y agregamos a un vector de su subespacio generado a este conjunto, entonces se vuelve linealmente dependiente. En contraposición, concluimos que en cualquier conjunto linealmente dependiente existe una especie de \emph{redundancia} entre sus elementos, ya que se puede remover a cualquiera de ellos sin alterar el subespacio generado por este conjunto. En cambio, remover un vector de un conjunto linealmente independiente \emph{sí} altera el subespacio generado por ese conjunto.
\end{teorema}


\subsection{Ejercicios de repaso}

\subsubsection{Combinaciones lineales}

\begin{enumerate}
    \item Demuestra que si tenemos tres vectores $\mathbf{u}, \mathbf{v}$ y $\mathbf{w}$ no nulos y ortogonales entre sí, entonces no puede haber una combinación lineal de dos de ellos que dé como resultado el tercero (Nota: repasa la definición de ortogonalidad y las propiedades del producto escalar vistas en la sec. \ref{Sec:2}.) 
    \item Da un ejemplo de la demostración anterior en $\mathbb{R}^3$, e interpreta este resultado geométricamente (Nota: recuerda la relación entre el producto escalar y las proyecciones.) 
\end{enumerate}

\subsubsection{Subespacio generado y conjunto generador}

\begin{enumerate}
    \item Sea $a\in\mathbb{R}$ un vector arbitrario del espacio vectorial real $\mathbb{R}$. Demuestra que este espacio vectorial sólo tiene subespacios vectoriales triviales. ¿Quiénes son los valores $a\in\mathbb{R}$ que generan a estos subespacios, en cada caso? 
    \hypertarget{Ejer:3.4.2.2}{} \item Sea $G=\{\begin{pmatrix} 3 & 0 & 3 \end{pmatrix}, \begin{pmatrix} -\frac{1}{2} & 0 & \frac{1}{2} \end{pmatrix}\}\subset\mathbb{R}^3.$ Escribe a $\langle G \rangle$ algebráicamente y descríbelo geométricamente. ¿A qué espacio vectorial real que conoces se parece? 
    \item Sea $\mathbf{c}$ un vector arbitrario del espacio vectorial complejo $\mathbb{C}$. Da una interpretación geométrica para $\langle \mathbf{c} \rangle$ (Nota: recuerda que, en este caso, $K=\mathbb{C}$; además, te sugiero que primero escribas a $\langle \mathbf{c}\rangle$ algebráicamente y, a partir de ahí, busques interpretarlo geométricamente.). 
    \item Sea $F=\{x^0,x^1,x^2,...,x^n\}$ con $n\in\mathbb{N}$ un conjunto de funciones reales de una variable real. ¿Quién es, entonces, $\langle F \rangle$? 
\end{enumerate}

\subsubsection{Dependencia e independencia lineal}

\begin{enumerate}
    \item Sea $n_i$ el $i$-ésimo dígito de tu número de cuenta. Determina si los vectores $\begin{pmatrix} n_1 + i(n_2) \end{pmatrix}$ y $\begin{pmatrix} n_3 + i(n_4) \end{pmatrix} \in \mathbb{C}$ son linealmente dependientes o independientes y muéstralo gráficamente en el plano complejo. 
    \item Repite el mismo ejercicio para los vectores reales $\begin{pmatrix} n_1 & n_4 & n_7 \end{pmatrix}, \begin{pmatrix} n_2 & n_5 & n_8 \end{pmatrix}$ y $\begin{pmatrix} n_3 & n_6 & n_9 \end{pmatrix}\in\mathbb{R}^3.$ ¿Cuál es el conjunto linealmente dependiente más grande que puedes armar con estos vectores, sin considerar los subespacios generados por éstos? ¿Qué hay del conjunto linealmente independiente más grande? 
    \item Sea $V$ un espacio vectorial y $S_1\subseteq S_2\subseteq V$. Demuestra que si $S_1$ es linealmente dependiente, entonces $S_2$ es linealmente dependiente. 
    \item Sean las funciones $p_1(x) = 3x^2-x, p_2(x) = x^3+5, p_3(x)=4x+1, p_4(x)=4x^3+4x+16$ vectores de $P^4$, el espacio vectorial real de funciones polinomiales de grado $n\le 4.$ ¿Cuáles son los tres conjuntos de tres vectores linealmente independientes que podemos formar a partir de estos cuatro vectores?  
    \item Da un ejemplo de un conjunto de tres vectores en $\mathbb{R}^3$ linealmente independientes y ortogonales entre sí, donde todas las entradas de dichos vectores sean distintas de cero.
    \item Describe con tus palabras y con los conceptos vistos en clase el procedimiento que seguiste para resolver el ejercicio 3.4.3.5.
    
\end{enumerate}

\end{document}
