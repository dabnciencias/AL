\documentclass[12pt,dvipsnames]{article}

\usepackage[margin=1.5cm]{geometry} 
\usepackage{amsmath,amsthm,amssymb}
\usepackage[spanish,es-tabla]{babel}
\decimalpoint
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage[dvipsnames]{xcolor}
\usepackage[hyphens]{url}
\usepackage{graphicx}
\graphicspath{ {images/} }
\usepackage{tcolorbox}
\usepackage{enumitem}
\setcounter{section}{0}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage[pdftex,
            pdfauthor={Diego Alberto Barceló Nieves},
            pdftitle={Curso de Álgebra Lineal (2022-I)}]{hyperref}
\usepackage{braket}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{caption}
\usepackage{mathrsfs}
\usepackage{subcaption}
\usepackage{subfiles}
\usepackage{mdframed}

\usetikzlibrary{babel}

\definecolor{NARANJA}{rgb}{1,0.467,0}
\definecolor{VERDE}{rgb}{0.31,1,0}
\definecolor{AZUL}{rgb}{0,0.53,1}
\definecolor{ROJO}{rgb}{1,0,0}
\definecolor{db}{rgb}{0.85,0.4,0}

\hypersetup{
    colorlinks=true,
    linkcolor=ROJO,
    filecolor=magenta,      
    urlcolor=AZUL,
}

\newtheorem{obs}{Observación}[section]
\newtheorem{lema}[obs]{Lema}
\newtheorem{prop}[obs]{Proposición}
\newtheorem{teo}[obs]{Teorema}
\newtheorem{coro}[obs]{Corolario}
 
\newtheoremstyle{dotlessP}{}{}{\color{db}}{}{\color{db}\bfseries}{.}{ }{}
\theoremstyle{dotlessP}
\newtheorem{ejer}{Ejercicio} 

\pgfplotsset{compat=1.15}
 
\begin{document}

\title{Curso de Álgebra Lineal \\ (2022-I)}
\author{Diego Alberto Barceló Nieves\\  Facultad de Ciencias \\ Universidad Nacional Autónoma de México}
\date{}
\maketitle

\iffalse
\section{Introducción}

La presente planeación del curso de Álgebra Lineal para la Licenciatura en Física Biomédica fue realizada tomando en cuenta el programa de la asignatura existente\footnote{El cual puede ser consultado en la página \url{http://www.fciencias.unam.mx/asignaturas/1330.pdf}.}, así como la retroalimentación del mismo por parte de estudiantes de la licenciatura.

%Al inicio del curso se procurará hacer un mayor énfasis en las estrechas relaciones existentes entre el álgebra lineal y el cálculo vectorial ya que, actualmente, se sugiere que las materias de Álgebra Lineal y Cálculo Avanzado se cursen simultáneamente. Además, durante todo el curso, se buscará sentar bases teóricas sólidas para diversas aplicaciones que serán útiles en materias posteriores del plan de estudios, tales como Ecuaciones Diferenciales (e.g., matrices, determinantes, eigenvalores y eigenvectores para aplicarse en sistemas de ecuaciones diferenciales), Matemáticas Avanzadas (e.g., bases ortogonales y ortonormales de funciones para ser aplicado en funciones especiales), Introducción a la Física Cuántica/Mecánica Cuántica (e.g., espacios de Hilbert, espacios duales y operadores hermitianos para aplicarse en el formalismo de la Mecánica Cuántica) e Imagenología Biomédica (e.g., matrices, transformaciones lineales y transformaciones rígidas para ser aplicado a procesamiento digital de imágenes).

Durante el curso, se sentarán bases teóricas sólidas de álgebra lineal para diversas aplicaciones que serán útiles en materias del plan de estudios, tales como Cálculo Avanzado, Ecuaciones Diferenciales, Matemáticas Avanzadas, Introducción a la Física Cuántica y Mecánica Cuántica, Algoritmos Computacionales y Física Computacional, entre otras.

\subsection*{Metolodogía de enseñanza}

 Este curso está diseñado para que l@s estudiantes aprendan álgebra lineal de manera mayormente autodidacta, a la vez que se les da acompañamiento en los temas del curso mediante sesiones cortas de asesoría en línea en grupos reducidos. Por lo tanto, la mayor parte del curso se llevará a cabo a través de actividades asíncronas.

 \vspace{3mm}
  Como plataforma para llevar el curso, utilizaremos la aplicación de mensajería instantánea Telegram -misma por donde publicaremos avisos, recursos didácticos (especificados más abajo), exámenes, etc. y atenderemos dudas de nuestr@s estudiantes fuera de las sesiones de videollamada- y, para las sesiones síncronas (videollamadas), utilizaremos Google Meet.

  \vspace{3mm}
Al inicio de cada semana, les indicaremos cuáles son las lecturas que deben realizar y los ejercicios que deben contestar durante la misma. Además, a lo largo de la semana les enviaremos recursos didácticos que complementen sus lecturas. Los días viernes nos reuniremos por videollamada en el horario de clase para resolver dudas sobre los ejercicios dejados. Al final de cada semana, les indicaremos las lecturas y ejercicios que deben resolver para la siguiente. Además, nos reuniremos algunos días miércoles (en promedio, cada dos semanas) por videollamada en el horario de clase para dar una exposición breve de algunos temas del módulo y para que nos pregunten sus dudas acerca de ellos.

\vspace{3mm}
Durante todo el curso, se procurará la interacción y colaboración entre estudiantes durante el proceso de aprendizaje.

\subsection*{Temario del curso por semana}

\textbf{Semana 1:}  Campos, espacios vectoriales, subespacios vectoriales.

\vspace{3mm}
\textbf{Semana 2:}  Producto escalar y norma, proyecciones y ortogonalidad.

\vspace{3mm}
\textbf{Semana 3:}  Combinaciones lineales, subespacio generado y conjunto generador, dependencia e independencia lineal.

\vspace{3mm}
\textbf{Semana 4:}  Bases y dimensión, ortogonalización y ortonormalización.

\vspace{3mm}
\textbf{Semana 5:}  Transformaciones lineales y sus representaciones matriciales.

\vspace{3mm}
\textbf{Semana 6:}  Composición de transformaciones lineales y multiplicación de matrices.

\vspace{3mm}
\textbf{Semana 7:}  Invertibilidad e isomorfismos.

\vspace{3mm}
\textbf{Semana 8:} Cambios de base.

\vspace{3mm}
\textbf{Semana 9:} El problema de la diagonalización, eigenvalores e eigenvectores, polinomio característico.

\vspace{3mm}
\textbf{Semana 10:} Diagonalizabilidad.

\vspace{3mm}
\textbf{Semana 11:} Sistemas lineales de ecuaciones diferenciales y aplicaciones en física y biología.

\vspace{3mm}
\textbf{Semana 12:} Repaso de diagonalizabilidad y diagonalización.

\vspace{3mm}
\textbf{Semana 13:} Funcionales y espacio dual, complemento ortogonal y proyecciones ortogonales.

\vspace{3mm}
\textbf{Semana 14:} Descomposición espectral (introducción) y operador adjunto.

\vspace{3mm}
\textbf{Semana 15:} Operadores normales, autoadjuntos y teorema espectral.

\vspace{3mm}
\textbf{Semana 16:} Operadores ortogonales y unitarios.

\vspace{3mm}
\textbf{Nota:} Debido al poco tiempo del que disponemos para el curso, \textbf{se asumirá que l@s estudiantes dominan los siguientes temas del} \href{https://web.fciencias.unam.mx/asignaturas/1130.pdf}{temario de Álgebra}: Matrices (definción y operaciones), matrices transpuestas, operaciones elementales y matrices elementales, rango de una matriz, matrices invertibles, cálculo de la inversa de una matriz invertible, determinante de una matriz cuadrada (definición y propiedades), cálculo de determinantes, soluciones de un sistema de ecuaciones lineales, sistemas de ecuaciones lineales homogéneos, espacio de soluciones de un sistema homogéneo, sistemas de ecuaciones lineales no homogéneos, criterios de existencia de soluciones y resolución de sistemas de ecuaciones lineales. Sin embargo, \textbf{no se asumirá conocimiento alguno del temario de Ecuaciones Diferenciales}.

\subsection*{Forma de evaluación}

 La evaluación se hará a través de 4 tareas-exámenes, que se realizarán cada cuarto viernes del curso. Las tareas-examen se realizarán y entregarán en equipos de pocas personas, que serán asignados por el profesor y cambiarán después de cada entrega; esto fomentará la amplia colaboración entre estudiantes, y permitirá que se le pueda brindar una atención más focalizada a cada equipo. Para asegurar que el trabajo sea colaborativo, en cada entrega se deberá indicar de forma clara las contribuciones que realizó cada miembro del equipo al trabajo entregado. La calificación que obtenga un equipo en una tarea-examen será la misma que se asigne a cada contribuyente para esa misma entrega. Quienes no contribuyan a su equipo en una entrega no tendrán derecho a la calificación correspondiente.

Además de las cuatro tareas-examen, se evaluará un trabajo final obligatorio (que puede realizarse de forma individual o en parejas) en el cual realicen una investigación sobre aplicaciones de los temas vistos durante el curso y, en su caso, realicen una implementación numérica (i.e., un programa) acerca de la aplicación que investigaron. Este trabajo final se promediará con las cuatro tareas-examen para obtener la calificación final del curso.

Al final del curso, quienes tengan un promedio reprobatorio en la materia deberán presentar un examen final de forma individual, que sustituirá sus calificaciones anteriores. Las personas con promedio aprobatorio también podrán optar por hacer el examen final para subir calificación; en ese caso, su calificación en el final reemplazará las anteriores.

\subsection*{Bibliografía recomendada para el curso} \label{Bibliografía}

\begin{itemize}
    \item S. H. Friedberg, \emph{Linear Algebra}, 4ta ed. (Pearson, 2014, EUA) - es el texto básico para este tipo de cursos.
    \item S. Lang, \emph{Linear Algebra}, 3a ed. (Springer, 1987, EUA) - buen complemento al Friedberg.
    \item D. Poole, \emph{Linear Algebra: A Modern Introduction}, 4ta ed. (Cengage Learning, 2015, EUA) - útil para quienes quieran ver algunas aplicaciones de los conceptos al mismo tiempo que los aprenden.
\end{itemize}{}

Les sugiero que hojeen \emph{todos} los libros recomendados al inicio del curso, y que consulten los de su agrado constantemente durante el mismo, o bien, busquen otros que les sirvan mejor para aprender.

\subsection*{Otros recursos educativos}

No sólo se aprende de libros; hay que aprovechar todo el contenido que ofrece el internet para nuestra educación. A lo largo de los apuntes pondré hipervínculos a algunas páginas con recursos relevantes para el tema en cuestión; sin embargo, aquí enlistaré algunos recursos útiles para aprender álgebra lineal:

\begin{itemize}
    \item Lista de reproducción \href{https://www.youtube.com/watch?v=fNk_zzaMoSs&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab}{Essence of Linear Algebra} del canal de YouTube 3Blue1Brown.
    \item Libros de texto interactivos \href{{http://immersivemath.com/ila/learnmore.html}}{Immersive Linear Algebra} e \href{https://textbooks.math.gatech.edu/ila/index2.html}{Interactive Linear Algebra}, que sirven para generar intuición acerca de algunos conceptos del álgebra lineal.
    \item Series de videos de sobre álgebra lineal de \href{https://www.lem.ma/books/AIApowDnjlDDQrp-uOZVow/landing}{lemma} y \href{https://www.khanacademy.org/math/linear-algebra}{Khan Academy} con interfaces para resolver ejercicios al final de cada lección.
\end{itemize}

Por supuesto, les invito a que busquen más recursos por su propia cuenta; de encontrarlos, les agradecería que me notificaran para revisarlos.
\fi

\newpage
\textbf{Notación}
\begin{tcolorbox} \label{Notación}
\centering
\begin{tabular}{cc}
    \\
    $\mathbf{u}, \mathbf{v}, \mathbf{w}, ...$ & vectores (elementos de un conjunto vectorial $V$) \\ \\
    $a,b,c, ...$ & escalares (elementos de un campo $K$ que define un espacio vectorial) \\ \\
    $ab$ & producto entre los escalares $a$ y $b$ \\ \\
    $a\mathbf{u}$ & producto del vector $\mathbf{u}$ por el escalar $a$\footnote{Algunos textos se refieren a esta operación \textemdash realizada entre un vector y un escalar, y que da como resultado un vector\textemdash\hspace{1.5mm} como \textit{multiplicación escalar} (o \emph{scalar multiplication}, en inglés); sin embargo, es fácil que esta operación se confunda con la de \textit{producto escalar}, que da como resultado un escalar. Debemos tener esto en mente cuando leamos otros textos de álgebra lineal, tanto en español como en inglés.} \\ \\
    $(x_1,...\hspace{0.5mm},x_n) $ & coordenada como n-tupla \\ \\
    $\begin{pmatrix}x_1&...&x_n\end{pmatrix}$ & vector como n-tupla \\ \\
    $V + W$ & suma de los espacios vectoriales $V$ y $W$ \\ \\
    $V \oplus W$ & suma directa de los espacios vectoriales $V$ y $W$ \\ \\

    $\langle\mathbf{u},\mathbf{v}\rangle$ & producto escalar entre los vectores $\mathbf{u}$ y $\mathbf{v}$ \\ \\
    $\overline{a}$ & complejo conjugado de $a$ \\ \\
    $u_i v_i \equiv \sum_{i=1}^n u_i v_i$ & \textit{notación de Einstein} para la suma sobre un índice $i$ \\ \\
    $||\mathbf{u}||$ & norma del vector $\mathbf{u}$ \\ \\
    $P_{\mathbf{u}}(\mathbf{v})$ & proyección del vector $\mathbf{v}$ sobre el vector $\mathbf{u}$ \\ \\
    $\mathbf{u}\perp\mathbf{v}$ & ortogonalidad de los vectores $\mathbf{u}$ y $\mathbf{v}$ \\ \\
    $\mathbf{a}\times\mathbf{b}$ & producto vectorial (cruz) de dos vectores $\mathbf{a},\mathbf{b}\in\mathbb{R}^3$ \\ \\
    $\mathbf{a}\cdot\mathbf{b}\times\mathbf{c}$ & triple producto escalar entre tres vectores $\mathbf{a},\mathbf{b},\mathbf{c}\in\mathbb{R}^3$. \\ \\


    $\langle G \rangle $ & Espacio vectorial generado por $G$ \\ \\
    $l.i.$ & Conjunto linealmente independiente \\ \\
    $l.d.$ & Conjunto linealmente dependiente \\ \\

    $\text{dim}(V)$ & Dimensión del espacio vectorial $V$ \\ \\

\end{tabular}
\end{tcolorbox}

\newpage
\subfile{s1}

\newpage
\subfile{s2}

\newpage
\subfile{s3}

\newpage
\subfile{s4}

\section{Transformaciones lineales y sus representaciones matriciales} \label{Sec:Transformaciones lineales y sus representaciones matriciales} 

\section{Composición de transformaciones lineales y multiplicación de matrices} \label{Sec: Composición de transformaciones lineales y multiplicación de matrices}

\section{Invertibilidad e isomorfismos} \label{Sec:Invertibilidad e isomorfismos}

\section{Matriz de cambio de base} \label{Sec:Matriz de cambio de base}

\section{Eigenvectores y eigenvalores} \label{Sec:Eigenvectores y eigenvalores} 

\section{Diagonalizabilidad} \label{Sec:Diagonalizabilidad} 

\newpage
\section{Introducción a sistemas lineales de ecuaciones diferenciales} \label{Sec: Sistemas lineales de ecuaciones diferenciales ordinarias} 

Esta pequeña sección de las notas tiene como finalidad introducir algunos de los conceptos que utilizaremos durante la segunda mitad del tercer módulo del curso, para el cual seguiremos principalmente el libro \emph{Linear Algebra: A Modern Introduction} de Poole (págs. 340-348).

\subsection*{Sistemas lineales de ecuaciones algebráicas}

Cuando hablamos de un \emph{sistema de ecuaciones}, generalmente nos referimos a un grupo de ecuaciones que deben cumplirse simultáneamente, por ejemplo, $$x_1 + x_2 = 1,$$ $$x_1 - x_2 = 5.$$ Para este sistema de ecuaciones, decimos que tenemos dos \emph{incógnitas}, $x_1$ y $x_2$, que queremos encontrar.

\vspace{3mm}
Como sabemos, podemos formar sistemas con más ecuaciones y/o incógnitas. En general, podemos expresar un sistema de $m$ ecuaciones y $n$ incógnitas como $$f_1(x_1,x_2,...,x_n)=0$$ $$f_2(x_1,x_2,...,x_n)=0$$ $$...$$ $$f_m(x_1,x_2,...,x_n)=0,$$ donde $f_j$, con $1\leq j\leq m$, es una función arbitraria de las incógnitas $x_i$, donde $1\leq i\leq n$. Decimos que tenemos una \emph{solución} al sistema cuando encontramos los valores de las incógnitas $x_i$ para los cuales todas las ecuaciones se verifican simultáneamente. Si reordenamos las ecuaciones del sistema que aparece al inicio de esta sección, podemos ver que en ese caso $f_1(x_1,x_2)=x_1+x_2-1$ y $f_2(x_1,x_2)=x_1-x_2-5$, y que la solución al sistema es $x_1=3, x_2=-2$. Observemos que $m=2$ y $n=2$ en este caso; precisamente por esto decimos que es un sistema de dos ecuaciones con dos incógnitas.

\vspace{3mm}
Por lo regular, trabajamos con sistemas de ecuaciones en donde las funciones $f_j$ son \emph{polinomios} en las variables $x_i$; a este tipo de ecuaciones se les llama \emph{algebráicas} y, por lo tanto, en esos casos decimos que tenemos un \emph{sistema de ecuaciones algebráicas}, como el ejemplo del inicio de esta sección.

\vspace{3mm}
Cuando en una ecuación algebráica las incógnitas no aparecen elevadas a una potencia mayor a uno, decimos que la ecuación es \emph{lineal}. Análogamente, cuando en un sistema de ecuaciones algebráicas las incógnitas no aparecen elevadas a una potencia mayor a uno \textemdash o, equivalentemente, cuando todos los polinomios $f_j$ son de grado uno\textemdash\hspace{1mm} tenemos un sistema de ecuaciones algebráicas lineales; generalmente, en estos casos decimos que tenemos un \emph{sistema lineal de ecuaciones algebráicas}.

\vspace{3mm}
Lo que caracteriza a los sistemas de ecuaciones algebráicas (de los cuales los sistemas lineales de ecuaciones algebráicas son un caso particular) es que sus soluciones pueden ser números enteros, racionales, irracionales o hasta complejos, pero siempre son números; es decir, las \emph{incógnitas} de este tipo de sistemas de ecuaciones \textemdash y, por tanto, sus soluciones\textemdash\hspace{1mm} son \emph{números}. Esta es la principal diferencia entre este tipo de sistemas y los sistemas de ecuaciones \emph{diferenciales}, que veremos más adelante. Antes de eso, tenemos que entender la diferencia entre las ecuaciones algebráicas y las ecuaciones diferenciales, lo cual haremos en la siguiente sección.

\newpage
\subsection*{Introducción a ecuaciones diferenciales (ordinarias)} \label{Subsec:Introducción a ecuaciones diferenciales ordinarias}

A diferencia de una ecuación algebráica, en una ecuación \emph{diferencial} la incógnita de la ecuación \textemdash y, por lo tanto, su solución\textemdash es una \emph{función}. Por ejemplo, la ecuación $$\dot{x} + x = 0,$$ donde $\dot{x}$ representa la derivada de la función $x$ con respecto a alguna variable independiente \textemdash usualmente llamada $t$, aunque esto es sólo una etiqueta\footnote{Con \emph{etiqueta} nos referimos a que, de haber postulado que la función $x$ dependía de una variable independiente $s$, entonces la solución a la ecuación diferencial sería $x(s)=e^{-s},$ lo cual no cambia en absoluto el sentido de la ecuación ni de su solución. A este tipo de variables ``etiqueta'' acostumbramos llamarles \emph{variables mudas}.}\textemdash, tiene como solución a la función $x(t)=e^{-t}$, y dicha solución es válida para todo $t\in\mathbb{R}$ ya que en todo este intervalo se verifica que\footnote{Observemos que pudimos haber planteado esta ecuación diferencial de manera equivalente como $\dot{x}=-x$, y la solución sería la misma, ya que $\frac{d}{dt}e^{-t}=-e^{-t}$ para todo $t\in\mathbb{R}$.} $$\frac{d }{dt}e^{-t} + e^{-t} = \bigg(\frac{d}{d(-t)}e^{-t}\bigg)\bigg(\frac{d }{dt}(-t)\bigg) + e^{-t} = \big(e^{-t}\big)\big(-1\big)+e^{-t} = -e^{-t}+e^{-t}= 0.$$

\vspace{3mm}
En general, las incógnitas de las ecuaciones diferenciales pueden ser funciones de más de una variable, y en la ecuación pueden aparecer dos o más de sus derivadas parciales; a este tipo de ecuaciones diferenciales les llamamos ecuaciones diferenciales \emph{parciales}. En cambio, cuando nuestra función incógnita depende de una sola variable, decimos que tenemos una ecuación diferencial \emph{ordinaria}; éstas últimas son con las cuales trabajaremos. A continuación, formalizamos algunas definiciones que utilizaremos.

\vspace{3mm}
\begin{tcolorbox}
\underline{Def.} Una \emph{ecuación diferencial ordinaria} es una relación entre una función incógnita $x(t)$, sus derivadas, y una variable independiente $t$. En general, podemos expresar una ecuación diferencial ordinaria como $$f\big(t,x(t),\dot{x}(t),\ddot{x}(t),...\hspace{0.5mm},x^{(n)}(t)\big)=0,$$ donde $f$ es una función arbitraria y $x^{(n)}(t)$ denota la $n$-ésima derivada de la función incógnita $x$ con respecto a su variable independiente $t$. Decimos que una función $x(t)$ es una \emph{solución} a la ecuación diferencial anterior en un intervalo $I$ si tanto $x(t)$ como sus derivadas cumplen dicha ecuación para todo $t\in I$.
\end{tcolorbox}

\vspace{2mm}
Podemos clasificar algunos tipos de ecuaciones diferenciales ordinarias, como sigue\footnote{Observen la analogía con la forma en que clasificamos ecuaciones algebráicas (polinomiales), pero también las diferencias. Las definiciones presentadas serán relevantes para nuestros fines.}:

\vspace{3mm}
\begin{tcolorbox}
\underline{Def.} Decimos que una ecuación diferencial ordinaria es:
\begin{itemize}
    \item \emph{de orden} $n$ si la derivada de mayor grado que aparece en la ecuación es de orden $n$;
    \item \emph{lineal} si se puede escribir de la forma $$a_n(t)x^{(n)}(t)+a_{n-1}(t)x^{(n-1)}(t)+...+a_1(t)\dot{x}(t)+a_0(t)x(t)+b(t)=0$$ para funciones $a_1(t),a_2(t),...\hspace{0.5mm},a_n(t),b(t)$ dadas (que pueden, en particular, ser constantes)\footnote{En particular, si $b(t)$ es la constante cero, decimos que la ecuación es lineal \textit{homogénea}.};
    \item \emph{autónoma} si en la ecuación no aparece la variable independiente $t$.
\end{itemize}
\end{tcolorbox}

\newpage
\subsection*{Ecuaciones diferenciales (ordinarias) con condiciones inciales y sistemas lineales de ecuaciones diferenciales} \label{Subsec:Ecuaciones diferenciales (ordinarias) con condiciones iniciales y sistemas lineales de ecuaciones diferenciales}

Con regularidad, nos encontramos con ecuaciones diferenciales que tienen más de una solución, e inclusive pueden tener una infinidad de ellas. Por ejemplo, regresando a la ecuación $$\dot{x} + x = 0,$$ podemos verificar que la función $x(t)=C e^{-t}$ es una solución en todo el intervalo $\mathbb{R}$ para cualquier $C\in\mathbb{R}$. Decimos entonces que $x(t)=Ce^{-t}$ es una \emph{solución general} a la ecuación diferencial anterior\footnote{Por dar otro ejemplo, podemos decir que la ecuación $\ddot{x}+x=0$ tiene como solución general a $x(t)=A\sin(t)+B\cos(t)$ en todo el intervalo $\mathbb{R}$ para cualesquiera valores $A,B\in\mathbb{R}$ (¡compruébalo!).}. En esta solución general, el valor $C$ representa un \emph{grado de libertad} de la solución, el cual puede removerse agregando una \emph{restricción} a la ecuación diferencial, la cual viene en forma de una \emph{condición inicial}.

\vspace{3mm}
Supongamos que nos piden resolver la ecuación diferencial anterior bajo la condición de que al evaluar la función en $t=0$ obtenemos un cierto valor $C_0\in\mathbb{R}$. Es decir que la solución $x(t)$, además de cumplir la ecuación diferencial, debe cumplir que $x(t)|_{t=0}=C_0.$ Partiendo de nuestra solución general, resolvemos ahora la última ecuación mostrada como sigue $$x(t)|_{t=0}=C_0\implies C e^{-t}|_{t=0}=C_0\implies C e^{0}=C_0\implies C(1)=C_0\implies C=C_0.$$ Por lo tanto, la solución a la ecuación diferencial $\dot{x}+x=0$ con condición inicial $x(0)=C_0$, donde $C_0\in\mathbb{R}$ es un valor determinado, es $x(t)=C_0 e^{-t}$. Ya que en este caso $x(t)$ sigue cumpliendo la ecuación diferencial original pero ya no tiene grados de libertad, decimos que es una \emph{solución particular} a la ecuación diferencial $\dot{x}+x=0$ sujeta a la \emph{condición inicial} $x(0)=C_0,$ con $C_0\in\mathbb{R}$.

\vspace{3mm}
\begin{tcolorbox}
\underline{Def.} Decimos que una función $x(t)$ es una \emph{solución particular} a una ecuación diferencial ordinaria de grado uno si, además de cumplir con dicha ecuación, cumple con la \emph{condición inicial} $$x(t_0)=C_0,$$ donde $C_0$ es un valor fijo.
\end{tcolorbox}

De forma análoga a como podemos formar sistemas de ecuaciones a partir de ecuaciones algebráicas, también podemos formarlos a partir de ecuaciones diferenciales. Por ejemplo, el sistema de ecuaciones diferenciales $$\dot{x_1} - x_2 = 0,$$ $$x_1 + \dot{x_2} = 0,$$ tiene como solución general a $x_1(t)=A\sin(t), x_2(t)=A\cos(t)$, válida en todo el intervalo $\mathbb{R}$ para cualquier $A\in\mathbb{R}$. Observemos que, en este sistema, ninguna de las incógnitas apareció elevada a una potencia mayor a uno. Por lo tanto, este sistema es un ejemplo de un \emph{sistema lineal de ecuaciones diferenciales ordinarias}.

\vspace{3mm}
\begin{tcolorbox}
\underline{Def.} Decimos que un sistema de ecuaciones diferenciales es un \emph{sistema lineal de ecuaciones diferenciales ordinarias} si todas las ecuaciones diferenciales ordinarias del sistema son lineales en todas sus incógnitas. Usualmente, se les abrevia como \emph{sistemas lineales de ecuaciones diferenciales}.
\end{tcolorbox}
\vspace{3mm}

Después de esta introducción, puedes continuar leyendo las págs. 273-274 del libro \emph{Linear Algebra} de Friedberg y 340-342 del libro \emph{Linear Algebra: A Modern Introduction} de Poole para ver cómo podemos aplicar herramientas de álgebra lineal (en particular, diagonalización) para resolver este tipo de sistemas. Si deseas repasar el material presente antes de continuar, te sugiero resolver los ejercicios al final de esta sección. Después, deberás resolver los ejercicios $59, 61$ y $63$ de la sección 4.6 del Poole.

\subsection*{Ejercicios de repaso}

\subsubsection*{Sistemas lineales de ecuaciones algebráicas}
\begin{enumerate}
    \item Realiza un cuadro comparativo con las definiciones de sistema de ecuaciones, sistema de ecuaciones algebráicas y sistema lineal de ecuaciones algebráicas. 
    \item Escribe o argumenta por qué no puedes escribir un ejemplo de:
        \begin{enumerate}[label=\alph*)]
        \item una ecuación \emph{trascendental} (i.e., no algebráica);
        \item un sistema no lineal de ecuaciones algebráicas;
        \item un sistema lineal de ecuaciones no algebráicas.
    \end{enumerate}
    \item Demuestra que en una ecuación lineal homogénea con más de una incógnita el conjunto de soluciones forma un espacio vectorial. Después, generaliza el resultado para demostrar que en un sistema lineal de ecuaciones algebráicas homogéneas con menos ecuaciones que incógnitas el conjunto de soluciones forma un espacio vectorial.
\end{enumerate}

\subsubsection*{Introducción a ecuaciones diferenciales (ordinarias)}
\begin{enumerate}
    \item Encuentra la solución general a la ecuación $\dot{x}-x=0$ en todo el intervalo $\mathbb{R}$.
    \item Sea $C^{\infty}(\mathbb{R})$ el conjunto de todas las funciones reales de variable real con derivadas de cualquier orden y sea $\frac{d}{dt}:C^{\infty}(\mathbb{R})\to C^{\infty}(\mathbb{R})$ el operador lineal \emph{derivada}. Supongamos que $x(t)$ es un eigenvector de $\frac{d}{dt}$ con eigenvalor $\lambda,$ ¿entonces quién es $x(t)$?
    \item Muestra que tanto la ecuación diferencial del ejemplo dado al inicio de la sección \ref{Subsec:Ecuaciones diferenciales (ordinarias) con condiciones iniciales y sistemas lineales de ecuaciones diferenciales} como su solución general son un caso particular del ejercicio anterior.
\end{enumerate}

\subsubsection*{Ecuaciones diferenciales (ordinarias) con condiciones iniciales}
\begin{enumerate}
    \item Encuentra la solución particular a la ecuación $\dot{x}+x=0$ con la condición inicial $x(0)=5$.
    \item Encuentra la solución particular a la ecuación $\dot{x}+x=0$ con la condición inicial $x(1)=5$.
    \item Muestra que cualquier solución particular (es decir, sin grados de libertad) de la ecuación $\ddot{x}=g$ debe tener dos condiciones iniciales: una para la función $x(t)$ y otra para $\dot{x}(t)$. ¿De qué ecuación se trata?
\end{enumerate}

\subsubsection*{Sistemas lineales de ecuaciones diferenciales (ordinarias)}
\begin{enumerate}
    \item Di si el siguiente sistema de ecuaciones diferenciales ordinarias es lineal, autónomo, ambos o ninguno, y explica por qué: $$\dot{x_1}+3x_2 = 0,$$ $$\dot{x_2} - x_3 = 0,$$ $$x_1-6\dot{x_3} = 0.$$
\end{enumerate}

\newpage
\section{Modelación con sistemas lineales de ecuaciones diferenciales} \label{Sec:Modelación con sistemas lineales de ecuaciones diferenciales} 
Para esta sección, seguiremos el libro \emph{Linear Algebra: A Modern Introduction} de Poole, págs. 342-348. Deberás realizar los ejercicios 65, 66, 67, 69, 71, 73 y 75 de la sección 4.6 del Poole, así como el ejercicio 15 de la sección 5.2 del Friedberg.

\newpage

\section{Funcionales y espacio dual, complemento ortogonal y proyecciones ortogonales} \label{Sec:13} 

Durante este último módulo del curso, nuestro principal objeto de estudio serán los espacios vectoriales con productos escalares, así como los operadores lineales que actúan sobre dichos espacios. Descubriremos que, con ayuda del producto escalar, podemos llegar a tener una comprensión profundamente geométrica sobre cómo actúan ciertos operadores en estos espacios \textemdash más allá de una simple descripción algrebráica dada por reglas de correspondencia. Este resultado es uno de los más importantes de todo el curso, y se le conoce como el \emph{Teorema espectral}. Dado que haremos uso frecuente del producto escalar  $\langle\cdot ,\cdot\rangle$ \textemdash principalmente, a partir de la sección \ref{Subsec:Correspondencia entre bras y kets}\textemdash \ en lo que resta del curso utilizaremos la llamada \emph{notación bra-ket}, también conocida como \emph{notación de Dirac}, por lo que se presenta a continuación.

\vspace{3mm}

\textbf{Notación de bra-ket (o de Dirac) para espacios vectoriales con producto escalar:}
\begin{tcolorbox}
    \centering
    \begin{tabular}{cc}
        \\
        $\ket{v}$ & vector o \emph{ket}  \\ \\
        $\braket{u|v}$ & producto escalar o \emph{bra-ket}  \\ \\
        $\bra{u}$ & vector dual (funcional) o \emph{bra} \\ \\
    \end{tabular}
\end{tcolorbox}

\noindent Los vectores duales (también conocidos como \emph{funcionales} serán definidos formalmente más adelante, con ayuda de esta notación. Una diferencia muy importante con respecto a la notación utilizada anteriormente es que en esta nueva notación, por razones que veremos más adelante, la entrada \emph{derecha} del producto escalar será lineal, mientras que la entrada \emph{izquierda} será antilineal. Es decir, que si $\ket{u},\ket{v}\in V$ y $\alpha\in K$, donde $(V,K)$ es un espacio vectorial, entonces en general \[
    \bra{u}\big (\ket{\alpha v}\big ) = \bra{u}\big(\alpha\ket{v}\big) = \alpha\braket{u|v}
\] \noindent y \[
\big (\bra{\alpha u}\big ) \ket{v} = \big( \overline{\alpha}\bra{u}\big)\ket{v}  = \overline{\alpha}\braket{u|v}.
\] De las ecuaciones anteriores podemos hacer las identificaciones $\ket{\alpha v}=\alpha\ket{v}$ y $\bra{\alpha u}= \overline{\alpha}\bra{u}$, las cuales utilizaremos ampliamente durante este módulo. 

Con esta notación podemos, por ejemplo, reescribir algunos resultados que ya conocemos sobre bases ortonormales de una manera mucho más sencilla, como se muestra a continuación\footnote{Asegúrate de entender bien esta notación y a qué resultados nos referimos antes de continuar leyendo el resto de las notas.}: sea $V$ un espacio vectorial de dimensión finita $n$ con base ortonormal $\beta=\{\ket{v_1},\ket{v_2},...,\ket{v_n}\},$ entonces para todo $\ket{v}\in V$ \[\ P_{\ket{v_i}}(\ket{v})=\braket{v_i|v},\] \[\ket{v}=\sum_{i=1}^n \braket{v_i|v}\ket{v_i},
\] \noindent y para todo operador lineal $T:V\to V$ tenemos que si \[A=[T]_{\beta} \implies A_{ij}=\braket{T(v_j)|v_i}
.\] 

\newpage
\subsection*{Funcionales y espacio dual} \label{Subsec:Funcionales_y_espacio_dual} 

Como hemos visto anteriormente, si $K$ es un campo y $V$ y $W$ son dos espacios vectoriales sobre $K$, entonces $\mathcal{L}(V,W)$ \textemdash el conjunto de todas las transformaciones lineales de $V$ a $W$\textemdash\hspace{0.5mm} es un espacio vectorial sobre $K$. En particular, como vimos a inicios del curso, $(K,K)$ también es un espacio vectorial, por lo cual podemos fijar nuestra atención en el espacio vectorial $\mathcal{L}(V,K)$. En esta sección veremos el importante papel que juega este espacio vectorial; principalmente, nos enfocaremos en los espacios vectoriales con producto escalar.

\vspace{3mm}
\begin{tcolorbox}
    \underline{Def.} Sea $(V,K)$ un espacio vectorial. Decimos que $\textbf{f}$ es un \emph{funcional lineal}, o simplemente un \emph{funcional}, si $\textbf{f}\in\mathcal{L}(V,K)$.

\vspace{3mm}
    \underline{Def.} Al espacio vectorial $\mathcal{L}(V,K)$ lo llamamos el \emph{espacio dual} de $V$, denotado por $V^*$. Por ello, a sus elementos también se les conoce como \emph{vectores duales}. 
\end{tcolorbox}

\vspace{3mm}
\textbf{Ejemplos de funcionales} \label{Ejem:Funcionales}

\vspace{3mm}
Sea $V$ un espacio vectorial de dimensión finita, $\beta=(\mathbf{b}_1,\mathbf{b}_2,...,\mathbf{b}_n)$ una base ordenada de $V$ y $[\hspace{0.5mm} \cdot\hspace{0.5mm}]_\beta$ el \emph{mapeo de coordenadas en la representación de la base ordenada} $\beta$ dado por $$[\mathbf{v}]_\beta=\begin{pmatrix} c_1&c_2&...&c_n \end{pmatrix}$$ para todo $\mathbf{v}\in V$, donde los coeficientes $c_i$ son tales que $\mathbf{v}=c_i \mathbf{b}_i$ en notación de Einstein. Si definimos a $\textbf{f}_i(\mathbf{v})=c_i$ para toda $1\le i\le n$, entonces $\textbf{f}_i$ es un funcional sobre $V$ conocido como la \emph{función de la i-ésima coordenada en la representación de la base ordenada} $\beta$. 

\vspace{3mm}
En los espacios vectoriales de matrices cuadradas $M_{n\times n}(K)$, donde $K$ es un campo, la traza $tr(\cdot):M_{n\times n}\to K$ y el determinante $det(\cdot):M_{n\times n}\to K$ son funcionales.

\vspace{3mm}
Sea $(V,K)$ un espacio vectorial con producto escalar y $\ket{v}\in V$ un vector arbitrario, entonces la proyección escalar sobre $\ket{v}$ dada (en notación de Dirac) por $$P_{\ket{{v}}}(\ket{u}) = \frac{\braket{v|u}}{||\ket{v}||}$$ para todo $\ket{u}\in V$ es un funcional sobre $V$. 

\newpage
\subsection*{Correspondencia entre bras y kets} \label{Subsec:Correspondencia entre bras y kets} 

Sea $(V,K)$ un espacio vectorial arbitrario con producto escalar $\braket{\cdot|\cdot}:V\times V\to K$ donde, siguiendo la notación bra-ket, la primera entrada es antilineal. Si elegimos a un vector arbitrario $\ket{v}\in V$ y lo fijamos en la entrada antilineal del producto escalar, podemos definir un funcional $\braket{v|\cdot}:V\to K$, el cual se puede escribir más sencillamente como $\bra{v}$. Ya que en notación bra-ket a los vectores $\ket{u}$ se les llama \emph{kets} y a los productos escalares $\braket{y|x}$, \emph{bra-kets} (proveniente del inglés \emph{brackets}), a este tipo de funcionales $\bra{v}$ se les conoce como \emph{bras}.

Dado que a partir de cualquier vector $\ket{v}\in V$ se puede definir un funcional $\bra{v}\in V^*$, existe una correspondencia natural entre \emph{bras} y \emph{kets}. Sin embargo, para entender bien esta correspondencia debemos observar un detalle crucial: si hacemos el producto de un ket $\ket{v}$ por un escalar $\alpha\in K$, obteniendo así el ket $\alpha\ket{v}$, entonces el bra correspondiente será $\overline{\alpha}\bra{v}$\footnote{En la notación que usábamos antes, esto se explica como que si tomamos al vector $\alpha\mathbf{v}$ y lo fijamos en la entrada antilineal del producto escalar, obtenemos el funcional $\langle\hspace{0.5mm}\cdot\hspace{0.5mm} , \alpha\mathbf{v}\rangle:V\to K$ que, como sabemos, es igual a $\overline{\alpha}\langle\hspace{0.5mm} \cdot\hspace{0.5mm} ,\mathbf{v}\rangle$.}. Es decir, la correspondencia entre bras (que son un tipo de vectores duales) y kets está dada por \[
    \alpha\ket{v} \leftrightarrow \overline{\alpha}\bra{v}
.\] 

De esta manera, si queremos hacer el producto escalar de un vector arbitrario $\alpha_1 \ket{u}$ con otro vector $\alpha_2 \ket{v}$, podemos simplemente unir el ket $\alpha_1 \ket{u}$ con el bra correspondiente al otro vector, que sería $\overline{\alpha_2}\ket{v}$, obteniendo \[
    \big(\overline{\alpha_2}\bra{v}\big)\big(\alpha_1\ket{u}\big) = \overline{\alpha_2}\alpha_1\braket{v|u}
.\] Una ventaja de esta notación es que \textemdash siempre y cuando recordemos bien la correspondencia entre bras y kets mostrada en el párrafo anterior\textemdash\hspace{0.5mm} ya no tenemos que preocuparnos por cómo sacar escalares del producto escalar.

Más generalmente, si $\{\ket{b_1},\ket{b_2},...,\ket{b_n}\}$ es una base de $V$, entonces la correspondencia entre bras de $V^*$ y kets de $V$ puede expresarse como \[
    \sum_{i=1}^n c_i \ket{b_i} \leftrightarrow \sum_{i=1}^n \overline{c_i}\bra{b_i}
.\]    

\newpage
\subsection*{Complemento ortogonal} \label{Subsec:Complemento_ortogonal} 

\begin{tcolorbox}
    \underline{Def.} Sea $V$ un espacio vectorial con producto escalar y $W\subseteq V$ un subespacio vectorial de $V$. Definimos al \emph{complemento ortogonal} de $W$ como \[
        W^{\perp}=\{\ket{x}\in V\mid \braket{x|w}=0\hspace{2mm}\forall\hspace{0.5mm} \ket{w}\in W\} 
    .\] 
\end{tcolorbox}

\noindent De la definición anterior podemos hacer varias observaciones:
\begin{itemize}
    \item Ya que para cualquier espacio vectorial $V$ con producto escalar el vector nulo es ortogonal a todos los vectores del espacio, tenemos que $\{\ket{0}\}^{\perp}=V$ y $V^{\perp}=\{\ket{0}\}.$
    \item Por la observación anterior de la ortogonalidad del vector nulo y las propiedades lineales del producto escalar, para todo subespacio vectorial $W\subseteq V$, $W^{\perp}$ también es un subespacio vectorial de $V$.
    \item Para cualquier subespacio vectorial $W\subseteq$ de V, $W\cap W^{\perp}=\{\ket{0}\}.$
\end{itemize}

\begin{lema} {13.3.1}
    Sea $V$ un espacio vectorial con producto esclar sobre un campo $K$, $W$ un subespacio vectorial de $V$ y $\beta=\{\ket{v_1}, \ket{v_2},..., \ket{v_k}\}$ una base de $W$, entonces $\ket{z}\in W^{\perp}$ si y sólo si $\braket{z|v_i}=0$ para toda $\ket{v_i}\in\beta.$

    \begin{proof}
        Sea $\ket{w}\in W$ un vector arbitrario. Como $\beta$ es base de $W$, existen coeficientes $c_i\in K$ tales que \[
            \ket{w}=\sum_{i=1}^k c_i\ket{v_i}
        .\] \noindent Sea $\ket{z}\in W^{\perp}$, entonces tenemos que $\braket{z|w}=0,$ pero esto ocurre si y sólo si \[
        \bra{z}\big(\sum_{i=1}^k c_i\ket{v_i}\big)\iff \sum_{i=1}^k c_i \braket{z|v_i}=0\iff \braket{z|v_i}=0
    \] \noindent para todo $\ket{v_i}\in\beta$, ya que los coeficientes $c_i$ son arbitrarios. 
    \end{proof}

\end{lema}

\begin{teo} {13.3.2}
    Sea $V$ un espacio vectorial con producto escalar y $W\subseteq V$ un subespacio vectorial de dimensión finita $k$, entonces para todo $\ket{y}\in V$ existen vectores únicos $\ket{u}\in W$ y $\ket{z}\in W^{\perp}$ tales que $\ket{y}=\ket{u}+\ket{z}.$ Además, si $\beta=\{\ket{v_1},\ket{v_2},...,\ket{v_k}\}$ es una base ortonormal de $W$, entonces $$\ket{u}=\sum_{i=1}^k \braket{v_i|y}\ket{v_i}.$$
    \begin{proof}
        Sea $\beta=\{\ket{v_1},\ket{v_2},...,\ket{v_k}\}$ una base ortonormal de $W$. Definimos \[
            \ket{u}=\sum_{i=1}^k\braket{v_i|y}\ket{v_i}\hspace{3mm}\text{y}\hspace{3mm}\ket{z}=\ket{y}-\ket{u}
        .\] \noindent Claramente $\ket{u}\in W$ y $\ket{y}=\ket{u}+\ket{z}.$ Además, observemos que \[
        \braket{z|u}=\big(\bra{y}-\bra{u}\big)\ket{u}=\braket{y|u}-\braket{u|u}=\bra{y}\bigg(\sum_{i=1}^k\braket{v_i|y}\ket{v_i}\bigg)-\bigg(\sum_{i=1}^k\braket{y|v_j}\bra{v_i}\bigg)\bigg(\sum_{j=1}^k \braket{v_j|y}\ket{v_j}\bigg)\] \[
    = \sum_{i=1}^k\braket{v_i|y}\braket{y|v_i}-\sum_{i=1}^k\sum_{j=1}^k\braket{y|v_i}\braket{v_j|y}\braket{v_i|v_j}=\sum_{i=1}^k\braket{v_i|y}\braket{y|v_i}-\sum_{i=1}^k\sum_{j=1}^k\braket{y|v_i}\braket{v_j|y}\delta_{ij}\] \[=\sum_{i=1}^k\braket{v_i|y}\braket{y|v_i}-\sum_{i=1}^k\braket{y|v_i}\braket{v_i|y}=0
,\] \noindent por lo que $\ket{z}\in W^{\perp}$. 

\vspace{3mm}
Para demostrar la unicidad de $\ket{u}$ y $\ket{z}$, supongamos que existen $\ket{u'}\in W$ y $\ket{z'}\in W^{\perp}$ tales que $\ket{y}=\ket{u'}+\ket{z'}$. Entonces, tenemos que \[
    \ket{y}=\ket{u'}+\ket{z'}=\ket{u}+\ket{z}\implies \ket{u}-\ket{u'}=\ket{z}-\ket{z'}
.\] \noindent Ya que el vector del lado izquierdo de la última ecuación es un elemento de $W$ y el del lado derecho, de $W^{\perp}$, tenemos que \[
\ket{u}-\ket{u'}=\ket{z}-\ket{z'}\in W\cap W^{\perp} \implies \ket{u}-\ket{u'}=\ket{0}=\ket{z}-\ket{z'}\implies \ket{u'}=\ket{u}\hspace{3mm} \text{y}\hspace{3mm} \ket{z'}=\ket{z}
.\] 
    \end{proof}
\end{teo}

\begin{teo} {13.3.3}
    Sea $S=\{\ket{v_1},\ket{v_2},...,\ket{v_k}\}$ un conjunto ortonormal de vectores de un espacio vectorial $V$ de dimensión finita $n$ con producto escalar. Entonces
    \begin{enumerate}[label=\alph*)]  
    \item $S$ se puede extender a una base ortonormal $\{\ket{v_1},\ket{v_2},...,\ket{v_k},\ket{v_{k+1}},...,\ket{v_n}\}$ de $V$.
    \item Usando la notación anterior, si $W=\langle S \rangle$, entonces $S_1=\{\ket{v_{k+1}},...,\ket{v_n}\}$ es base ortonormal de $W^{\perp}$. 
    \item Para cualquier subespacio $W \subseteq V, \text{dim}(V) = \text{dim}(W) + \text{dim}(W^{\perp}).$
    \end{enumerate}
    \begin{proof}
        \begin{enumerate}[label=\alph*)]
            \item Por el Teorema de reemplazamiento (ver sec. \ref{Subsubsec:Teo_de_reemplazamiento}), sabemos que podemos extender a $S$ para formar una base $S'=\{\ket{v_1},\ket{v_2},...,\ket{v_k},\ket{w_{k+1}},...,\ket{w_n}\}$ de $V$. Aplicando el Teorema de Gram-Schmidt, podemos obtener una base $\{\ket{v_1},\ket{v_2},...,\ket{v_k},\ket{v_{k+1}},...,\ket{v_n}\}$ de $V$.
            \item Observemos que, por la construcción de la base $\{\ket{v_1},\ket{v_2},...,\ket{v_k},\ket{v_{k+1}},...,\ket{v_n}\}$ de $V$, $\langle S_1 \rangle \subseteq W^{\perp}$ y $S_1$ es un conjunto linalmente independiente. Por último, observemos que para cualquier $\ket{v}\in V$, $$\ket{v}=\sum_{i=1}^n \braket{v_i|v}\ket{v_i}.$$ \noindent En particular, para todo $\ket{v}\in W^\perp$ tenemos que $\braket{v_i|v}=0$ para $1\le i\le k$. En este caso, \[
                \ket{v}=\sum_{i=k+1}^n\braket{v_i|v}\ket{v_i}=\sum_{i=k+1}^n c_i\ket{v_i}\implies \ket{v}\in \langle S_1 \rangle \implies W^\perp \subseteq \langle S_1 \rangle .\] \noindent Por otro lado, como $S_1 \subseteq W^\perp$ tenemos que $\langle S_1 \rangle \subseteq W^\perp$, por lo que $\langle S_1 \rangle = W^\perp.$ Por lo tanto, $S_1$ es base ortonormal de $W^\perp$. 
            \item Sea $W$ un subespacio vectorial de $V$. Como $V$ es de dimensión finita, entonces $W$ también lo es. Por ende, $W$ tiene una base ortonormal $\{\ket{v_1},\ket{v_2},...,\ket{v_k}\}.$ Por los incisos a) y b), tenemos que \[
                    \text{dim}(V) = n = k + (n-k) = \text{dim}(W) + \text{dim}(W^\perp).\]  
        \end{enumerate}
    \end{proof}
\end{teo}

\newpage
\subsection*{Proyecciones ortogonales}

En la sección \ref{Subsec:Interpretación geométrica del producto escalar} vimos que, en un espacio vectorial con producto escalar, podemos definir funciones de \emph{proyección vectorial} que toman un vector del espacio y devuelven la componente de ese vector a lo largo de un subespacio vectorial particular. Además, en el Teorema 4.3.1.1 (ver sec. \ref{Subsec:Ortogonalización y ortonormalización}) vimos cómo podemos descomponer un vector en sus componentes ortogonales de manera sencilla a partir de una base ortogonal u ortonormal. Ambos resultados reflejan la importancia que tienen los llamados \emph{operadores de proyección ortogonal} en este tipo de espacios vectoriales.

\begin{tcolorbox}
    \underline{Def.} Sea $V$ un espacio vectorial arbitrario. Decimos que un operador lineal $P:V\to V$ es un \emph{operador de proyección} si \[
        P(P(\ket{v}))=P(\ket{v}) \hspace{3mm} \forall \ \ket{v}\in V.
    \] Definiendo $P^2:=P\circ P$ podemos reescribir esta condición simplemente como $P^2=P$. En particular, si $V$ tiene producto escalar y $P$ es tal que \[
        \text{Im}(P)^\perp=\text{Ker}(P)\hspace{3mm}\text{y}\hspace{3mm}\text{Ker}(P)^\perp=\text{Im}(P),
    \] decimos que $P$ es un \emph{operador de proyección ortogonal}.
\end{tcolorbox}

Observemos que la condición $P^2=P$ para un operador de proyección $P$ en un espacio vectorial $V$ codifica algebráicamente la noción intuitiva de que, después de proyectar un vector arbitrario $\ket{v}$ en un subespacio de $V$ y obtener la componente de $\ket{v}$ en dicho subespacio, proyectar esa componente \emph{en el mismo subespacio} no le hará nada \textemdash ya que se encuentra totalmente dentro de ese subespacio.

\vspace{3mm}
Por otro lado, a pesar de que no sea fácil de ver directamente de la definición, cualquier operador de proyección actuando sobre un espacio vectorial de dimensión finita lo divide en dos subespacios vectoriales ``ajenos'' (excepto por el vector nulo, por supuesto) a través de su imagen (el subespacio sobre el cual proyectamos) y su núcleo (el conjunto de vectores que no tienen componentes en dicho subespacio). Dicho de otra forma, cualquier operador de proyección actuando sobre un espacio vectorial de dimensión finita descompone al espacio como suma directa de su imagen y su núcleo (¿Se te ocurre por qué\footnote{Pista: demuestra que, si $V$ es un espacio vectorial de dimensión finita y $T:V\to V$ es un operador lineal, entonces $\text{Ker}(T)+\text{Im}(T)=V \iff \text{Ker}(T)\cap\text{Im}(T)=\{\ket{0}\}$.}?). 

\vspace{3mm}
Para cualquier espacio vectorial $V$, el operador nulo y el operador identidad son ejemplos triviales de operadores de proyección. Si $V$ tiene producto escalar, entonces la función de proyección vectorial sobre $\ket{v}$ es un operador de proyección ortogonal para todo $\ket{v}\in V$ no nulo; en términos más precisos, este operador proyecta sobre el subespacio $\langle \ket{v} \rangle$ de dimensión uno. Observemos que podemos escribir a este operador en términos de bras y kets simplemente como $\frac{\ket{v}\bra{v}}{\braket{v|v}}:V\to V$, de donde resulta fácil demostrar que efectivamente es un operador de proyección, ya que \[
    \bigg( \frac{\ket{v}\bra{v}}{\braket{v|v}}\bigg)^2=\bigg(\frac{1}{\braket{v|v}}\ket{v}\bra{v}\bigg)^2=\frac{1}{\braket{v|v}^2}\big(\ket{v}\bra{v}\big)^2=\frac{1}{\braket{v|v}^2}\ket{v}\braket{v|v}\bra{v}=\frac{\braket{v|v}}{\braket{v|v}^2}\ket{v}\bra{v}=\frac{\ket{v}\bra{v}}{\braket{v|v}}.
\] Además, si en particular $\ket{v}$ es un vector normal, la expresión se simplifica a $\ket{v}\bra{v}$, de donde se ve directamente que $(\ket{v}\bra{v})^2=\ket{v}\braket{v|v}\bra{v}=\ket{v}\bra{v}$. Por lo tanto, todo vector normal $\ket{v}$ en un espacio vectorial con producto escalar define un operador de proyección ortogonal, dado por $\ket{v}\bra{v}$, y dicho operador proyecta a los vectores de todo el espacio sobre el subespacio generado por $\ket{v}$.

\newpage
\begin{tcolorbox}
\begin{center}
    \textbf{Nota aclaratoria: \emph{Sobre bras, kets, y bra-kets...}}
\end{center}

\hspace{3mm} Como hemos visto en esta sección, en un espacio vectorial $(V,K)$ los \emph{kets} $\ket{x},\ket{y},\ket{z}$, etc. son simplemente vectores de $V$, mientras que los \emph{bras} $\bra{x},\bra{y},\bra{z}$, etc. son funcionales sobre $V$, es decir, transformaciones lineales que van de $V$ a $K$.

\vspace{5mm}
\hspace{3mm} A pesar de que exista la correspondencia entre kets $\alpha\ket{x}\in V$ y bras $\overline{\alpha}\bra{x}\in V^*$ descrita en la sección \ref{Subsec:Correspondencia entre bras y kets}, debemos remarcar una diferencia sutil pero importante entre la operación de aplicarle un funcional a un vector y la operación de obtener el producto escalar entre dos vectores antes de pasar a la siguiente sección.

\vspace{5mm}
\hspace{3mm} Si $\ket{x}\in V$ y $\bra{y}\in V^*$ entonces, como $\bra{y}:V\to K$, tenemos que $$\bra{y}(\ket{x}) = \braket{y|x},$$ donde $\bra{y}$ es el vector dual correspondiente al vector $\ket{y}$. La observación crucial es esta: del lado izquierdo de la ecuación estamos aplicándole el funcional $\bra{y}$ al vector $\ket{x}$, mientras que del lado derecho estamos obteniendo el producto escalar entre el vector $\ket{x}$ (en la entrada lineal) y el vector $\ket{y}$ (en la entrada antilineal)\footnote{Inclusive l@s más atrevid@s entre ustedes se podrían animar a agregar una equivalencia más a la ecuación anterior: $\braket{y|x}=(\bra{y})\ket{x}$; es decir, podrían entretener la idea de que $\ket{x}$ sea ahora un \emph{funcional} que actúa sobre los vectores duales $\bra{y}\in V^*$ por la derecha; esto está relacionado con el hecho de que, si $V$ es un espacio de dimensión finita, entonces $(V^*)^*$ es isomorfo a $V$ \textemdash aunque no lo demostraremos en este curso.}; ambas operaciones involucran objetos distintos \textemdash la primera se realiza entre un funcional ($\bra{y}$) y un ket ($\ket{x}$), mientras que la segunda se realiza entre dos kets ($\ket{x}$ y $\ket{y}$)\textemdash\hspace{1mm} y, sin embargo, dan el mismo escalar como resultado.

\vspace{5mm}

\end{tcolorbox}


\subsection*{Ejercicios de repaso} \label{}

\subsubsection*{Funcionales y espacio dual} \label{Ejer:Funcionales_y_espacio_dual}
\begin{enumerate}
    \item Demuestra que si $V$ es un espacio vectorial de dimensión finita, entonces $\text{dim}(V^*)=\text{dim}(V)$.
    \item Consideremos el espacio de funciones reales de variable real continuas definidas en el intervalo $[0,2\pi]$, que denotaremos como $C([0,2\pi],\mathbb{R})$. Sean \[
            F_{1,n}(f):=\int_0^{2\pi} \sin(nx)f(x) \ dx, \hspace{3mm} F_{2,n}(f):=\int_0^{2\pi} \cos(nx)f(x) \ dx \hspace{5mm} \forall \ f\in C([0,2\pi],\mathbb{R}), n\in\mathbb{N}.
        \] Demuestra que $F_{1,n}$ y $F_{2,n}$ son funcionales de $C([0,2\pi],\mathbb{R})$ para todo $n\ge 0$. Los valores de estos funcionales para una función $g$ en particular se conocen como los \emph{n-ésimos coeficientes de Fourier}\footnote{Estos coeficientes están relacionados con una rama de las matemáticas llamada \emph{Análisis de Fourier}, la cual tiene diversas aplicaciones en la solución de ecuaciones diferenciales parciales, muchas de las cuales son utilizadas para modelar procesos físicos (como la ecuación de calor o la ecuación de onda) o biológicos.} de la función $g$.
    \item Calcula los primeros 3 coeficientes de Fourier para las funciones $\sin(x)$, $f(x)=x$ y $g(x)=x^2$.
\end{enumerate}

\subsubsection*{Correspondencia entre bras y kets} \label{Ejer:Correspondencia_entre_bras_y_kets}
\begin{enumerate}
    \item Sea $(V,K)$ un espacio vectorial con producto escalar. Demuestra que el conjunto de todos los bras correspondientes a los vectores de $V$ forma un subespacio vectorial de $V^*$.
    \item Demuestra que el subespacio de $V^*$ obtenido en el ejercicio anterior es isomorfo a $V$. En otras palabras, demuestra que la correspondencia entre bras y kets dada en la sección \ref{Subsec:Correspondencia entre bras y kets} es un isomorfismo.
\end{enumerate}

\subsubsection*{Complemento ortogonal} \label{Ejer:Complemento_ortogonal}
\begin{enumerate}
    \item Demuestra que para todo subespacio vectorial $W$ de un espacio vectorial $V$ de dimensión finita $n$ y con producto escalar, $V$ es igual a una suma directa de $W$ y $W^{\perp}.$ ¿Por qué es importante que $V$ tenga dimensión finita? (Nota: tendrás que demostrar las observaciones hechas al principio de la sec. \ref{Subsec:Complemento_ortogonal}.)
\end{enumerate}

\subsubsection*{Proyecciones ortogonales} \label{Ejer:Proyecciones_ortogonales}
\begin{enumerate}
    \item Sea $V$ un espacio vectorial. Demuestra que un operador lineal $P:V\to V$ es un operador de proyección si y sólo si $V=\text{Ker}(P)\oplus\text{Im}(P)$.
    \item Sea $V$ un espacio vectorial con producto escalar. Demuestra que un operador lineal $P:V\to V$ es un operador de proyección ortogonal si y sólo si $V=\text{Ker}(P)^\perp\oplus\text{Im}(P)^\perp$.
    \item Sea $V$ un espacio vectorial de dimensión finta $n$ con producto escalar y sea $\{\ket{v_1},\ket{v_2},... \ ,\ket{v_k}\}$ un conjunto ortogonal de vectores de $V$ con $k\le n$. Demuestra que $\sum_{i=1}^k\frac{\ket{v_i}\bra{v_i}}{\braket{v_i|v_i}}:V\to V$ es un operador de proyección ortogonal.
\end{enumerate}

\newpage
\section{Descomposición espectral y operadores adjuntos} \label{Sec:14} 


Supongamos que tenemos un espacio vectorial $(V,K)$ de dimensión finita $n$ con producto escalar. Entonces, siguiendo del último ejercicio de la sección \ref{Sec:13}, tendríamos que si $\{\ket{b_1},\ket{b_2},... \ ,\ket{b_n}\}$ es una base ortogonal de $V$, entonces $\sum_{i=1}^n \frac{\ket{b_i}\bra{b_i}}{\braket{b_i|b_i}}:V\to V$ es un operador de proyección ortogonal\footnote{Este operador no depende de la base ortogonal elegida... ¿puedes adivinar de qué operador se trata?}. En particular, para cualquier base ortonormal $\{\ket{u_1},\ket{u_2},... \ ,\ket{u_n}\}$ de $V$ tenemos que $\sum_{i=1}^n\ket{u_i}\bra{u_i}$ es un operador de proyección ortogonal (equivalente al anterior, para cualquier base ortonormal arbitraria).

\vspace{3mm}
Ambos operadores anteriores no son más que el operador identidad en $V$, lo cual podemos ver en abstracto aplicándolos a un vector arbitrario $\ket{v}\in V$ o, de forma matricial, representándolos en las mismas bases que utilizamos para construirlos. A continuación, veremos cómo esta forma de entender al operador identidad (como suma de operadores de proyección más pequeños) en espacios vectoriales de dimensión finita con producto escalar nos ayuda a entender cómo actúan ciertos tipos de operadores lineales de forma geométrica. 

\subsection*{Descomposición espectral (introducción)} \label{Subsec:Descomposición espectral (introducción)}

Sean $V$ un espacio vectorial de dimensión finita $n$ con producto escalar y $T:V\to V$ un operador lineal diagonalizable tal que sus eigenvectores $\ket{g_1},\ket{g_2},... \ ,\ket{g_n}$ forman una base\footnote{Recuerdemos que un operador $T:V\to V$ es diagonalizable si y sólo si se puede formar una base de $V$ compuesta de eigenvectores de $T$. Si $V$ es dimensión $n$, entonces cualquier base de $V$ tiene exactamente $n$ elementos, por lo que $T$ debe tener $n$ eigenvectores linealmente independientes; sin embargo, esto no necesariamente implica que $T$ tenga $n$ eigenvalores distintos, como hemos visto. Por lo tanto, si $T$ tiene eigenvalores $\lambda_1,\lambda_2,... \ ,\lambda_k$ con $k<n$, necesariamente habrá elementos distintos de la eigenbase que tengan el mismo eigenvalor y, en este caso, la correspondencia entre los subíndices de los $k$ eigenvalores distintos y los $n$ elementos de la eigenbase no será uno a uno, por lo que hay que tener cuidado y no dejarse llevar por la notación.} \emph{ortogonal} de $V$; es decir, que se puede elegir a una base ortogonal de $V$ compuesta por eigenvectores de $T$. Supongamos, adicionalmente, que normalizamos a cada uno de los vectores de la eigenbase y los redefinimos de tal forma que $\{\ket{g_1},\ket{g_2},... \ ,\ket{g_n}\}$ sea una base \emph{ortonormal} de $V$ compuesta por eigenvectores de $T$. En ese caso, tendríamos que \[
    I_V = \sum_{i=1}^n \ket{g_i}\bra{g_i}.
\] 

\vspace{3mm}
Sean $\lambda_1,\lambda_2,... \ ,\lambda_n$ los eigenvalores (no necesariamente todos distintos entre sí) de $T$ correspondientes a los eigenvectores normales $\ket{g_1},\ket{g_2},... \ ,\ket{g_n}$. Consideremos ahora al operador $T':=\sum_{i=1}^n \lambda_i \ket{g_i}\bra{g_i}$. En particular, observemos que para todo elemento $\ket{g_j}$ de la eigenbase tenemos que \[
    T'(\ket{g_j})=\sum_{i=1}^n \lambda_i \ket{g_i}\bra{g_i}(\ket{g_j})=\sum_{i=1}^n \lambda_i \ket{g_i}\braket{g_i|g_j}=\sum_{i=1}^n \lambda_i \ket{g_i}\delta_{ij}=\lambda_j \ket{g_j},
\] por lo que $T'$ tiene los mismos vectores y valores \emph{característicos} que $T$.  Ya que $T$ es diagonalizable y, por lo tanto, podemos formar una base de $V$ compuesta de sus eigenvectores (por ejemplo, $\{\ket{g_1},\ket{g_2},... \ ,\ket{g_n}$\}), se sigue que $T'=T$ y que, por ende, \[
T = \sum_{i=1}^n \lambda_i \ket{g_i}\bra{g_i},
\] donde $\{\ket{g_1},\ket{g_2},... \ ,\ket{g_n}\}$ es una base ortogonnal de $V$ compuesta de eigenvectores de $T$ normalizados y $\lambda_1,\lambda_2,... \ ,\lambda_n$ son los eigenvalores correspondientes (no necesariamente todos distintos entre sí). Esta expresión se conoce como la \emph{descomposición espectral} de $T$.

\vspace{3mm}
Observemos que la descomposición espectral de $T$ nos da información completa de cómo actúa $T$ en $V$ geométricamente: la expresión anterior nos dice que $T$ proyecta a un vector arbitrario $\ket{v}\in V$ en cada una de sus componentes a lo largo de los diferentes eigenespacios de $T$ (a través de las proyecciones ortogonales $\ket{g_i}\bra{g_i}$) \textemdash los cuales son ortogonales entre sí\textemdash \ , luego multiplica cada una de esas componentes por el eigenvalor correspondiente al eigenespacio en el que se encuentran, y luego reconstruye el vector resultante a partir de las componentes reescaladas\footnote{Aquí utilizamos la palabra ``reescalamiento'' como sinónimo de ''producto de un vector por un reescalar''; recordemos que, geométricamente, en general esta operación puede involucrar un reescalamiento y una rotación.}.

\vspace{3mm}
En otras palabras, hemos visto que si $T$ es un operador lineal sobre un espacio vectorial $V$ de dimensión finita con producto escalar y cumple que se pueda formar una base ortogonal u ortonormal de $V$ a partir de sus eigenvectores o, equivalentemente, que $$V=E_{\lambda_1}\oplus E_{\lambda_2}\oplus...\oplus E_{\lambda_k}\hspace{3mm}\text{y}\hspace{3mm}E_{\lambda_i}^\perp=\oplus_{j\neq i}E_{\lambda_j} \ \ \forall \ i\in \{1,2,... \ ,n\},$$ donde $\lambda_1, \lambda_2, ... \ , \lambda_k$ son los eigenvalores de $T$, entonces podemos entender a $T$ de una forma extremadamente sencilla: a través de proyecciones (en los eigenespacios de $T$, que son ortogonales entre sí) y reescalamientos (por los eigenvalores de $T$). Un ejemplo de esto se muestra en la Figura \ref{fig:8}.

\begin{figure}[h!]
    \centering
\begin{tikzpicture}[thick,scale=1, every node/.style={scale=0.9}]
    \draw[thick,<->] (-3,0) -- (3,0);
    \draw[thick,<->] (0,-3) -- (0,3);
    \draw[step=1cm,gray,very thin,dashed] (-3,-3) grid (3,3);
    \foreach \x in {-2,-1,0,1,2}
        \draw (\x cm, 1pt) -- (\x cm, -1pt);
    \foreach \y in {-2,-1,0,1,2}
        \draw (1pt, \y cm) -- (-1pt, \y cm);
    \draw[AZUL,dotted,thin] (-3,-3) -- (3,3) node[] at (-1.5,-2) {$E_{\lambda_1}$};
    \draw[ROJO,dotted,thin] (3,-3) -- (-3,3) node[] at (1.5,-2) {$E_{\lambda_2}$};
    \draw[darkgray,->] (0,0) -- (0.5,2) node[] at (0.5,2.5) {$\ket{v}$};
    \draw[AZUL,->] (0,0) -- (-0.707,-0.707) node[] at (-1.2,-0.7) {$\ket{g_1}$};
    \draw[ROJO,->] (0,0) -- (0.707,-0.707) node[] at (1.2,-0.7) {$\ket{g_2}$};
    \draw[magenta,->] (0,0) -- (-2.75,0.25) node[] at (-1.5,0.5) {$T(\ket{v})$};
    \draw[] node[] at (2.8,2.8) {\textbf{a)}};
\end{tikzpicture}
\hspace{3mm}
\begin{tikzpicture}[thick,scale=1, every node/.style={scale=0.9}]
    \draw[thick,<->] (-3,0) -- (3,0);
    \draw[thick,<->] (0,-3) -- (0,3);
    \draw[step=1cm,gray,very thin,dashed] (-3,-3) grid (3,3);
    \foreach \x in {-2,-1,0,1,2}
        \draw (\x cm, 1pt) -- (\x cm, -1pt);
    \foreach \y in {-2,-1,0,1,2}
        \draw (1pt, \y cm) -- (-1pt, \y cm);
        \draw[AZUL,->] (0,0) -- (1.25,1.25) node[] at (2,0.5) {$\ket{g_1}\braket{g_1|v}$};
        \draw[ROJO,->] (0,0) -- (-0.75,0.75) node[] at (-2,0.5) {$\ket{g_2}\braket{g_2|v}$};
        \draw[AZUL,dashed,very thin] (-0.75,0.75) -- (0.5,2);
        \draw[ROJO,dashed,very thin] (1.25,1.25) -- (0.5,2);
        \draw[darkgray,->] (0,0) -- (0.5,2) node[] at (0.5,2.5) {$\ket{v}$};
    \draw[] node[] at (2.8,2.8) {\textbf{b)}};
\end{tikzpicture}

\vspace{5mm}

\begin{tikzpicture}[thick,scale=1, every node/.style={scale=0.9}]
    \draw[thick,<->] (-3,0) -- (3,0);
    \draw[thick,<->] (0,-3) -- (0,3);
    \draw[step=1cm,gray,very thin,dashed] (-3,-3) grid (3,3);
    \foreach \x in {-2,-1,0,1,2}
        \draw (\x cm, 1pt) -- (\x cm, -1pt);
    \foreach \y in {-2,-1,0,1,2}
        \draw (1pt, \y cm) -- (-1pt, \y cm);
        \draw[AZUL,->] (0,0) -- (-1.25,-1.25) node[] at (-1.5,-1.55) {$\lambda_1\ket{g_1}\braket{g_1|v}$};
        \draw[ROJO,->] (0,0) -- (-1.5,1.5) node[] at (-1.55,2) {$\lambda_2\ket{g_2}\braket{g_2|v}$};
    \draw[] node[] at (2.8,2.8) {\textbf{c)}};
\end{tikzpicture}
\hspace{3mm}
\begin{tikzpicture}[thick,scale=1, every node/.style={scale=0.9}]
    \draw[thick,<->] (-3,0) -- (3,0);
    \draw[thick,<->] (0,-3) -- (0,3);
    \draw[step=1cm,gray,very thin,dashed] (-3,-3) grid (3,3);
    \foreach \x in {-2,-1,0,1,2}
        \draw (\x cm, 1pt) -- (\x cm, -1pt);
    \foreach \y in {-2,-1,0,1,2}
        \draw (1pt, \y cm) -- (-1pt, \y cm);
        \draw[AZUL,->] (0,0) -- (-1.25,-1.25) node[] at (-1.5,-1.55) {$\lambda_1\ket{g_1}\braket{g_1|v}$};
        \draw[ROJO,->] (0,0) -- (-1.5,1.5) node[] at (-1.55,2) {$\lambda_2\ket{g_2}\braket{g_2|v}$};
        \draw[AZUL,dashed,thin] (-1.5,1.5) -- (-2.75,0.25);
        \draw[ROJO,dashed,thin] (-1.25,-1.25) -- (-2.75,0.25);
        \draw[magenta,->] (0,0) -- (-2.75,0.25) node[] at (-1.5,0.5) {$T(\ket{v})$};
    \draw[] node[] at (2.8,2.8) {\textbf{d)}};
\end{tikzpicture}
\caption{Aplicación de un operador $T:\mathbb{R}^2\to \mathbb{R}^2$ a un vector $\ket{v}$ mediante descomposición espectral. En la subfigura \textbf{a)} se muestra a $\ket{v}$ y a su imagen $T(\ket{v})$, junto con los eigenvectores $\ket{g_1}$ y $\ket{g_2}$ de $T$, que forman una base ortonormal; los eigenespacios correspondientes se representan con líneas punteadas. En \textbf{b)} se muestran las proyecciones ortogonales de $\ket{v}$ en cada eigenespacio de $T$. En \textbf{c)} se muestran las componentes anteriores reescaladas por los eigenvalores correspondientes al eigenespacio en que se encuentran. Finalmente, en \textbf{d)} vemos que la suma de estas componentes reescaladas es igual a $T(\ket{v})$.}
    \label{fig:8}
\end{figure}

\vspace{3mm}
Las condiciones descritas en el párrafo anterior en general no son fáciles de verificar, por lo que nos gustaría tener algún criterio equivalente que sea más sencillo de aplicar; de cierta manera, de eso se tratará el resto del curso. Empezando nuestra búsqueda por hipótesis más precisas que deban cumplir operadores lineales sobre espacios vectoriales de dimensión finita con producto escalar para que les podamos aplicar la descomposición espectral, nuestra primera pista se encuentra en los operadores adjuntos, que definiremos y estudiaremos en la siguiente sección.

\subsection*{Operadores adjuntos}

Como vimos en la sección \ref{Subsec:Correspondencia entre bras y kets}, en cualquier espacio vectorial $(V,K)$ con producto escalar podemos tomar cualquier vector $\ket{y}\in V$ y definir un funcional $\textbf{g}:V\to K$ como \[
    \textbf{g}(\ket{x})=\braket{y|x}
,\] que podemos escribir simplemente como el bra $\bra{y}$.

Lo interesante de este tipo de funcionales es que, en espacios vectoriales de dimensión finita, todas las transformaciones lineales de $V$ a $K$ son de esta forma; es decir, podemos hacer una identificación entre las transformaciones lineales $\textbf{g}\in\mathcal{L}(V,K)$ y los bras $\bra{y}$, como demostraremos a continuación.

\begin{teo} {14.1}
    Sea $(V,K)$ un espacio vectorial de dimensión finita con producto escalar y sea $\textbf{g}:V\to K$ un funcional. Entonces existe un único $\bra{y}\in V^*$ tal que $\textbf{g}(\ket{x})=\braket{y|x}$ para todo $\ket{x}\in V$.

    \begin{proof}
        Sea $\beta=\{\ket{b_1},\ket{b_2},...,\ket{b_n}\}$ una base ortonormal de $V$ y sea \[
            \ket{y}=\sum_{i=1}^n \overline{\textbf{g}(\ket{b_i})}\ket{b_i} 
            .\] Entonces el vector dual correspondiente a $\ket{y}\in V$ es el bra $\bra{y}\in V^*$ dado por \[
            \bra{y}=\sum_{i=1}^n\textbf{g}(\ket{b_i})\bra{b_i}
        .\]  

        Por otro lado, definamos a $\textbf{h}:V\to K$ como $\textbf{h} (\ket{x})=\braket{y|x}$ para todo $\ket{x}\in V$. Claramente, $\textbf{h}$ es una transformación lineal. Observemos que para todos los elementos de la base $\beta$, sus imágenes bajo $\textbf{g}$ y $\textbf{h}$ coinciden; es decir, para todo $1\leq j\leq n$ tenemos que \[
            \textbf{h}(\ket{b_j})=\braket{y|b_j}=\bigg(\sum_{i=1}^n \textbf{g}(\ket{b_i})\bra{b_i}\bigg)\ket{b_j}=\sum_{i=1}^n \textbf{g}(\ket{b_i}) \braket{b_i|b_j}=\sum_{i=1}^n \textbf{g}(\ket{b_i})\delta_{ij}=\textbf{g}(\ket{b_j})
        .\] 

        Dado que $\textbf{g}$ y $\textbf{h}$ coinciden para todos los elementos de la base y son transformaciones lineales, entonces coinciden para todos los vectores $\ket{v}\in V$. Por ende, $\textbf{g}=\textbf{h}$. 

        Para demostrar la unicidad de $\bra{y}$, supongamos que $\textbf{g}(\ket{x})=\braket{y'|x}$ para todo $\ket{x}\in V$. Entonces tenemos que $$\braket{y|x}=\braket{y'|x} \implies \braket{y|x}-\braket{y'|x}=0 \implies \big(\bra{y}-\bra{y'}\big)\ket{x}=0$$ para todo $\ket{x}\in V$, pero esto se cumple si y sólo si $$\bra{y}-\bra{y'}=\bra{0}\implies \bra{y}=\bra{y'}.$$

    \end{proof}
\end{teo}

Antes de continuar, recordamos la siguiente definición.

\hspace{5mm} 
\begin{tcolorbox}
    \underline{Def.} Decimos que una transformación lineal es un \emph{operador lineal} cuando su dominio y contradominio son el mismo espacio vectorial. Por lo tanto, si decimos que $T$ es un operador lineal \textemdash o, simplemente, operador\textemdash \ sobre $V$, nos referimos a una transformación lineal $T:V\to V$.
\end{tcolorbox}

\vspace{3mm}

\begin{teo}{14.2}
    Sea $(V,K)$ un espacio vectorial de dimensión finita con producto escalar y sea $T:V\to V$ un operador lineal. Entonces existe una única función $T^*:V\to V$ tal que $\braket{y|T(x)}=\braket{T^*(y)|x}$ para toda $\ket{x},\ket{y}\in V$. Además, $T^*$ es lineal.
    
    \begin{proof}
        Sea $\ket{y}\in V$. Definimos a $\textbf{g}:V\to K$ como $\textbf{g}(\ket{x})=\braket{y|T(x)}$ para todo $\ket{x}\in V.$ Primero, verificamos que $\textbf{g}$ es lineal: sean $\ket{x_1},\ket{x_2}\in V$ y $c\in K$, entonces $$\textbf{g}(c\ket{x_1}+\ket{x_2})=\braket{y|T(cx_1+x_2)}=\braket{y|cT(x_1)+T(x_2)}=\braket{y|cT(x_1)}+\braket{y|T(x_2)}$$ $$=c\braket{y|T(x_1)}+\braket{y|T(x_2)}=c\textbf{g}(\ket{x_1})+\textbf{g}(\ket{x_2}),$$ por lo que $T$ es lineal.
        
        Luego, aplicamos el Teorema 14.1 para obtener un vector único $\ket{y'}\in V$ tal que $\textbf{g}(\ket{x})=\braket{y'|x}$ para todo $\ket{x}\in V.$ Observemos que, por definición de $\textbf{g}$, $\braket{y|T(x)}=\braket{y'|x}$ para todo $\ket{x}\in V$. Definiendo a $T^*:V\to V$ como $T^*(\ket{y})=\ket{y'}$ tenemos que $\braket{y|T(x)}=\braket{T^*(y)|x}$, como se deseaba. Ahora, debemos demostrar que $T^*$ es lineal: sean $\ket{y_1},\ket{y_2}\in V$ y $c\in K$, entonces para todo $\ket{x}\in V$ se cumple que $$\braket{T^*(cy_1+y_2)|x}=\braket{cy_1+y_2|T(x)}=\overline{c}\braket{y_1|T(x)}+\braket{y_2|T(x)}$$ $$=\overline{c}\braket{T^*(y_1)|x}+\braket{T^*(y_2)|x}=\braket{cT^*(y_1)+T^*(y_2)|x}$$ $$\implies\braket{T^*(cy_1+y_2)|x}-\braket{cT^*(y_1)+T(y_2)|x}=0$$ $$\implies \braket{T^*(cy_1+y_2)-(cT^*(y_1)+T^*(y_2))|x}=0.$$ Como esto vale para todo $\ket{x}\in V$, entonces esto implica que $$ \ket{T^*(cy_1+y_2)-(cT^*(y_1)+T(y_2))}=\ket{0}$$ $$\implies T^*(\ket{cy_1+y_2})=cT^*(\ket{y_1})+T^*(\ket{y_2}).$$
        
        Finalmente, para ver que $T^*$ es única: sea $U:V\to V$ un operador lineal tal que $\braket{y|T(x)}=\braket{U(y)|x}$ para todo $\ket{x},\ket{y}\in V$, entonces $\braket{T^*(y)|x}=\braket{U(y)|x}$ para todo $\ket{x},\ket{y}\in V$, por lo que $U=T^*$.
    \end{proof}
\end{teo}

Como veremos dentro de poco, el operador $T^*$ descrito en el Teorema 14.2 es de gran importancia en los espacios vectoriales con producto escalar, por lo cual le damos un nombre especial.

\hspace{0.5mm}
\begin{tcolorbox} \label{Def:Operador_adjunto}
\underline{Def.} Sea $V$ un espacio vectorial de dimensión finita y $T^*:V\to V$ el operador lineal único que para todo $\ket{x},\ket{y}\in V$ satisface la relación $$\braket{y|T(x)}=\braket{T^*(y)|x},$$ donde $T:V\to V$ es un operador lineal. Entonces, decimos que $T^*$ es el \emph{operador adjunto} de $T$.

\vspace{5mm}
\hspace{3mm} En el caso en que $V$ sea un espacio vectorial de dimensión infinita y para un operador lineal $T:V\to V$ exista otro operador lineal $T^*:V\to V$ tal que la relación anterior se cumpla, entonces nuevamente diremos que $T^*$ es el operador adjunto de $T$ \textemdash sin embargo, en este tipo de espacios vectoriales la existencia de $T^*$ no está asegurada para todo operador lineal $T$.
\end{tcolorbox}
\vspace{5mm}

Ya que varios de los teoremas que demostraremos a continuación aplican tanto para espacios vectoriales de dimensión finita como aquellos de dimensión infinita, de ahora en adelante, cuando hagamos referencia a un operador adjunto en un espacio vectorial de dimensión infinita, asumiremos implícitamente su existencia.

A continuación, demostraremos que en espacios vectoriales de dimensión finita podemos obtener la representación matricial de un operador adjunto a partir de la representación matricial del operador original de una forma muy sencilla, siempre y cuando utilicemos una base ortonormal para representar ambas matrices.

% PODRÍA METER A_ij=<b_i,T(b_j)> con beta una base ortonormal como un Teorema anterior al actual Teorema 14.3

\begin{teo} {14.3}
Sea $(V,K)$ un espacio vectorial de dimensión finita $n$ con producto escalar, $\beta=\{\ket{b_1},\ket{b_2},...,\ket{b_n}\}$ una base ortonormal de $V$ y $T:V\to V$ un operador lineal. Entonces $$[T^*]_\beta=\big([T]_\beta\big)^*,$$ donde $A^*$ se define mediante la relación $A^*_{ij}=\overline{A_{ji}}$ para toda $A\in M_{n\times n}(K)$.
\begin{proof}
Sean $A=[T]_\beta$ y $B=[T^*]_\beta$. Si partimos del sistema de ecuaciones que define a la representación matricial $A$ y hacemos producto escalar por la izquierda en ambos lados de cada una de las ecuaciones por un elemento $\ket{b_i}$ de la base y aplicamos ortonormalidad, obtenemos la relación $$A_{ij}=\braket{b_i|T(b_j)}.$$ 

Haciendo un procedimiento análogo con la representación matricial $B$, obtenemos que $$B_{ij}=\braket{b_i|T^*(b_j)},$$ de donde se sigue que $$B_{ij}=\overline{\braket{T^*(b_j)|b_i}}=\overline{\braket{b_j|T(b_i)}}=\overline{A_{ji}},$$ lo cual implica que $B=A^*$, es decir, que $[T^*]_\beta=\big([T]_\beta\big)^*$.
\end{proof}
\end{teo}

A la luz del Teorema 14.3, damos una nueva definición, que quizá ya esperes.

\vspace{5mm}
\begin{tcolorbox}
\underline{Def.} Sea $K$ un campo y $A\in M_{n\times n}(K)$. Decimos que $A^*\in M_{n\times n}(K)$, definida a través de la relación $$A^*_{ij}=\overline{A_{ji}},$$ es la \emph{matriz adjunta} de $A$.
\end{tcolorbox}
\vspace{5mm}

\noindent Recordando que para matrices cuadradas $A$ la matriz transpuesta $A^T$ se define mediante la relación $$A^T_{ij}=A_{ji},$$ podemos ver que la matriz adjunta $A^*$ se obtiene mediante la transposición de la matriz $A$ y la conjugación de sus entradas, sin importar el orden en que se realicen estas dos operaciones. Si entendemos por $\overline{A}$ a la matriz que se obtiene al conjugar todas las entradas de $A$, podemos escribir esto simplemente como $A^*=\overline{A^T}=\overline{A}^T$.

\subsection*{Ejercicios de repaso}

\subsubsection*{Descomposición espectral (introducción)} \label{Ejer:Descomposición espectral (introducción)}

\begin{enumerate}
\item Sean $V$ un espacio vectorial de dimensión finita, $T:V\to V$ un operador lineal y $\{\lambda_1,\lambda_2,... \ ,\lambda_k\}$ el espectro de $T$. Demuestra que $T$ es diagonalizable si y sólo si $V=E_{\lambda_1}\oplus E_{\lambda_2}\oplus...\oplus E_{\lambda_k}$.
\item Sean $V$ un espacio de dimensión finita con producto escalar, $T:V\to V$ un operador lineal y $\Lambda=\{\lambda_1,\lambda_2,... \ ,\lambda_k\}$ el espectro de $T$. Demuestra que $T$ se puede descomponer espectralmente si y sólo si $V = E_{\lambda_i} \oplus (E_{\lambda_i})^\perp$ para toda $\lambda_i\in\Lambda$.
\item Sean $V$ un espacio de dimensión finita $n$ con producto escalar, $T:V\to V$ un operador lineal, $\{\ket{g_1}, \ket{g_2}, ... \ ,\ket{g_n}\}$ un conjunto de eigenvectores normales de $T$ y $\Lambda=\{\lambda_1,\lambda_2,... \ ,\lambda_n\}$ un conjunto de eigenvalores de $T$ (no necesariamente todos distintos) tales que $T(\ket{g_i})=\lambda_i \ket{g_i} \ \forall \ i\in\{1,2,... \ ,n\}$. Demuestra que $T=\sum_{i=1}^n \lambda_i \ket{g_i}\bra{g_i}$ si y sólo si $I = \sum_{i=1}^n \ket{g_i}\bra{g_i}$.
\end{enumerate}

\subsubsection*{Operadores adjuntos} \label{Ejer:Operadores adjuntos}
\begin{enumerate}
    \item Sea $V$ un espacio vectorial de dimensión finita $n$ con producto escalar y $\beta=\{\ket{b_1},\ket{b_2},...,\ket{b_n}\}$ una base ortonormal de $V$. Demuestra que si $T:V\to V$ es lineal, entonces $([T]_\beta)_{ij}=\braket{b_i|T(b_j)}.$
    \item Sea $K$ un campo y $A\in M_{n\times n}(K)$. Demuestra que
    $A^*=\overline{A^T}=(\overline{A})^T$ y, por lo tanto, que $\text{det}(A^*)=\overline{\text{det}(A)}$.
    \item Sea $(V,K)$ un espacio vectorial con producto escalar y sean $T$ y $U$ operadores lineales sobre $V$ para los cuales existen operadores adjuntos $T^*$ y $U^*$, respectivamente. Demuestra las siguientes propiedades:
        \begin{enumerate}[label=\alph*)]
            \item $(T+U)^*=T^*+U^*$;
            \item $(cT)^*=\overline{c}T^*$ para todo $c\in K$;
            \item $(TU)^*=U^*T^*$;
            \item $(T^*)^*=T$;
            \item $I^*=I$.
        \end{enumerate}
    \item Sea $K$ un campo y sean $A,B\in M_{n\times n}(K)$. Demuestra las siguientes propiedades:
        \begin{enumerate}[label=\alph*)]
            \item $(A+B)^*=A^*+B^*$;
            \item $(cA)^*=\overline{c}A^*$ para todo $c\in K$;
            \item $(AB)^*=B^*A^*$;
            \item $(A^*)^*=A$;
            \item $I^*=I$.
        \end{enumerate}
\end{enumerate}

Una vez que hayas resuelto los ejercicios anteriores, observa de qué formas el álgebra de los operadores (que tienen operadores adjuntos) y las matrices cuadradas (que siempre tienen matrices adjuntas) junto con la operación de ``sacar el adjunto'' es similar al álgebra de los números complejos con la operación de \emph{conjugar}. Luego, observa de qué formas difieren, ¿a qué se deberán estas diferencias?

\newpage
\section{Operadores normales y autoadjuntos y teorema espectral} \label{Sec:15} 

\subsection*{Operadores normales} \label{Ssec:Operadores normales}

Para determinar si un operador tiene una base ortogonal de eigenvectores y, por lo tanto, si se puede descomponer espectralmente, una herramienta muy útil es entender cómo se relaciona con su operador adjunto. En un primer acercamiento, podemos analizar cómo se relacionan los eigenvalores de uno y otro en el caso de operadores lineales en  espacios vectoriales de dimensión finita.

\begin{lema} {15.1.1} \label{Lema: 15.1.1}
    Sea $T$ un operador lineal en un espacio vectorial $V$ de dimensión finita con producto escalar. Si $T$ tiene un eigenvector con eigenvalor $\lambda$, entonces $T^*$ tiene un eigenvector con eigenvalor $\overline{\lambda}$.

    \begin{proof}
        Sea $\ket{v}\in V$ un eigenvector de $T$ con eigenvalor $\lambda$. Entonces, por el ejercicio 3 de la sección \ref{Ejer:Operadores adjuntos}, para toda $\ket{u}\in V$ tenemos que \[
            0=\braket{0|u}=\braket{(T-\lambda I_V)(v)|u}=\braket{v|(T-\lambda I_V)^*(u)}=\braket{v|(T^*-\overline{\lambda}I_V)(u)}.
        \] Por ende, $\ket{v}\in V$ es un vector no nulo que es ortogonal a todos los vectores de $\text{Im}(T^*-\overline{\lambda}I_V)$. Esto implica que el operador lineal $T^*-\overline{\lambda}I_V$ no es suprayectivo y, por la fórmula de la dimensión, que tiene un núcleo no trivial. Por lo tanto, todo vector no nulo en $\text{Ker}(T^*-\lambda I_V)$ es un eigenvector de $T^*$ con eigenvalor $\overline{\lambda}$. El hecho de que el núcleo sea no trivial nos asegura que existe al menos un eigenvector de $T^*$ con eigenvalor $\overline{\lambda}$.
    \end{proof}
\end{lema}

El lema anterior nos dice que para cualquier operador $T$ con espectro $\Lambda$ y operador adjunto $T^*$, el espectro de $T^*$ se compondrá de los complejos conjugados de los elementos de $\Lambda$; regresaremos a él más adelante.

\vspace{3mm}
Para continuar con nuestra búsqueda, haremos una observación interesante apoyándonos en el isomorfismo de representación matricial de operadores lineales. Supongamos que $T$ es un operador lineal en un espacio vectorial de dimensión finita $V$ con producto escalar tal que $\gamma$ es una base ortonormal de $V$ compuesta por eigenvectores de $T$. Entonces, $[T]_\gamma$ es diagonal y, por el Teorema 14.3, se sigue que $$[T^*]_\gamma=([T]_\gamma)^*=\overline{([T]_\gamma^T)}=\overline{
([T]_\gamma)},$$ por lo que $[T^*]$ también es diagonal. Ya que las matrices diagonales conmutan, se sigue que $[T]_\gamma[T^*]_\gamma=[T^*]_\gamma[T]_\gamma$. En particular, como este isomorfismo es compatible con la multiplicación de matrices y la composición de transformaciones lineales, tenemos que $[TT^*]_\gamma=[T^*T]_\gamma$, de donde se sigue que $TT^*=T^*T$. Es decir que, si la matriz $[T]_\gamma$ es diagonal, donde $\gamma$ es una base ortonormal de un espacio vectorial $V$ de dimensión finita, entonces $TT^*=T^*T$, lo nos lleva a las siguientes definiciones.

\begin{tcolorbox}
    \underline{Def.} Sea $V$ un espacio vectorial con producto escalar. Decimos que un operador lineal $T:V\to V$ es \emph{normal} si $TT^*=T^*T$, suponiendo que $T^*$ exista.

    \vspace{3mm}
    \underline{Def.} Sea $A\in M_{n\times n}(K)$ con $K=\mathbb{R}$ ó $K=\mathbb{C}$. Decimos que $A$ es una matriz \emph{normal} si $AA^*=A^*A$.
\end{tcolorbox}

Con estas nuevas definiciones, podemos ver directamente del Teorema 14.3 que $[T]_\gamma$ es normal si y sólo si $T$ es normal, donde $\gamma$ es una base \emph{ortonormal} de un espacio vectorial $V$ de dimensión finita sobre el que actúa $T$. Más aún, del mismo teorema se sigue que, si dicha base ortonormal $\gamma$ \emph{diagonaliza} a $T$, entonces también diagonaliza a $T^*$, como observamos anteriormente. Es decir que, en este caso, $T$ y $T^*$ son simultáneamente diagonalizables. En particular, esto implica que ambos operadores tienen los mismos eigenvectores \textemdash aunque no necesariamente con los mismos eigenvalores correspondientes, como hemos visto en el Lema Lema: 15.1.1 A continuación, veremos varias propiedades que caracterizan a los operadores normales, incluyendo la forma en que se relacionan sus eigenvalores con los de su operador adjunto.

\begin{teo} {15.1.2}
    Sean $(V,K)$ un espacio vectorial con producto escalar y $T$ un operador normal en $V$. Entonces $T$ verifica las siguientes propiedades:

\begin{enumerate}[label=(\alph*)]
    \item $||T(\ket{x})||=||T^*(\ket{x})||$;
    \item $T-cI$ es un operador normal para todo $c\in K$;
    \item para todo $\ket{v}\in V$ tal que $T(\ket{v})=\lambda \ket{v}$, tenemos que $T^*(\ket{v})=\overline{\lambda}\ket{v}$;
    \item Si $\lambda_1$ y $\lambda_2$ son eigenvalores distintos de $T$ con eigenvectores correspondientes $\ket{g_1}$ y $\ket{g_2}$, respectivamente, entonces $\ket{g_1}$ y $\ket{g_2}$ son ortogonales.
\end{enumerate}

\begin{proof}
    Consultar en el Friedberg y reescribir en notación de Dirac (ejercicio).

    \iffalse


    (a)

    \vspace{3mm}
    (b)

    \vspace{3mm}
    (c) 

    \vspace{3mm}
    (d)


    \fi


\end{proof}
\end{teo}

\vspace{3mm}
Después de demostrar el Lema 15.1.1, vimos que si $V$ es un espacio vectorial de dimensión finita y $T:V\to V$ es un operador lineal con una base ortonormal $\gamma$ de $V$ tal que $[T]_\gamma$ es \emph{diagonal} (i.e., una base ortonormal de $V$ compuesta por eigenvectores de $T$), entonces $T$ es normal. Se puede demostrar que ambas condiciones son equivalentes, es decir, que existe una base ortonormal de $V$ compuesta por eigenvectores de $T$ si y sólo si $T$ es un operador normal; la implicación contraria se deja como ejercicio. Por ende, tenemos que para operadores lineales $T$ que actúan en un espacio vectorial $V$ de dimensión finita, las siguientes condiciones son equivalentes:

\begin{itemize}
    \item $T$ es un operador normal (es decir, conmuta con su operador adjunto).
    \item $[T]_\gamma$ es normal para toda base ortonormal $\gamma$ de $V$. 
    \item Existe una base ortonormal $\gamma$ de $V$ compuesta por eigenvectores de $T$.
    \item Existe una base ortonormal $\gamma$ de $V$ que diagonaliza a $T$.
\end{itemize}

En particular, las últimas dos propiedades junto con el inciso (a) del Teorema 14.4 nos dan una pista de por qué a este tipo de operadores se les llama \emph{normales}.

\vspace{3mm}
Parece que estamos muy cerca de nuestra meta de encontrar condiciones que aseguren cuándo podemos descomponer espectralmente a un operador lineal; sin embargo, observemos lo siguiente: sean $T$ un operador normal en un espacio vectorial $(V,K)$ de dimensión finita con producto escalar y $\ket{g_i},\ket{g_j}$ eigenvectores de $T$ con eigenvalores respectivos $\lambda_i, \lambda_j$ tales que $\lambda_i\neq \lambda_j$. Entonces, tenemos que 
$$\braket{g_i|TT^*(g_j)}=\braket{T^*(g_i)|(T^*(g_j))}=\braket{\overline{\lambda_i}g_i|\overline{\lambda_j}g_j}=\lambda_i \overline{\lambda_j}\braket{g_i|g_j},$$ 
mientras que, por otro lado, 
$$\braket{g_i|TT^*(g_j)}=\braket{g_i|T^*T(g_j)}=\braket{T(g_i)|T(g_j)}=\braket{\lambda_i g_i|\lambda_jg_j}=\overline{\lambda_i}\lambda_j\braket{g_i|g_j}=\overline{(\lambda_i \overline{\lambda_j})}\braket{g_i|g_j}.$$
Por lo tanto, llegamos a la igualdad
$$\big( \ \lambda_i \overline{\lambda_j} - \overline{(\lambda_i \overline{\lambda_j})} \ \big)\braket{g_i|g_j}=0.$$

Notemos que, si $K=\mathbb{C}$, entonces forzosamente $\ket{g_i}$ y $\ket{g_j}$ son ortogonales, de donde se sigue que $E_{\lambda_i}^\perp=\oplus_{j\neq i}E_{\lambda_j}$ para todo eigenvalor $\lambda_i$ de $T$ \textemdash y, por ende, que $T$ se puede descomponer espectralmente, de acuerdo al ejercicio 2 de la sección \ref{Ejer:Descomposición espectral (introducción)}. Sin embargo, si $K=\mathbb{R}$, entonces $\lambda_i \overline{\lambda_j} = \overline{(\lambda_i \overline{\lambda_j})}$ trivialmente, por lo que la igualdad del párrafo anterior siempre se cumple, sin que necesariamente $\braket{g_i|g_j}=0$. Por lo tanto, en un espacio vectorial de dimensión finita y con producto escalar, la condición de que $T$ sea normal es suficiente para que se pueda descomponer espectralmente \emph{siempre y cuando el espacio sea complejo} mientras que, si el espacio es real, \emph{esta condición no es suficiente}. Esto nos lleva a estudiar un tipo particular de operadores normales, que veremos a continuación.

\subsection*{Operadores autoadjuntos} \label{Subsec:Operadores autoadjuntos}

Como demostraremos más adelante, la condición suficiente para que un operador lineal que actúa en un espacio vectorial \emph{real} de dimensión finita y con producto escalar se pueda descomponer espectralmente es que sea \emph{igual} a su operador adjunto. Por ende, damos las siguientes definiciones.

\begin{tcolorbox}
    \underline{Def.} Sea $V$ un espacio vectorial con producto escalar. Decimos que un operador lineal $T:V\to V$ es \emph{autoadjunto} o \emph{hermitiano}\footnote{Generalmente, el término \emph{autoadjunto} es el predominante en matemáticas, mientras que el término \emph{hermitiano} es preferido en física. A pesar de eso, son equivalentes, y en estas notas utilizaremos ambos términos de manera intercambiable.} si $T=T^*$, suponiendo que $T^*$ exista.

    \vspace{3mm}
    \underline{Def.} Sea $A\in M_{n\times n}(K)$ con $K=\mathbb{R}$ ó $K=\mathbb{C}$. Decimos que $A$ es una matriz \emph{autoadjunta} o \emph{hermitiana} si $A=A^*$.
\end{tcolorbox}

Nótese que, para el caso de matrices cuadradas con entradas reales, la condición de que una matriz sea autoadjunta se reduce a $A=A^T$, es decir, que sea simétrica. Además, por un desarrollo similar al de la sección \ref{Ssec:Operadores normales}, podemos ver que $T$ es un operador hermitiano si y sólo si $[T]_\gamma$ es una matriz hermitiana, con $\gamma$ una base ortonormal arbitraria de $V$.

\begin{lema} {15.2.1}
    Sea $T$ un operador autoadjunto en un espacio vectorial de dimensión finita con producto escalar $V$. Entonces:
    \begin{enumerate}[label=(\alph*)]
        \item Todos los eigenvalores de $T$ son reales.
        \item Si $V$ es un espacio vectorial real, entonces el polinomio característico de $T$ es separable.
    \end{enumerate}

    \begin{proof}
        (a) Sean $\lambda$ un eigenvalor de $T$ y $\ket{v}$ un eigenvector correspondiente a $\lambda$. Entonces, por el Teorema 15.1.2.(c), tenemos que \[
            \lambda \ket{v} = T(\ket{v}) = T^*(\ket{v}) = \overline{\lambda} \ket{v} \implies \lambda = \overline{\lambda},
        \] de donde se sigue que $\lambda\in\mathbb{R}$. Por lo tanto, todos los eigenvalores del operador autoadjunto $T$ son reales.

        \vspace{3mm}
        (b) Consultar en el Friedberg y reescribir en notación de Dirac.
    \end{proof}
\end{lema}

Con el lema anterior en mente, podemos ver el siguiente resultado.

\begin{teo} {15.2.2}
    Sean $V$ un espacio vectorial real de dimensión finita $n$ con producto escalar y $T:V\to V$ un operador lineal. Entonces, $T$ es autoadjunto si y sólo si existe una base ortonormal $\gamma$ de $V$ compuesta por eigenvectores de $T$.

    \begin{proof}
    $(\Rightarrow)$ Ejercicio. (Pista: utiliza el lema anterior.).

        \vspace{3mm}
        $(\Leftarrow)$ Supongamos que existe una base ortonormal $\gamma=\{\ket{g_1}, \ket{g_2}, ... \ ,\ket{g_n}\}$ de $V$ compuesta por eigenvectores de $T$. Sea $\Lambda=\{\lambda_1,\lambda_2, ... \ ,\lambda_n\}$ el espectro de $T$ tal que $T(\ket{g_i})=\lambda_i \ket{g_i} \ \forall \ i\in\{1,2,... \ ,n\}$. 

        \vspace{3mm}
        Ya que $V$ es un espacio vectorial real, entonces $\overline{\lambda}_i = \lambda_i \ \ \forall \ \lambda_i\in\Lambda$. Ahora, observemos que \[
            \forall \ i\in \{1,2,... \ ,n\}, \ \ \braket{g_i|T(g_i)} = \braket{g_i|\lambda_i g_i} = \bra{g_i}(\lambda_i \ket{g_i}) = (\bra{g_i}\lambda_i)\ket{g_i} = \braket{\overline{\lambda}_i g_i|g_i} = \braket{\lambda_i g_i | g_i} = \braket{T(g_i)|g_i}
        \] \[
        \text{y}, \ \forall \ j\neq i, \ \ \braket{g_j|T(g_i)} = \braket{g_j|\lambda_i g_i} = \lambda_i \braket{g_j|g_i} = \lambda_j \braket{g_j|g_i} = \braket{\overline{\lambda}_j g_j|g_i} = \braket{\lambda_j g_j|g_i} = \braket{T(g_j)|g_i}.
    \] Por lo tanto, el operador lineal $T$ es tal que $\braket{g_j|g_i} = \braket{g_j|g_i}$ para cualesquiera elementos $\ket{g_i}, \ket{g_j}$ de la base $\gamma$ de $V$, de donde se sigue que \[
    \braket{y|T(x)}=\braket{T(y)|x} \ \ \forall \ \ket{x},\ket{y}\in V.
\] Como $V$ es de dimensión finita, por el Teorema 14.2 se sigue que $T:V\to V$ es el único operador con esta propiedad. Por ende, concluimos que $T$ es autoadjunto.
    \end{proof}
\end{teo}


\subsection*{Teorema espectral}

Concluida nuestra búsqueda por las hipótesis precisas bajo las cuales podemos llevar a cabo la descomposición espectral, enunciamos a continuación un teorema que recopila y sintetiza muchos resultados que hemos demostrado a lo largo de este módulo, y que utiliza una inmensa cantidad de conceptos que hemos aprendido durante todo el curso. La demostración de este teorema se deja como ejercicio.

\begin{teo} {15.3.1 (Teorema espectral)}
Sea $(V,K)$ un espacio vectorial de dimensión finita con producto escalar y $T:V\to V$ un operador lineal con eigenvalores distintos $\lambda_1,\lambda_2,...,\lambda_k$. Supongamos que $T$ es normal si $K=\mathbb{C}$ y que $T$ es autoadjunto si $K=\mathbb{R}$. Sea $W_i$ el eigenespacio de $T$ correspondiente al eigenvalor $\lambda_i$, con $1\leq i\leq k$, y sea $T_i$ la proyección ortogonal de $V$ sobre $W_i$. Entonces, se cumple que:
\begin{enumerate}[label=\alph*)]
    \item $V = W_1 \oplus W_2 \oplus ... \oplus W_k$;
    \item si $W_i'=\oplus_{j\neq i}W_j$, entonces $W_i^\perp=W_i'$;
    \item $T_i T_j = \delta_{ij}T_i$ para $1\leq i,j\leq k$;
    \item $I = T_1+T_2+...+T_k$;
    \item $T = \lambda_1 T_1 + \lambda_2 T_2 + ... + \lambda_k T_k$.
\end{enumerate}

\begin{proof}
    
    Ejercicio.

%\begin{enumerate}[label=\alph*)]
 %   \item Por los teoremas 6.16 y 6.17 sabemos que $T$ es diagonalizable. Entonces, para cada eigenespacio $W_i$ podemos elegir una base ordenada $\beta_i$ tal que la unión $\beta_1\cup\beta_2\cup...\cup\beta_k$ sea una base ordenada de $V$; en otras palabras, $W_1+W_2+...+W_k=V$. Por otra parte, ya que, por definición de eigenespacios, $W_i\cap W_j=\{\mathbf{0}\}$ si $i\neq j$, tenemos que $$V = W_1 \oplus W_2 \oplus ... \oplus W_k.$$
    %\item 
%\end{enumerate}
\end{proof}

\end{teo}

\subsection*{Ejercicios de repaso}

\subsubsection*{Operadores normales}
\begin{enumerate}
    \item Demuestra que cualesquiera dos matrices diagonales con entradas en un campo $K$ conmutan y que, por lo tanto, los operadores lineales que representan dichas matrices también deben conmutar.
    \item Demuestra el Teorema 15.1.2.
\end{enumerate}

\subsubsection*{Operadores autoadjuntos} \label{Sssec: Operadores autoadjuntos}
\begin{enumerate}
    \item Completa la demostración del Lema 15.2.1.
    \item Completa la demostración del Teorema 15.2.2.
\end{enumerate}

\subsubsection*{Teorema espectral}

\begin{enumerate}
    \item Demuestra el teorema espectral.
\end{enumerate}

\newpage
\begin{tcolorbox}
    \begin{center}
    \textbf{Sobre la descomposición espectral de matrices...}
    \end{center}

\hspace{2.5mm} A estas alturas del curso, nos encontramos en una excelente posición para volver a hacer énfasis en un punto de fundamental importancia en el álgebra lineal que muchas veces es pasado por alto: a simple vista, \emph{las matrices no cuentan toda la historia}\footnote{Esto es similar a cómo, en primera instancia, las $n$-tuplas que podemos utilizar para representar vectores abstractos \textemdash a través de isomorfismos\textemdash \ tampoco nos cuentan la historia completa.}.

\vspace{3mm}
\hspace{2.5mm} Pensemos un momento en la teoría de descomposición espectral: para encontrar a un operador que se pueda descomponer espectralmente \textemdash es decir, que proyecte vectores del dominio sobre eigenespacios ortogonales, reescale cada componente por el eigenvalor correspondiente al eigenespacio en que se encuentra, y construya a partir de estas proyecciones ortogonales reescaladas a los vectores imagen\textemdash \ requerimos poder formar una base ortonormal del espacio a partir de eigenvectores del operador.

\vspace{3mm}
\hspace{2.5mm} Supongamos que tenemos un operador que cumple las características deseadas y actúa sobre un espacio vectorial de dimensión finita\footnote{Aquí ya está implícito que el espacio en cuestión debe tener producto escalar. ¿Por qué?}. ¿Cómo se ve la representación matricial de nuestro operador en su base de eigenvectores? Pues, resulta que es una matriz diagonal, como cualquier otra... 

\vspace{3mm}
\hspace{2.5mm} ¿Cómo puede ser? ¡Esto debe ser mentira! En efecto, lo es: realmente no es como \emph{cualquier} matriz diagonal arbitraria, ya que la nuestra, en el fondo, \emph{está representada en una base ortonormal} del espacio\textemdash \ pero eso no se ve a simple vista.

\vspace{3mm}
\hspace{2.5mm} En realidad, el punto sobre el que hago énfasis es el de las \emph{representaciones}. Dado el poder que tienen las herramientas de cambio de representación en el álgebra lineal (permitiéndonos diagonalizar matrices, por ejemplo), cuando veamos $n$-tuplas y matrices, sería erróneo de nuestra parte siempre asumir que se encuentran en una base ortonormal \textemdash como suele manejarse implícitamente cuando trabajamos en espacios euclideanos. En cambio, al ver $n$-tuplas o matrices, debemos saber que lo que estamos viendo son apenas \emph{representaciones} de vectores y que, ya que dichas representaciones dependen totalmente de la base elegida (o \emph{las bases elegidas}, en el caso de las matrices, las cuales representan transformaciones lineales que, como sabemos, son vectores), el tener presentes las características de la(s) base(s) en la(s) que se esté representando pueden darnos información crucial acerca de los vectores representados; ¡no lo olviden!
\end{tcolorbox}

\newpage
\section{Operadores unitarios y ortogonales} \label{Sec:16} 

A lo largo del curso, hemos estudiado funciones que preservan la estructura algebráica del espacio subyaciente, empezando por las transformaciones lineales \textemdash que preservan las operaciones esenciales de los espacios vectoriales (suma vectorial y producto de un vector por un escalar)\textemdash \ y los isomorfismos \textemdash que, al ser transformaciones lineales \emph{invertibles}, preservan la estructura de todo el espacio vectorial, estableciendo una correspondencia biunívoca entre dos espacios vectoriales que, en \emph{esencia}, son lo mismo. Para cerrar el curso, dado que durante el último módulo nos hemos enfocado en espacios vectoriales con producto escalar, estudiaremos operadores lineales invertibles que preservan el producto escalar. Como veremos, en espacios vectoriales de dimensión finita, esto es equivalente a que dichos operadores preserven la norma inducida por el producto escalar. Además, veremos que algunos operadores de este tipo pueden descomponerse espectralmente\footnote{¿Cómo imaginas que serán sus espsectros? Pista: cambia dependiendo de si el campo es real o complejo.}, aplicando lo aprendido en la sección anterior.

\begin{tcolorbox}
\underline{Def.} Sea $T$ un operador lineal sobre un espacio vectorial $(V,K)$ de dimensión finita con producto escalar. Si $T$ es tal que \[
    ||T(\ket{v})|| = ||\ket{v}|| \ \ \ \forall \ \ket{v}\in V,
\] entonces diremos que $T$ es un operador \emph{unitario} si $K=\mathbb{C}$ o un operador \emph{ortogonal} si $K=\mathbb{R}$\footnote{Para el caso de dimensión infinita, $T$ debe de preservar la norma \emph{y ser invertible} para que se lo considere \emph{unitario} si $K=\mathbb{C}$ u \emph{ortogonal} si $K=\mathbb{R}$; si sólamente preserva la norma, se le conoce como una \emph{isometría}. Claramente, esto indica que los operadores unitarios u ortogonales en espacios de dimensión finita son invertibles.}.
\end{tcolorbox}

En $\mathbb{R}^3$ (y, más generalmente, en $\mathbb{R}^n$), las rotaciones alrededor de y las reflexiones con respecto a cualquier eje que pase por el origen son ejemplos de operadores ortogonales\footnote{¿Recuerdas haber visto un operador de este tipo en tu segundo examen?}. En el espacio vectorial complejo $\mathbb{C}$ (más generalmente, $\mathbb{C}^n$), cualquier operador que reescale por un número complejo de norma (valor absoluto) uno \textemdash o, en otras palabras, por un número complejo \emph{unitario}\textemdash \ es un ejemplo de un operador unitario\footnote{¡Recuerda tu primer examen!}.

\vspace{3mm}
A continuación, demostraremos un lema que nos ayudará a encontrar una serie de criterios equivalentes a la definición anterior de un operador unitario u ortogonal. Las demostraciones del lema y del teorema posterior quedan como ejercicio.

\begin{lema} {16.1}
    Sea $U$ un operador autoadjunto en un espacio vectorial $V$ de dimensión finita con producto escalar. Entonces, si $\braket{v|U(v)}=0 \ \ \forall \ \ket{v}\in V$, entonces $U=T_0$, la transformación nula. 
\end{lema}

\begin{proof}
    Ejercicio.
\end{proof}

\begin{teo} {16.2}
    Sea $T$ un operador lineal en un espacio vectorial de dimensión finita con producto escalar. Entonces, la siguientes afirmaciones son equivalentes.

    \begin{enumerate}[label=(\alph*)]
        \item $||T(\ket{v})|| = || \ket{v} || \ \ \forall \ \ket{v}\in V$.
        \item $\braket{T(v)|T(v)} = \braket{v|v} \ \ \forall \ \ket{v}\in V$.
        \item $TT^*=T^*T=I_V$.
        \item $\braket{T(u)|T(v)} = \braket{u|v} \ \ \forall \ \ket{u},\ket{v}\in V$.
        \item Si $\beta$ es una base ortonormal de $V$, entonces $T(\beta)$ también lo es.
        \item Existe una base ortonormal $\beta$ de $V$ tal que $T(\beta)$ también es una base ortonormal de $V$.
    \end{enumerate}
\end{teo}

\begin{proof}
    Ejercicio.
\end{proof}

Observemos de la primera igualdad del inciso (c) que todo operador unitario u ortogonal es normal y, por ende, los operadores unitarios pueden descomponerse espectralmente; además, a partir de este inciso, podemos hacer una analogía entre los operadores unitarios/ortogonales en un espacio vectorial con producto escalar y los elementos $z\in \mathbb{C}$ tales que $z \overline{z} = \overline{z} z = 1$.

\vspace{3mm}
Por otro lado, ya que la norma inducida por el producto escalar es escalable de forma absoluta, entonces se sigue que todos los eigenvalores de un operador unitario/ortogonal tienen valor absoluto 1 directamente de la definición. Es decir, sus eigenvalores son precisamente aquellos escalares $\lambda\in\mathbb{C}$ ó $\lambda\in\mathbb{R}$ tales que $\lambda \overline{\lambda} = \overline{\lambda} \lambda = 1$. Finalmente, aplicamos el Teorema Espectral a los operadores unitarios y ortogonales en los siguientes corolarios.

\begin{coro} {16.3}
    Sean $T:V\to V$ un operador unitario sobre un espacio vectorial complejo de dimensión finita $n$ con producto escalar y $\Lambda=\{\lambda_1,\lambda_2,... , \lambda_k\}$ el espectro de $T$. Entonces, tenemos que $T = \lambda_1 T_1 + \lambda_2 T_2 + ... + \lambda_k T_k,$ con $T_i$ el operador de proyección ortogonal sobre el eigenespacio $E_{\lambda_i}$, y $\Lambda\subset\{z\in \mathbb{C} \mid z \overline{z}= \overline{z}z = 1\}$.

    \begin{proof}
        Ejercicio.
    \end{proof}
       
\end{coro}

\begin{coro} {16.4}
    Sean $T:V\to V$ un operador ortogonal sobre un espacio vectorial real de dimensión finita $n$ con producto escalar y $\Lambda=\{\lambda_1,\lambda_2,... , \lambda_k\}$ el espectro de $T$. Entonces, si $T$ es autoadjunto, tenemos que $T = \lambda_1 T_1 + \lambda_2 T_2 + ... + \lambda_k T_k,$ con $T_i$ el operador de proyección ortogonal sobre el eigenespacio $E_{\lambda_i}$, y $\Lambda\subseteq\{\pm 1\}$.

    \begin{proof}
        Ejercicio.
    \end{proof}
       
\end{coro}

\subsection*{Ejercicios de repaso}

\subsubsection*{Operadores lineales ortogonales y unitarios}

\begin{enumerate}
    \item Sean $(V,K)$ un espacio vectorial de dimensión finita con producto escalar y $T:V\to V$ un operador lineal tal que $T$ es unitario si $K=\mathbb{C}$ y $T$ es ortogonal si $K=\mathbb{R}$. Demuestra que $T$ es invertible.
    \item Da un ejemplo de un operador ortogonal $T:\mathbb{R}^2\to \mathbb{R}^2$ que no tenga eigenvectores y que, por lo tanto, no se pueda formar una base ortogonal de $\mathbb{R}^2$ compuesta por eigenvectores de $T$. Esto ilustra la importante diferencia entre operadores ortogonales y autoadjuntos.
    \item Da un ejemplo de un operador normal $U:\mathbb{C}\to \mathbb{C}$ que no sea unitario.
    \item Demuestra el Lema 16.1.
    \item Demuestra el Teorema 16.2.
    \item Demuestra el Corolario 16.3.
    \item Demuestra el Corolario 16.4.
\end{enumerate}



\end{document}
