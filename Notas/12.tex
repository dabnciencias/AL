\documentclass[notasLineal]{subfiles}
\begin{document}

\section{Norma inducida y bases ortonormales, ortogonalización y ortonormalización}

\subsubsection*{Norma} \label{Ejer:Norma}

\begin{enumerate}
    \item Calcula la norma de las funciones dadas en el primer ejercicio de la sección \ref{Ejer:Producto_escalar}. (Nota: utiliza la norma inducida por ese mismo producto escalar.) 
    \item ¿El conjunto $\{(x_1,x_2,...,x_n)\mathop|\mathop x_i\in\mathbb{R} \land ||(x_1,x_2,...,x_n)||\leq1\}$ con las operaciones entre $n$-tuplas vistas en la sección \ref{Ejem:Espacios_vectoriales} puede formar un espacio vectorial sobre el campo real? Argumenta. 
    \item Argumenta e ilustra la interpretación geométrica de la desigualdad de Cauchy-Schwarz para dos vectores cualesquiera de $\mathbb{R}^2$ (Nota: ver sec. \ref{Teo:Cauchy-Schwarz}). 
    \item Demuestra la desigualdad del triángulo $||\mathbf{a}+\mathbf{b}|| \leq ||\mathbf{a}||+||\mathbf{b}||$ para cualesquiera dos vectores en un espacio $V$ con producto escalar positivo definido donde la norma $||\mathbf{a}||\equiv+\sqrt{(\mathbf{a},\mathbf{a})}\hspace{3mm}\forall\hspace{1.5mm} \mathbf{a}\in V$ (pista: usa la desigualdad de Cauchy-Schwarz)\footnote{Con esto habrás demostrado que a partir de cualquier producto escalar positivo definido $(\mathbf{a},\mathbf{b})$ se puede definir una norma como $||\mathbf{a}|| = +\sqrt{(\mathbf{a},\mathbf{a})}.$}. Argumenta e ilustra su interpretación geométrica para dos vectores cualesquiera de $\mathbb{R}^2$. 
\end{enumerate}

\subsubsection*{Interpretación geométrica del producto escalar: proyecciones y ortogonalidad}
\begin{enumerate}
\item Sea $\mathbf{v}\in\mathbb{R}^2$ no-nulo. Prueba que el conjunto $\{\mathbf{u}\in\mathbb{R}^2\hspace{0.5mm} |\hspace{0.5mm} \langle\mathbf{u},\mathbf{v}\rangle =0\}$ es una recta que pasa por el origen y llévala a una expresión de la forma $ax+by=c.$ 
    \item Sean $\mathbf{v}=\begin{pmatrix} v_1 & v_2 \end{pmatrix}, \mathbf{u}=\begin{pmatrix}u_1 & u_2 \end{pmatrix}$ vectores del espacio real $\mathbb{R}^2.$ Deduce que $\mathbf{u}\cdot\mathbf{v}=u_1v_1+u_2v_2=||\mathbf{u}||\hspace{0.5mm}||\mathbf{v}||\cos(\theta)$, donde $\theta$ es el ángulo mínimo entre ambos vectores en el plano cartesiano. (Nota: supon que ambos vectores son no nulos y encuentra al escalar $k\in\mathbb{R}$ tal que $\langle\mathbf{u},\mathbf{u}-k\mathbf{v}\rangle=0$. ¿Por qué es especial este escalar?) 
    \item Define a los vectores $\mathbf{u}=\begin{pmatrix} a_1 & a_2 \end{pmatrix}, \mathbf{v}=\begin{pmatrix} a_3 & a_4\end{pmatrix}$ y $\mathbf{w}=\begin{pmatrix} a_5 & a_6\end{pmatrix}\in\mathbb{R}^2,$ donde $a_i$ corresponde al $i$-ésimo dígito de tu número de cuenta. Explica gométricamente el hecho de que $\langle\mathbf{u}+\mathbf{v},\mathbf{w}\rangle=\langle\mathbf{u},\mathbf{w}\rangle+\langle\mathbf{v},\mathbf{w}\rangle$, en términos de las proyecciones discutidas en la sección \ref{Subsec:Interpretación geométrica del producto escalar} (Nota: recuerda la Ley del paralelogramo). 
\end{enumerate}

\subsubsection*{Producto vectorial (cruz)*}

\begin{enumerate}
    \item Demuestra las propiedades del producto vectorial de la sección \ref{Prop:Producto_vectorial}. Además, da una interpretación geométrica para la primera, tercera y cuarta propiedad enlistadas. (Nota: si quieres hacer estas demostraciones utilizando una notación condensada, te recomiendo investigar acerca del símbolo de Levi-Civita el cual, junto con la delta de Kronecker, facilitan la escritura de muchas demostraciones de cálculo vectorial, entre otras áreas de las matemáticas.) 
%    \item Da una definición de un producto cruz entre vectores de $\mathbb{C}^3$ tal que se mantengan las propiedades vistas en la sección \ref{Prop:Producto_vectorial}.
\end{enumerate}

\subsubsection*{Triple producto escalar*}

\begin{enumerate}
    \item Demuestra las propiedades del triple producto escalar de la sección \ref{Prop:Triple_producto_escalar}. 
\end{enumerate}{}

\begin{tcolorbox}
\begin{center}
    \textbf{Nota aclaratoria: \emph{Sobre nombres y traducciones...}}
\end{center}

\hspace{2.5mm}Como seguramente habrás notado al leer los libros recomendados, al producto escalar en inglés se le conoce como \emph{inner product}, y a los espacios vectoriales dotados de un producto escalar se les conoce como \emph{inner product spaces}. En español, al producto escalar también se le conoce como \emph{producto interior}; sin embargo, existe otro tipo de producto diferente al producto escalar al cual en inglés, desafortunadamente, le llaman \emph{interior product}.

\vspace{5mm}
\hspace{2.5mm}Esto significa que, en inglés, la convención es que \emph{scalar product} e \emph{interior product} sean operaciones diferentes mientras que, en español, la convención es que \emph{producto escalar} y \emph{producto interior} se refieran a la misma operación. Algunos textos en español utilizan \emph{producto interno} (en vez de producto interior) como sinónimo de \emph{producto escalar} para homologar los nombres con los utilizados en inglés pero, por ahora, las convenciones preponderantes en español e inglés no permiten una traducción directa.

\vspace{5mm}
\hspace{2.5mm}Por lo anterior, en estas notas decidí usar únicamente el nombre de \emph{producto escalar} para la operación entre dos vectores que da como resultado un escalar (la cual acostumbramos llamar \emph{producto punto} cuando esos vectores son $n$-tuplas), pero es importante que sepan que esta operación es equivalente al \emph{\underline{inner} product} de los textos en inglés.

\vspace{5mm}
\hspace{2.5mm} Para empeorar la situación, algunos textos en inglés se refieren a la operación de producto de un vector por un escalar como \emph{scalar multiplication} \textemdash por lo cual algunos textos en español pueden referirse a esta operación como \emph{multiplicación escalar} o, inclusive, \emph{producto escalar}\textemdash \hspace{0.5mm} mientras que otros textos utilizan el término \emph{scalar multiplication} para referirse a la multiplicación entre dos elementos del campo (escalares) que da como resultado otro elemento del campo (escalar). Por lo tanto, debemos recordar que el significado de estos términos depende del contexto en que se utilicen. 

\vspace{5mm}
\hspace{2.5mm} Finalmente, remarcamos que en la convención de este texto: 
    \begin{itemize}
        \item Los términos \emph{producto de un vector por un escalar} y \emph{reescalamiento} se emplean para referirnos a la operación realizada entre un vector del conjunto vectorial y un escalar del campo que da como resultado otro vector;
        \item el término \emph{producto escalar} se reserva para la operación realizada entre dos vectores que da como resultado un escalar;
        \item el término \emph{producto entre escalares} se utiliza para referirnos a la multiplicación entre dos escalares del campo que da como resultado otro escalar del campo.
    \end{itemize}

\end{tcolorbox}

\subsection*{Ortogonalización y ortonormalización} \label{Subsec:Ortogonalización y ortonormalización}

Recordemos de la sección \ref{Subsec:Interpretación geométrica del producto escalar} que dos vectores $\mathbf{u}$ y $\mathbf{v}$ son ortogonales si $\langle\mathbf{u},\mathbf{v}\rangle=0$. Supongamos que este no es el caso, i.e., que $\langle\mathbf{u},\mathbf{v}\rangle\neq 0$\footnote{Aquí implícitamente estamos asumiendo que $\mathbf{u}$ y $\mathbf{v}$ son vectores no nulos ya que, por definición, el producto escalar \emph{distingue} al vector nulo (ver sec. \ref{Def:Producto_escalar}).}. Existe una manera de modificar cualquiera de los vectores de tal forma que se vuelva ortogonal al otro: simplemente definimos $\mathbf{u'}=\mathbf{u}-\langle\mathbf{u},\mathbf{v}\rangle\frac{\mathbf{v}}{||\mathbf{v}||^2}$ y comprobamos que $$\langle\mathbf{u'},\mathbf{v}\rangle=\big\langle\mathbf{u}-\langle\mathbf{u},\mathbf{v}\rangle\frac{\mathbf{v}}{||\mathbf{v}||^2},\mathbf{v}\big\rangle=\langle\mathbf{u},\mathbf{v}\rangle-\big\langle\frac{\langle\mathbf{u},\mathbf{v}\rangle}{||\mathbf{v}||^2}\mathbf{v},\mathbf{v}\big\rangle=\langle\mathbf{u},\mathbf{v}\rangle-\frac{\langle\mathbf{u},\mathbf{v}\rangle}{\langle\mathbf{v},\mathbf{v}\rangle}\langle\mathbf{v},\mathbf{v}\rangle=\langle\mathbf{u},\mathbf{v}\rangle-\langle\mathbf{u},\mathbf{v}\rangle\cdot 1=0.$$ 

A esto se le conoce como un proceso de \emph{ortogonalización}. Observemos que, si $\mathbf{v}$ es un vector unitario (i.e., si $||\mathbf{v}||=1$) entonces la definición de $\mathbf{u'}$ se reduce a $\mathbf{u'}=\mathbf{u}-\langle\mathbf{u},\mathbf{v}\rangle\mathbf{v}$, simplificando el proceso. Por razones que irán quedando más claras con la experiencia, a menudo es conveniente trabajar con bases de vectores que sean ortogonales entre sí y, en casos específicos, que además sean unitarios, por lo cual damos las siguientes definiciones..

\subsubsection*{Definiciones}

\begin{tcolorbox}

    \underline{Def.} Sea $O=\{\mathbf{o}_1, \mathbf{o}_2, ..., \mathbf{o}_n\}$ una base de un espacio vectorial $V$. Decimos que $O$ es una \emph{base ortogonal} si cada uno de sus vectores es ortogonal a todos los demás, i.e., si $\langle\mathbf{o}_i,\mathbf{o}_j\rangle=0\hspace{3mm}\forall\hspace{1.5mm} \mathbf{o}_i, \mathbf{o}_j\in O, \hspace{1.5mm} i\neq j$.

\vspace{3mm}

    \underline{Def.} Sea $N=\{\mathbf{n}_1, \mathbf{n}_2, ..., \mathbf{n}_n\}$ una base de un espacio vectorial $V$. Decimos que $N$ es una base \emph{ortonormal} si es una base ortogonal y, además, todos sus vectores son unitarios, i.e., si $\langle\mathbf{n}_i,\mathbf{n}_j\rangle=\delta_{ij}\hspace{3mm}\forall\hspace{1.5mm} \mathbf{n}_i,\mathbf{n}_j\in N,$ donde $\delta_{ij}$ es la \emph{delta de Kronecker} de dos índices\footnote{En general, la delta de Kronecker de $n$ índices se define como $\delta_{ab...n}=1$ si $a=b=...=n$ y $\delta_{ab...n}=0$ en cualquier otro caso.}.

\end{tcolorbox}

Antes de ver cómo podemos construir bases ortogonales y ortonormales, el siguiente teorema y corolario nos ayudarán a comenzar a entender su utilidad.

\begin{Teo} {4.3.1.1} \label{Teo:4.3.1.1}
    Sea $V$ sobre $K$ un espacio vectorial con producto escalar y $S=\{\mathbf{v}_1, ..., \mathbf{v}_n\}$ un subconjunto ortogonal de $V$ con $\mathbf{v}_i\neq\mathbf{0}\hspace{3mm}\forall\hspace{1.5mm} 1\leq i\leq n.$ Si $\mathbf{u}\in \langle S \rangle \implies$ $$\mathbf{u} = \sum_{i=1}^n \frac{\langle\mathbf{u},\mathbf{v}_i\rangle}{||\mathbf{v}_i||^2}\mathbf{v}_i=\sum_{i=1}^n P_{\mathbf{v}_i}(\mathbf{u})\frac{\mathbf{v}_i}{||\mathbf{v}_i||}.$$

\begin{proof}
    Ya que $\mathbf{u}\in\langle S \rangle \implies \mathbf{u}=c_1\mathbf{v}_1+...+c_n\mathbf{v}_n=\sum_{i=1}^n c_i\mathbf{v}_i$ para algunos coeficientes $c_i\in K$. Para ver precisamente quiénes son esos coeficientes $c_i$, observemos que para $1\leq j\leq n$ $$\langle\mathbf{u},\mathbf{v}_j)=\big \langle \sum_{i=1}^n c_i\mathbf{v}_i, \mathbf{v}_j \big \rangle.$$ \noindent Aplicando las propiedades del producto escalar y la definición de conjunto ortogonal, tenemos que $$\langle\mathbf{u},\mathbf{v}_j\rangle=\sum_{i=1}^n \langle c_i\mathbf{v}_i, \mathbf{v}_j\rangle= \sum_{i=1}^n c_i\langle\mathbf{v}_i,\mathbf{v}_j\rangle=c_j\langle\mathbf{v}_j,\mathbf{v}_j\rangle=c_j||\mathbf{v}_j||^2\iff c_j||\mathbf{v}_j||^2=\langle\mathbf{u},\mathbf{v}_j\rangle\iff c_j=\frac{\langle\mathbf{u},\mathbf{v}_j\rangle}{||\mathbf{v}_j||^2}.$$ Sustituyendo, tenemos que \[
        \mathbf{u}=\sum_{i=1}^n c_i \mathbf{v}_i=\sum_{i=1}^n \frac{\langle\mathbf{u},\mathbf{v}_i\rangle}{||\mathbf{v}_i||^2}\mathbf{v}=\sum_{i=1}^n P_{\mathbf{v}_i}(\mathbf{u})\frac{\mathbf{v}_i}{||\mathbf{v}_i||}
    ,\] \noindent como se quería demostrar originalmente. En particular, observamos que $c_i= \frac{P_{\mathbf{v}_i}(\mathbf{u})}{||\mathbf{v}_i||}.$
\end{proof}

Habíamos visto con anterioridad que cualquier vector puede ser expresado como una combinación lineal única de elementos de su base; sin embargo, no habíamos entrado en detalles sobre cómo obtener los coeficientes necesarios para esto más allá de plantear y resolver un sistema de ecuaciones. Si aplicamos el teorema anterior a una base ortogonal $O$ de un espacio vectorial $V$, entonces $\forall\hspace{1.5mm} \mathbf{u}\in V $ tenemos una \emph{receta} para obtener directamente los coeficientes necesarios para expresar a $\mathbf{v}$ como combinación lineal de los vectores de $O$: de ahí viene la utilidad de las bases ortogonales.

    Nótese que, en particular, si el conjunto $S$ es \emph{ortonormal}, entonces $c_j=\langle\mathbf{u},\mathbf{v}_j\rangle$ en la demostración anterior y el resultado que obtuvimos se reduce a \[
        \mathbf{u} = \sum_{i=1}^n \langle\mathbf{u},\mathbf{v}_i\rangle\mathbf{v}_i
    .\] \noindent Por lo tanto, si aplicamos el Teorema 4.3.1.1 a una base ortonormal $N$, los coeficientes mencionados en el párrafo anterior son simplemente el producto escalar del vector $\mathbf{u}$ con cada vector de la base ortonormal.

\end{Teo}

\begin{Coro} {4.3.1.2}
Sea $V$ sobre $K$ un espacio vectorial con producto interior y $S$ un subconjunto ortogonal con vectores no nulos, entonces $S$ es linealmente independiente.

\begin{proof}
    Supongamos que $S=\{\mathbf{v}_1, ..., \mathbf{v}_n\}.$ Fijamos nuestra atención en la combinación lineal $\sum_{i=1}^n c_i\mathbf{v}_i=\mathbf{0}$, con $c_i\in K$. Aplicando el Teorema 4.3.1.1 con $\mathbf{u}=\mathbf{0}$ tenemos que $c_i=\frac{\langle\mathbf{0},\mathbf{v}_i\rangle}{||\mathbf{v}_i||^2}=0\hspace{3mm}\forall\hspace{1.5mm}  c_i\implies S$ es linealmente independiente.
\end{proof}

Observemos que este corolario es una genearlización del primer ejercicio de repaso de la sección 3. Además, este corolario y el teorema del cual se desprende nos dicen que los conjuntos ortogonales son buenos candidatos para bases, ya que son linealmente independientes y el cálculo de los coeficientes $c_i$ es sumamente sencillo. En particular, aplicando este corolario a los conjuntos ortogonales vemos que, si $V$ es un espacio vectorial de dimensión $n$, entonces cualquier conjunto ortogonal de $n$ vectores es una base de $V$ (ver teorema 4.2.3). 
\end{Coro}

Así como a partir de cualquier conjunto linealmente independiente de un espacio vectorial $V$ se puede construir una base para $V$ se puede, además, realizar un proceso de \emph{ortogonalización} u \emph{ortonormalización} de esa misma base. La demostración de este hecho\textemdash que también nos deletrea el proceso a seguir para lograrlo\textemdash \hspace{0.5mm} se conoce como el Teorema de Gram-Schmidt.

\subsubsection*{Teorema de Gram-Schmidt} \label{Teo:Gram-Schmidt}

\begin{Teo} {4.3.2.1 (Gram-Schmidt)}
    Sea $V$ sobre $K$ un espacio vectorial y $S=\{\mathbf{u}_1, ..., \mathbf{u}_n\}$ un subconjunto linealmente independiente de $V$. Si definimos al conjunto $S'=\{\mathbf{v}_1, ..., \mathbf{v}_n\}$ de tal forma que $\mathbf{v}_1=\mathbf{u}_1$ y $$\mathbf{v}_k=\mathbf{u}_k-\sum_{j=1}^{k-1}\frac{\langle\mathbf{u}_k,\mathbf{v}_j\rangle}{||\mathbf{v}_j||^2} \mathbf{v}_j=\mathbf{u}_k-\sum_{j=1}^{k-1} P_{\mathbf{v}_j}(\mathbf{u}_k) \frac{\mathbf{v}_j}{||\mathbf{v}_j||},\hspace{3mm 2\leq k\leq n},$$ entonces $S'$ es un subconjunto ortogonal de $V$ tal que $\langle S' \rangle = \langle S \rangle.$

\begin{proof}
    Esta demostración se hará por inducción sobre $n$ y, para realizarla, definimos a $S_k\equiv\{\mathbf{u}_1, ..., \mathbf{u}_k\}$ para $k=1,2, ..., n.$

    \vspace{3mm}
\textbf{Base inductiva}
Si $n=1$, entonces el teorema se demuestra trivialmente, ya que $S'=S$, por lo cual $\langle S' \rangle =\langle S \rangle$ y, además,  $S'$ sería un subconjunto ortogonal de $V$ por vacuidad.

    \vspace{3mm}
\textbf{Hipótesis de inducción}
Supongamos que el conjunto $S'_{k-1}=\{\mathbf{v}_1, ..., \mathbf{v}_{k-1}\}$ ha sido construido siguiendo el proceso descrito en el planteamiento del teorema y que cumple las propiedades deseadas, es decir, que es un conjunto ortogonal tal que $\langle S'_{k-1} \rangle = \langle S_{k-1} \rangle,$ donde $S_{k-1}$ es linealmente independiente.

    \vspace{3mm}
\textbf{Paso inductivo}
    Sea $S_k$ un conjunto linealmente independiente y $S'_k=\{\mathbf{v}_1, ..., \mathbf{v}_{k-1}, \mathbf{v}_k\}$ tal que $$\mathbf{v}_k=\mathbf{u}_k-\sum_{j=1}^{k-1}\frac{\langle\mathbf{u}_k,\mathbf{v}_j\rangle}{||\mathbf{v}_j||^2}\mathbf{v}_j.$$

    Si $\mathbf{v}_k=\mathbf{0}$ entonces la ecuación anterior implicaría que $\mathbf{u}_k\in\langle S'_{k-1} \rangle = \langle S_{k-1} \rangle,$ lo cual contradice el hecho de que $S_k$ es un conjunto linealmente independiente, por lo cual forzozamente $\mathbf{v}_k\neq\mathbf{0}.$ 

    Para $1\leq i\leq k-1$ se sigue que $$\langle\mathbf{v}_k,\mathbf{v}_i\rangle=\big \langle\mathbf{u}_k-\sum_{j=1}^{k-1}\frac{\langle\mathbf{u}_k,\mathbf{v}_j\rangle}{||\mathbf{v}_j||^2}\mathbf{v}_j,\mathbf{v}_i \big \rangle=\langle\mathbf{u}_k,\mathbf{v}_i\rangle-\sum_{j=1}^{k-1}\frac{\langle\mathbf{u}_k,\mathbf{v}_j\rangle}{||\mathbf{v}_j||^2}\langle\mathbf{v}_j,\mathbf{v}_i\rangle=\langle\mathbf{u}_k, \mathbf{v}_i\rangle-\frac{\langle\mathbf{u}_k,\mathbf{v}_i\rangle}{||\mathbf{v}_i||^2}||\mathbf{v}_i||^2=\mathbf{0},$$ ya que en nuestra hipótesis inductiva supusimos que $S'_{k-1}$ es ortogonal. Por ende, $S'_k$ es un conjunto ortogonal de vectores no nulos.

Finalmente, por construcción $S'_k\subseteq \langle S_k \rangle$, lo cual también implica que $\langle S'_k \rangle \subseteq \langle S_k \rangle;$ análogamente, por la forma en que construimos $S'_k$, $S_k\subseteq \langle S'_k \rangle$, lo cual implica que $\langle S_k \rangle \subseteq \langle S'_k \rangle.$ Juntando ambos resultados, concluimos que $\langle S_k \rangle = \langle S'_k \rangle ,$ como queríamos demostrar.

\end{proof}
    Aplicando este teorema a bases, nos dice que a partir de una base arbitraria $B$ de un espacio vectorial $V$, se puede construir una base ortogonal $O$ para ese mismo espacio $V$: sólo hace falta escoger algún vector $\mathbf{b}_1\in B$ con el cual definir $\mathbf{o}_1\equiv\mathbf{b}_1$ y después seguir el procedimiento descrito en el teorema. 

    Además, observemos que se podría modificar ligeramente el procedimiento seguido en el teorema anterior definiendo al primer vector de la nueva base como un vector unitario. Es más, supongamos que definimos un nuevo conjunto generador $N$ con $\mathbf{n}_1=\frac{\mathbf{b}_1}{||\mathbf{b}_1||}$ y después, cada vez que obtenemos un nuevo vector ortogonal a los anteriores siguiendo el proceso del Teorema 4.3.2.1, lo normalizamos antes de añadirlo a $N$: en este caso, el conjunto generador resultante será una base \emph{ortonormal} de $V$. Es decir, podríamos hacer un Teorema de Gram-Scmidt \emph{modificado} de tal forma que a partir de cualquier subconjunto linealmente independiente podamos generar un subconjunto ortonormal que genere al mismo subespacio que el anterior. Aplicado a bases, estaríamos asegurando que a partir de cualquier base $B$ de un espacio vectorial $V$, se puede construir una base ortonormal para $V$, además de detallar el proceso mediante el cual se contstruye dicha base ortonormal.  

\end{Teo}

%\subsubsection*{Ejemplos de ortogonalización y ortonormalización} 

\subsubsection*{Ortogonalización y ortonormalización}
\begin{enumerate}
    \item Sea $B$ la base que obtuviste del ejercicio 4.4.1.2. Aplica el Teorema de Gram-Schmidt y construye una base ortogonal de $\mathbb{R}^3$ a partir de $B$. 
    \item Aplica el Teorema de Gram-Schmidt \emph{modificado} y construye una base ortonormal a partir de $B$. 
\item Sea $P_0(x)=1, P_1(x)=x, P_2(x)=\frac{3x^2-1}{2}, P_3(x)=\frac{5x^3-3x}{2}.$ Demuestra que el conjunto $\{P_0,P_1,P_2,P_3\}$ es una base del espacio vectorial $P^3$ y di si es ortogonal y/o ortonormal en el intervalo $[-1,1]$ (Nota: tendrás que utilizar la definición de producto escalar para funciones vista en la sec. \ref{Ejem:Producto_escalar}). 
\end{enumerate}

\end{document}
