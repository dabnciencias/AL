\documentclass[12pt,dvipsnames]{article}

\usepackage[margin=1.5cm]{geometry} 
\usepackage{amsmath,amsthm,amssymb}
\usepackage[spanish,es-tabla]{babel}
\decimalpoint
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage[dvipsnames]{xcolor}
\usepackage[hyphens]{url}
\usepackage{graphicx}
\graphicspath{ {images/} }
\usepackage{tcolorbox}
\usepackage{enumitem}
\setcounter{section}{0}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage[pdftex,
            pdfauthor={Diego Alberto Barceló Nieves},
            pdftitle={Curso de Álgebra Lineal (2020-IV)}]{hyperref}
\usepackage{braket}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{caption}
\usepackage{subcaption}
\usetikzlibrary{babel}

\definecolor{NARANJA}{rgb}{1,0.467,0}
\definecolor{VERDE}{rgb}{0.31,1,0}
\definecolor{AZUL}{rgb}{0,0.53,1}
\definecolor{ROJO}{rgb}{1,0,0}

\hypersetup{
    colorlinks=true,
    linkcolor=ROJO,
    filecolor=magenta,      
    urlcolor=AZUL,
}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newenvironment{theorem}[2][Theorem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{teorema}[2][Teorema]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{lema}[2][Lema]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries}]}{\end{trivlist}}
\newenvironment{exercise}[2][Exercise]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{question}[2][Question]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{corollary}[2][Corollary]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{corolario}[2][Corolario]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{solution}{\begin{proof}[Solution]}{\end{proof}}
 
\pgfplotsset{compat=1.15}
 
\begin{document}

\title{Curso de Álgebra Lineal \\ (2021-I)}
\author{Diego Alberto Barceló Nieves\\ Licenciatura en Física Biomédica \\ Facultad de Ciencias \\ Universidad Nacional Autónoma de México}
\date{}
\maketitle

\iffalse
\section{Introducción}

La presente planeación del curso de Álgebra Lineal para la Licenciatura en Física Biomédica fue realizada tomando en cuenta el programa de la asignatura existente\footnote{El cual puede ser consultado en la página \url{http://www.fciencias.unam.mx/asignaturas/1330.pdf}.}, así como la retroalimentación del mismo por parte de estudiantes de la licenciatura.

%Al inicio del curso se procurará hacer un mayor énfasis en las estrechas relaciones existentes entre el álgebra lineal y el cálculo vectorial ya que, actualmente, se sugiere que las materias de Álgebra Lineal y Cálculo Avanzado se cursen simultáneamente. Además, durante todo el curso, se buscará sentar bases teóricas sólidas para diversas aplicaciones que serán útiles en materias posteriores del plan de estudios, tales como Ecuaciones Diferenciales (e.g., matrices, determinantes, eigenvalores y eigenvectores para aplicarse en sistemas de ecuaciones diferenciales), Matemáticas Avanzadas (e.g., bases ortogonales y ortonormales de funciones para ser aplicado en funciones especiales), Introducción a la Física Cuántica/Mecánica Cuántica (e.g., espacios de Hilbert, espacios duales y operadores hermitianos para aplicarse en el formalismo de la Mecánica Cuántica) e Imagenología Biomédica (e.g., matrices, transformaciones lineales y transformaciones rígidas para ser aplicado a procesamiento digital de imágenes).

Durante el curso, se sentarán bases teóricas sólidas de álgebra lineal para diversas aplicaciones que serán útiles en materias del plan de estudios, tales como Cálculo Avanzado, Ecuaciones Diferenciales, Matemáticas Avanzadas, Introducción a la Física Cuántica y Mecánica Cuántica, Algoritmos Computacionales y Física Computacional, entre otras.

\subsection{Metolodogía de enseñanza}

Este curso está diseñado para desarrollar las habilidades autodidactas de cada estudiante, a la vez que se les da acompañamiento en los temas del curso mediante sesiones cortas de asesoría en línea. Desde el principio del curso, l@s estudiantes tendrán acceso a notas del curso, que incluyen ejercicios de repaso para reforzar los aprendizajes: esto permitirá que cada quien avance a su ritmo. En conjunto con el grupo, se establecerán horarios fijos para atender dudas de la materia de manera virtual.

\subsection{Forma de evaluación}

La evaluación se hará a través de 4 tareas-examen, que se realizarán cada segundo viernes del curso intersemestral. Los exámenes se entregarán en equipos pequeños -los cuales serán asignados por el profesor y cambiados cada dos semanas. La calificación obtenida por el equipo para un examen será la misma que será asignada a cada participante para ese examen. Al final del curso, quienes tengan un promedio reprobatorio en la materia deberán presentar un examen final, que sustituirá sus calificaciones anteriores. Las personas con promedio aprobatorio también podrán optar por hacer el examen final para subir calificación; en ese caso, su calificación en el final reemplazará las anteriores.

\subsection{Temario del curso intersemestral por semana}

\textbf{Semana 1:} Campos, espacios vectoriales, subespacios vectoriales, producto escalar y norma, proyecciones y ortogonalidad.

\vspace{3mm}

\textbf{Semana 2:} Combinaciones lineales, subespacio generado y conjunto generador, dependencia e independencia lineal, bases y dimensión, ortogonalización y ortonormalización.

\vspace{3mm}

\textbf{Semana 3:} Transformaciones lineales, espacio de transformaciones lineales, núcleo e imagen de una transformación lineal, nulidad y rango, representación matricial de una transformación lineal.

\vspace{3mm}

\textbf{Semana 4:} Composición de transformaciones lineales y multiplicación de matrices, invertibilidad e isomorfismos, cambios de base, espacios duales.

\vspace{3mm}

\textbf{Semana 5:} El problema de la diagonalización, eigenvalores e eigenvectores, polinomio característico.

\vspace{3mm}

\textbf{Semana 6:} Diagonalizabilidad, subespacios invariantes y sumas directas.

\vspace{3mm}

\textbf{Semana 7:} Operadores lineales adjuntos, normales y autoadjuntos.

\vspace{3mm}

\textbf{Semana 8:} Operadores unitarios y ortogonales, proyecciones ortogonales y teorema espectral para operadores autoadjuntos.

\vspace{3mm}

\textbf{Nota:} Debido al poco tiempo del que disponemos para el curso intersemestral, se asumirá que l@s estudiantes dominan los siguientes temas del \href{https://web.fciencias.unam.mx/asignaturas/1130.pdf}{temario de Álgebra}: Matrices (definción y operaciones), matrices transpuestas, operaciones elementales y matrices elementales, rango de una matriz, matrices invertibles, cálculo de la inversa de una matriz invertible, determinante de una matriz cuadrada (definición y propiedades), cálculo de determinantes, soluciones  de  un  sistema de ecuaciones lineales, sistemas de ecuaciones lineales homogéneos, espacio  de soluciones de un sistema homogéneo, sistemas de ecuaciones lineales no homogéneos, criterios de existencia de soluciones y resolución de sistemas de ecuaciones lineales.

\subsection{Bibliografía recomendada para el curso} \label{Bibliografía}

\begin{itemize}
    \item S. H. Friedberg, \emph{Linear Algebra}, 4ta ed. (Pearson, 2014, EUA) - es el texto básico para este tipo de cursos.
    \item S. Lang, \emph{Linear Algebra}, 3a ed. (Springer, 1987, EUA) - buen complemento al Friedberg.
    \item D. Poole, \emph{Linear Algebra: A Modern Introduction}, 4ta ed. (Cengage Learning, 2015, EUA) - útil para quienes quieran ver algunas aplicaciones de los conceptos al mismo tiempo que los aprenden.
\end{itemize}{}

Les sugiero que hojeen \emph{todos} los libros recomendados al inicio del curso, y que consulten los de su agrado constantemente durante el mismo, o bien, busquen otros que les sirvan mejor para aprender.

\subsection{Otros recursos educativos}

No sólo se aprende de libros; hay que aprovechar todo el contenido que ofrece el internet para nuestra educación. A lo largo de los apuntes pondré hipervínculos a algunas páginas con recursos relevantes para el tema en cuestión; sin embargo, aquí enlistaré algunos recursos útiles para aprender álgebra lineal:

\begin{itemize}
    \item Lista de reproducción \href{https://www.youtube.com/watch?v=fNk_zzaMoSs&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab}{Essence of Linear Algebra} del canal de YouTube 3Blue1Brown.
    \item Libros de texto interactivos \href{{http://immersivemath.com/ila/learnmore.html}}{Immersive Linear Algebra} e \href{https://textbooks.math.gatech.edu/ila/index2.html}{Interactive Linear Algebra}, que sirven para generar intuición acerca de algunos conceptos del álgebra lineal.
    \item Series de videos de sobre álgebra lineal de \href{https://www.lem.ma/books/AIApowDnjlDDQrp-uOZVow/landing}{lemma} y \href{https://www.khanacademy.org/math/linear-algebra}{Khan Academy} con interfaces para resolver ejercicios al final de cada lección.
\end{itemize}

Por supuesto, les invito a que busquen más recursos por su propia cuenta; de encontrarlos, les agradecería que me notificaran para revisarlos.
\fi

\newpage
\textbf{Notación}
\begin{tcolorbox} \label{Notación}
\centering
\begin{tabular}{cc}
    \\
    $\mathbf{u}, \mathbf{v}, \mathbf{w}, ...$ & vectores (elementos de un conjunto vectorial $V$) \\ \\
    $a,b,c, ...$ & escalares (elementos de un campo $K$ que define un espacio vectorial) \\ \\
    $ab$ & producto entre los escalares $a$ y $b$ \\ \\
    $a\mathbf{u}$ & producto del vector $\mathbf{u}$ por el escalar $a$\footnote{Algunos textos se refieren a esta operación \textemdash realizada entre un vector y un escalar, y que da como resultado un vector\textemdash\hspace{1.5mm} como \textit{multiplicación escalar} (o \emph{scalar multiplication}, en inglés); sin embargo, es fácil que esta operación se confunda con la de \textit{producto escalar}, que da como resultado un escalar. Debemos tener esto en mente cuando leamos otros textos de álgebra lineal, tanto en español como en inglés.} \\ \\
    $(x_1,...\hspace{0.5mm},x_n) $ & coordenada como n-tupla \\ \\
    $\begin{pmatrix}x_1&...&x_n\end{pmatrix}$ & vector como n-tupla \\ \\
    $V + W$ & suma de los espacios vectoriales $V$ y $W$ \\ \\
    $V \oplus W$ & suma directa de los espacios vectoriales $V$ y $W$ \\ \\

    $\langle\mathbf{u},\mathbf{v}\rangle \equiv \mathbf{u}\cdot\mathbf{v}$ & producto escalar (punto) entre los vectores $\mathbf{u}$ y $\mathbf{v}$ \\ \\
    $\overline{a}$ & complejo conjugado de $a$ \\ \\
    $u_i v_i \equiv \sum_{i=1}^n u_i v_i$ & \textit{notación de Einstein} para la suma sobre un índice $i$ \\ \\
    $||\mathbf{u}||$ & norma del vector $\mathbf{u}$ \\ \\
    $P_{\mathbf{u}}(\mathbf{v})$ & proyección del vector $\mathbf{v}$ sobre el vector $\mathbf{u}$ \\ \\
    $\mathbf{u}\perp\mathbf{v}$ & ortogonalidad de los vectores $\mathbf{u}$ y $\mathbf{v}$ \\ \\
    $\mathbf{a}\times\mathbf{b}$ & producto vectorial (cruz) de dos vectores $\mathbf{a},\mathbf{b}\in\mathbb{R}^3$ \\ \\
    $\mathbf{a}\cdot\mathbf{b}\times\mathbf{c}$ & triple producto escalar entre tres vectores $\mathbf{a},\mathbf{b},\mathbf{c}\in\mathbb{R}^3$. \\ \\


    $\langle G \rangle $ & Espacio vectorial generado por $G$ \\ \\
    $l.i.$ & Conjunto linealmente independiente \\ \\
    $l.d.$ & Conjunto linealmente dependiente \\ \\

    $\text{dim}(V)$ & Dimensión del espacio vectorial $V$ \\ \\

\end{tabular}
\end{tcolorbox}

\newpage
\section{Campos, espacios vectoriales, interpretación geométrica de las operaciones esenciales de los espacios vectoriales y subespacios vectoriales} \label{Sec:1}

\subsection{Campos} \label{Subsec:Campos}
Uno de los conceptos más fundamentales del álgebra lineal es el de espacio vectorial; sin embargo, para definir formalmente un espacio vectorial se requiere de una estructura algebráica conocida como campo \textemdash el ejemplo más común siendo el \emph{campo de los números reales}. Esta estructura quizá la viste explícitamente en tu curso de Álgebra y/o implícitamente en tu curso de Cálculo I (a través de los \textit{axiomas de campo} \textemdash o de cuerpo\textemdash\hspace{1.5mm} \textit{para los números reales}); sin embargo, aquí mencionaremos su definición y los dos ejemplos de campos que más utilizaremos durante este curso (el campo de los números reales y el de los complejos) antes de definir los espacios vectoriales.

\subsubsection{Definición de campo} \label{Def:Campo}

\begin{tcolorbox}
\underline{Def.} Un \emph{campo} es un conjunto $K$ con dos operaciones (llamadas \emph{adición} o \emph{suma} y \emph{multiplicación}) que cumplen las propiedades siguientes:

\begin{center}
\begin{tabular}{lr}
    \\
    $\forall\hspace{1.5mm}a,b\in K\hspace{2.5mm} a+b=b+a;\hspace{1.5mm} ab = ba
    $ & Conmutatividad \\ \\
    $\forall\hspace{1.5mm}a,b,c\in K\hspace{2.5mm}a+(b+c)=(a+b)+c;\hspace{1.5mm}a(bc)=(ab)c$ & Asociatividad \\ \\
    $\exists\hspace{1.5mm} 0,1\in K$ t.q. $a+0=a$, $1a=a\hspace{2.5mm} \forall\hspace{1.5mm}a \in K$ & Elementos identidad (neutros) \\ \\
    $\forall\hspace{1.5mm}a\in K\hspace{2.5mm}\exists\hspace{0.5mm}-a\in K$ t.q. $a + (-a) = 0$ & Elemento inverso de la suma \\ \\
    $\forall\hspace{1.5mm}a\neq0\in K \hspace{2.5mm} \exists\hspace{1.5mm}a^{-1}\in K$ t.q. $a(a^{-1})= 1$ & Elemento inverso de la multiplicación \\ \\
    $\forall\hspace{1.5mm}a,b,c\in K\hspace{2.5mm}a(b+c) = ab+ac$ & Distributividad.\\ \\
\end{tabular}
\end{center}

\noindent A estas propiedades también se les conoce como \emph{axiomas de campo}\footnote{Nótese que la propiedad de distributividad es la única que \emph{combina} ambas operaciones definidas en el campo.}. Por otro lado, a pesar de que la definición de campo incluya un conjunto $K$ y dos operaciones que cumplen las propiedades anteriormente mencionadas, por simplicidad, se suele denotar a todo el campo como $K$.
\end{tcolorbox}

\subsubsection{El campo real}

El conjunto de los números reales $\mathbb{R}$ junto con las operaciones de suma y multiplicación (que aprendimos de forma intuitiva durante nuestra educación básica) cumplen todas las propiedades enlistadas en la sección \ref{Def:Campo}, ya que dichas operaciones son conmutativas, asociativas y cumplen la propiedad de distributividad. El elemento identidad (neutro) de la suma es $0\in\mathbb{R}$ y el de la multiplicación es $1\in\mathbb{R}$. Para todo número $a\in\mathbb{R}$, el elemento inverso de la suma es $-a$, mientras que para todo $b\neq0\in\mathbb{R}$, el elemento inverso de la multiplicación es $b^{-1} = \frac{1}{b}\in\mathbb{R}$. Al conjunto $\mathbb{R}$ junto con estas dos operaciones se le conoce como el \emph{campo de los números reales} o, simplemente, el \emph{campo real}.

\subsubsection{El campo complejo} \label{Ejem:Campo_complejo}

El conjunto de los números complejos se define como $\mathbb{C} \equiv \{a+ib\hspace{1.5mm}|\hspace{1.5mm}a,b\in\mathbb{R}\}$, donde $i\equiv+\sqrt{-1}$. Definiendo la suma entre números complejos como $(a+ib)+(q+ir)\equiv(a+q) + i(b+r)$, la multiplicación entre números complejos como\footnote{Nótese que esta definición es simplemente el resultado de desarrollar $(a+ib)(c+id)$ como un producto de binomios.} $(a+ib)(q+ir)\equiv (aq-br) + i(ar+bq)$ y apoyándonos en el campo real, podemos comprobar que estas dos operaciones junto con el conjunto $\mathbb{C}$ forman un campo. A los números complejos de la forma $0+ia, a\in\mathbb{R}$ se les conoce como números \emph{imaginarios} o \emph{laterales}\footnote{El mismísimo Gauss hubiera preferido este segundo nombre, ya que creía que era mucho más intuitivo, y que llamarlos \emph{imaginarios} les dotaba de una opacidad misteriosa e innecesaria. Sugiero ver este video introductorio (o la serie completa, llamada \emph{Imaginary Numbers Are Real}) para perderles el miedo: \url{https://www.youtube.com/watch?v=T647CGsuOVU}.}. A continuación, demostraremos que el conjunto $\mathbb{C}$ con las dos operaciones definidas previamente cumplen las primeras tres propiedades de los campos enlistadas en la sección \ref{Def:Campo}.

Sean $a,b,q,r,s,t$ elementos del campo $\mathbb{R}$ tal que $a+ib, q+ir, s+it\in\mathbb{C}.$

Ya que $a,b,q,r$ pertenecen al campo $\mathbb{R}$, entonces las sumas y multiplicaciones entre estos elementos son conmutativas. Por lo tanto, tenemos que $$(a+ib)+(q+ir)=(a+q)+i(b+r)=(q+a)+i(r+b)=(q+ir)+(a+ib),$$\noindent por lo cual la suma en $\mathbb{C}$ que definimos en el párrafo anterior es conmutativa, y que $$(a+ib)(s+it)=(as-bt)+i(bs+at)=(sa-tb)+i(sb+ta)=(s+it)(a+ib),$$\noindent por lo cual la multiplicación en $\mathbb{C}$ que definimos también es conmutativa.

Nuevamente, ya que $a,b,r,q,s,t$ pertenecen al campo $\mathbb{R}$, entonces las sumas y multiplicaciones entre estos elementos son asociativas. Por ende, tenemos que $$(a+ib)+((q+ir)+(s+it))=(a+ib)+((q+s)+i(r+t))=(a+q+s)+i(b+r+t)=$$ $$((a+q)+s)+i((b+r)+t)=((a+q)+i(b+r))+(s+it)=((a+ib)+(q+ir))+(s+it),$$ por lo cual la suma en $\mathbb{C}$ es asociativa, y que $$(a+ib)((q+ir)(s+it))=(a+ib)((qs-rt)+i(qt+rs))=((a(qs-rt)-b(qt+rs))+i(a(qt+rs)+b(qs-rt))=$$ $$(aqs-art-bqt-brs)+i(aqt+ars+bqs-brt)=(aqs-brs-art-bqt)+i(aqt-brt+ars+bqs)=$$ $$((aq-br)s-(ar+bq)t)+i((aq-br)t+(ar+bq)s)=((aq-br)+i(ar+bq))(s+it)=((a+ib)(q+ir))(s+it)),$$\noindent por lo cual concluimos que la multiplicación también es asociativa.

Por otro lado, recordando la definición de $\mathbb{C}=\{a+ib\hspace{1.5mm}|\hspace{1.5mm}a,b\in\mathbb{R}\}$ vemos que, en particular, $0+i0\in\mathbb{C}$ y $1+i0\in\mathbb{C}$. Observemos que $$(a+ib)+(0+i0)=((a+0)+i(b+0))=(a+ib),$$ por lo cual existe un elemento identidad de la suma (neutro aditivo) en este campo, y que $$(1+i0)(q+ir)=(1q-0r+i(1r+0q))=((q-0)+i(r+0))=(q+ir),$$ \noindent por lo cual también existe un elemento identidad de la multiplicación (neutro multiplicativo); por simplicidad, escribimos $0+i0$ como $0$ y $1+i0$ como $1$: se pueden identifiicar con los elementos neutros del campo real. Las últimas tres propiedades de la definición de campo de la sección \ref{Def:Campo} se demuestran de manera similar, y se dejan como ejercicio. A este campo se le conoce como el \emph{campo de los números complejos} o \emph{campo complejo}.

Observemos que prácticamente todas las operaciones que realizamos en nuestra vida cotidiana como calcular fechas, dar o recibir cambio, aproximar áreas, repartir comida, etc., toman lugar en un campo. Es decir, las ideas intuitivas que nos formamos durante la educación básica de que la suma siempre debe ser conmutativa y asociativa \textemdash al igual que la multiplicación\textemdash\hspace{0.5mm}, que existe la resta y la división, que el $0$ y el $1$ son números \emph{especiales} en cierto sentido y que siempre se cumple la propiedad de distributividad, son un \emph{hecho} para cualquier estructura de campo. Sin embargo, estas mismas ideas intuitivas \emph{no siempre se cumplen en otros tipos de estructuras algebráicas}\textemdash algunas de las cuales veremos más adelante\textemdash\hspace{0.5mm}, ¡así que no te confíes!

\newpage
\subsection{Espacios vectoriales} \label{Subsec:Espacios_vectoriales}

Un espacio vectorial es una estructura algebráica abstracta que cumple una serie de propiedades específicas que veremos en el siguiente apartado. Dicha estructura tiene una gran variedad de aplicaciones en muchas áreas de las matemáticas, la física, la computación y la biomedicina, por lo cual, para arrancar el curso, es vital su comprensión desde un punto de vista teórico.

\subsubsection{Definición de espacio vectorial} \label{Def:Espacio_vectorial}

\begin{tcolorbox} \label{Def:Espacio_vectorial}
\underline{Def.} Un \textit{espacio vectorial} sobre un campo\footnote{Ver sec. \ref{Def:Campo}.} $K$ es un conjunto $V$ con dos operaciones (llamadas \textit{adición} o \textit{suma vectorial} y \textit{producto de un vector por un escalar}) que satisfacen las siguientes propiedades:

\begin{center}
\begin{tabular}{lr}
    $\forall\hspace{1.5mm} \mathbf{u},\mathbf{v}\in V \hspace{3mm}\exists \hspace{1.5mm} \mathbf{u}+\mathbf{v}\in V$ & Cerradura de la adición \\ \\ \multirow{2}{0.4\textwidth}{$\forall\hspace{1.5mm} \mathbf{v}\in V, a\in K \hspace{3mm}\exists \hspace{1.5mm} a\mathbf{v}\in V$} & \multirow{2}{0.28\textwidth}{Cerradura del producto de un vector por un escalar} \\ \\ \\
    $\forall\hspace{1.5mm} \mathbf{u},\mathbf{v},\mathbf{w}\in V\hspace{3mm}\mathbf{u}+(\mathbf{v}+\mathbf{w})=(\mathbf{u}+\mathbf{v})+\mathbf{w}$  & Asociatividad de la adición\\ \\
    $\forall\hspace{1.5mm} \mathbf{u},\mathbf{v}\in V\hspace{3mm}\mathbf{u}+\mathbf{v}=\mathbf{v}+\mathbf{u}$ & Conmutatividad de la adición \\ \\
    $\exists \hspace{1.5mm} \mathbf{0}\in V$ t.q. $\mathbf{v}+\mathbf{0}=\mathbf{v}\hspace{3mm}\forall\hspace{1.5mm} \mathbf{v} \in V$ & Elemento identidad de la adición (neutro aditivo) \\ \\
    $\forall\hspace{1.5mm}\mathbf{v}\in V \hspace{3mm}\exists\hspace{1.5mm} -\mathbf{v}\in V$ t.q. $\mathbf{v}+(-\mathbf{v})=\mathbf{0}$ & Elemento inverso de la adición (inverso aditivo) \\ \\
    \multirow{2}{0.35\textwidth}{$a(b\mathbf{v})=(ab)\mathbf{v}\hspace{3mm}\forall a,b\in K, \mathbf{v}\in V$} & \multirow{2}{0.47\textwidth}{Compatibilidad del producto de un vector por un escalar con el producto entre escalares} \\ \\ \\
    \multirow{2}{0.4\textwidth}{$\exists\hspace{1.5mm}1\in K$ \hspace{1.5mm} t.q. $\hspace{1.5mm}1\mathbf{v}=\mathbf{v}\hspace{3mm}\forall\hspace{1.5mm} \mathbf{v}\in V$} & \multirow{2}{0.35\textwidth}{Elemento identidad del producto de un vector por un escalar} \\ \\ \\
    \multirow{2}{0.4\textwidth}{$a(\mathbf{v}+\mathbf{w})=a\mathbf{v}+a\mathbf{w}\hspace{3mm}\forall\hspace{1.5mm} \mathbf{v},\mathbf{w}\in V, a\in K$} & \multirow{2}{0.47\textwidth}{Distributividad del producto de un vector por un escalar con respecto a la adición vectorial}  \\ \\ \\
    \multirow{2}{0.4\textwidth}{$(a+b)\mathbf{v}=a\mathbf{v}+b\mathbf{v}\hspace{3mm}\forall\hspace{1.5mm} a,b\in K, \mathbf{v}\in V$} & \multirow{2}{0.47\textwidth}{Distributividad del producto de un vector por un escalar con respecto a la suma escalar.} \\ \\
\end{tabular}
\end{center}

\hspace{2.5mm} A los elementos $a,b \in K$ del campo utilizado para definir el espacio vectorial se les llama \textit{escalares} y los elementos $\mathbf{u},\mathbf{v},\mathbf{w}\in V$ que cumplen todas las propiedades anteriores son llamados \textit{vectores}. A las propiedades anteriores también se les conoce como \textit{axiomas de espacio vectorial}.

\end{tcolorbox}

Partiendo de esta definición, podemos hacer varias observaciones:

\begin{itemize}
    \item La definición matemática de \textit{vectores} como \textit{elementos cualesquiera de un conjunto V que \textemdash junto con un campo $K$ y las operaciones $+:V\times V\to V$ (suma vectorial) y $\hspace{1mm} \cdot:K\times V\to V$ (producto de un vector por un escalar) \textemdash\hspace{0.5mm} cumplen las propiedades de un espacio vectorial} es muy distinta a la definición de vector como \textit{elemento con magnitud, dirección y sentido (y, más precisamente, que además es invariante bajo rotaciones propias e impropias)} utilizada en algunas áreas de la física, siendo la primera definición más general.
    \item La definición de \textit{espacio vectorial} incluye dos operaciones \textit{nuevas} (con respecto a las operaciones de campo) con una importante diferencia entre ellas: una es sólamente entre los elementos del conjunto $V$ (adición o suma vectorial) y, la otra, entre los elementos del conjunto $V$ y el campo $K$ (producto de un vector por un escalar)\footnote{Más adelante veremos otras operaciones que se pueden definir entre vectores y escalares, pero las dos que hemos visto hasta ahora son las únicas necesarias para \textit{definir} los espacios vectoriales. Por ende, más adelante podremos referirnos a ellas como las operaciones \emph{esenciales} de los espacios vectoriales.}. Sin embargo, \emph{ambas dan como resultado un vector en $V$}.
    \item Así como la definición de \textit{campo} incluye un conjunto $K$ con dos operaciones (suma y producto) entre sus elementos que cumplen propiedades específicas, la definición de \textit{espacio vectorial} incluye un conjunto $V$ y un campo $K$ con dos operaciones (suma vectorial y producto de un vector por un escalar) entre sus elementos que cumplen propiedades específicas. Por simplicidad, al campo se le denota como $K$ y al espacio vectorial, como $V$.
    
\end{itemize}{}

Para complementar la discusión al respecto de qué es un vector y apreciar cómo funcionan las operaciones de los espacios vectoriales (suma vectorial y producto de un vector por un escalar) de manera visual, sugiero ver el siguiente video: \url{https://www.youtube.com/watch?v=fNk_zzaMoSs}.

\subsubsection{Ejemplos de espacios vectoriales} \label{Ejem:Espacios_vectoriales}

El producto cartesiano del conjunto de los números reales consigo mismo $\mathbb{R}\times\mathbb{R}$ (o $\mathbb{R}^2$) sobre el campo $\mathbb{R}$ es un espacio vectorial, ya que los elementos de $\mathbb{R}^2$ (conocidos como \textit{pares ordenados} o \textit{2-tuplas}) junto con los de $\mathbb{R}$ satisfacen la definición de la sección \ref{Def:Espacio_vectorial}. Usualmente, en geometría analítica, estos pares ordenados se escriben en forma de \emph{coordenadas} $(x,y)$ con $x,y \in \mathbb{R}$ y tienen una correspondencia uno a uno con \emph{puntos} en el plano cartesiano; sin embargo, en álgebra lineal, cuando hablemos de pares ordenados como \emph{vectores} de $\mathbb{R}^2$, es preferible emplear la notación\footnote{A esto se le conoce como \emph{escribir un vector como una columna de matriz} y facilita la notación al momento de hacer productos de matrices por la izquierda con vectores, como haremos más adelante en el curso.} $\textbf{x}=\begin{pmatrix}x_1\\x_2\end{pmatrix}\in\mathbb{R}^2,\hspace{1.5mm}x_1,x_2\in\mathbb{R}$ (lo cual también puede escribirse como $\begin{pmatrix}x_1&x_2\end{pmatrix}^T$ ó, simplemente, $\begin{pmatrix}x_1&x_2\end{pmatrix}$). Estos vectores se pueden representar visualmente mediante una correspondencia uno a uno con \emph{flechas} en el plano cartesiano, las cuales \emph{tienen su cola en el origen} y \emph{su punta en el punto correspondiente a las coordenadas} $(x_1,x_2)$. La suma vectorial se define naturalmente como $\begin{pmatrix}x_1\\x_2\end{pmatrix}+\begin{pmatrix}y_1\\y_2\end{pmatrix}=\begin{pmatrix}x_1+y_1\\x_2+y_2\end{pmatrix}$. El elemento identidad de la suma (neutro aditivo) es el \emph{vector origen} $\mathbf{0}=\begin{pmatrix}0\\0\end{pmatrix}$ y el inverso aditivo de un vector $\mathbf{u}=\begin{pmatrix}u_1\\u_2\end{pmatrix}$ es $\begin{pmatrix}-u_1\\-u_2\end{pmatrix}$, denotado como $-\mathbf{u}$. En este caso, el producto de un vector $\textbf{v}=\begin{pmatrix}v_1\\v_2\end{pmatrix}\in\mathbb{R}^2$ por un escalar $a\in\mathbb{R}$ se define como $a\textbf{v}=a\begin{pmatrix}v_1\\v_2\end{pmatrix} \equiv \begin{pmatrix}av_1\\av_2\end{pmatrix}$ (ó $a\begin{pmatrix}v_1&v_2\end{pmatrix} \equiv \begin{pmatrix}av_1&av_2\end{pmatrix}$) y el elemento de identidad de esta operación es el escalar $1\in\mathbb{R}$.

\vspace{3mm}

El espacio vectorial de $\mathbb{R}^3$ sobre $\mathbb{R}$ se define de manera análoga. Los vectores de $\mathbb{R}^3$ también se pueden representar como flechas que parten del origen de un espacio tridimensional, en una correspondencia uno a uno con las coordenadas del espacio tridimensional. Este tipo de espacios vectoriales se pueden generalizar de manera abstracta (i.e., no visual), como veremos en el siguiente ejemplo.

\vspace{3mm}

El conjunto obtenido al realizar un producto cartesiano de un número entero positivo $n$ de conjuntos $\mathbb{R}$ ($\mathbb{R}\times\mathbb{R}\times...\times\mathbb{R}) = \mathbb{R}^n$ sobre el campo $\mathbb{R}$ también es un espacio vectorial. Sus elementos vectoriales son de la forma $\mathbf{v} = \begin{pmatrix}v_1&v_2& ... & v_n\end{pmatrix}, \hspace{1.5mm} v_i \in \mathbb{R}, \hspace{1.5mm} i \in \{1,2,...,n\}$ y son conocidos como \textit{n-tuplas}\footnote{Nótese que inclusive en el caso $n=1$ el conjunto $\mathbb{R}$ sobre sí mismo forma un espacio vectorial. Es decir, en este caso, $\mathbb{R}$ funciona como conjunto vectorial y como campo.}. Las operaciones entre los vectores de $\mathbb{R}^n$ y con los escalares en $\mathbb{R}$ se definen de manera análoga al ejemplo de $\mathbb{R}^2$. A este tipo de espacios vectoriales les llamamos \textit{espacios vectoriales reales}, de acuerdo con la definición siguiente:

\vspace{1.5mm} 

\begin{tcolorbox}
\underline{Def.} Un \textit{espacio vectorial real} es aquel definido sobre el campo $\mathbb{R}$ (campo real) o, equivalentemente, aquel donde los escalares son números reales.
\end{tcolorbox}{}

El conjunto de todas las funciones polinomiales de una variable real de grado $n$ (i.e., con regla de correspondencia de la forma $f(x) = c_1 x^1 + c_2 x^2 + ... + c_n x^n, \hspace{1.5mm} c_i \in \mathbb{R}, \hspace{1.5mm} i \in \{1,2,...,n\}$) y con un mismo dominio $\mathcal{D}$ forma un espacio vectorial sobre el campo $\mathbb{R}$. Aquí, las definiciones de suma vectorial y de producto de un vector por un escalar se siguen naturalmente de la definición de la suma de funciones $(f+g)(x)\equiv f(x)+g(x)$ y del producto de una función arbitraria $f(x)$ por una función constante $a$, respectivamente, vistas en cálculo \textemdash las cuales aplican para la intersecciones de los dominios. El elemento identidad de la suma vectorial (neutro aditivo) es la función constante cero $f(x)=0\hspace{2.5mm} \forall\hspace{0.5mm}x\in\mathcal{D}$ y el inverso aditivo de una función $g(x)$ es $-g(x)$. Observemos que, en este caso, los \textit{vectores} de nuestro espacio vectorial \textit{son funciones} (en particular, en este ejemplo, son funciones polinomiales).

\vspace{3mm}

El conjunto de todas las funciones de una variable real derivables y con derivada continua (i.e., funciones de clase $C^1$) sobre el campo $\mathbb{R}$ forma un espacio vectorial\footnote{En general, el conjunto de funciones de clase $C^n$ sobre el campo $\mathbb{R}$ forma un espacio vectorial.}. Esto probablemente lo viste de manera implícita en tu curso de cálculo diferencial de una variable, cuando viste los teoremas de derivada de una suma/multiplicación/división de funciones (también conocido como \emph{álgebra de derivadas}) para funciones de este tipo. Las operaciones en este espacio vectorial, así como los elementos identidad (neutros) e inversos, se definen de la misma forma que en el ejemplo de las funciones polinomiales.

\vspace{3mm}

El conjunto $\mathbb{C}\times\mathbb{C}$ ($\mathbb{C}^2$) sobre el campo $\mathbb{C}$ también es un espacio vectorial. Sus vectores son de la forma $\begin{pmatrix}a+ib&c+id\end{pmatrix}$ con $a,b,c,d\in\mathbb{R}$ e $i\equiv+\sqrt{-1}$ (ya que esto implica que $a+ib, c+id\in\mathbb{C}$, es decir, que sus entradas son complejas). El elemento identidad de la suma vectorial es $\mathbf{0}=\begin{pmatrix}0+i0&0+i0\end{pmatrix}$ y el del producto de un vector por un escalar es $1 + i0\in\mathbb{C}$; las operaciones en $\mathbb{C}^2$ y $\mathbb{C}$ se definen como las del ejemplo de $\mathbb{R}^2$ y $\mathbb{R}$. Análogamente, el conjunto $\mathbb{C}^n$ sobre el campo $\mathbb{C}$ es un espacio vectorial: sus vectores tienen $n$ entradas complejas y sus escalares también son complejos. A este tipo de espacio vectorial le llamamos \textit{espacio vectorial complejo}, de acuerdo a la siguiente definición:

\vspace{1.5mm} 
\begin{tcolorbox}
\underline{Def.} Un \textit{espacio vectorial complejo} es aquel definido sobre el campo $\mathbb{C}$ (campo complejo) o, equivalentemente, aquel donde los escalares son números complejos.
\end{tcolorbox}{}

Nota: no podemos visualizar los vectores de $\mathbb{R}^n$ con $n>3$, los de $\mathbb{C}^m$ con $m>1$, ni los del conjunto de funciones de clase $C^1$, etc., como \emph{flechas que parten de un mismo origen}\footnote{Es posible visualizar vectores más abstractos de otras formas: \url{https://www.youtube.com/watch?v=zwAD6dRSVyI}.}. Sin embargo, sí podemos hacer operaciones entre estos vectores de manera análoga a como lo haríamos con vectores de $\mathbb{R}^2$ o $\mathbb{R}^3$, por lo cual trabajar en estos espacios \emph{visualizables} puede ayudarnos a generar intuición sobre espacios vectoriales más abstractos.

\vspace{3mm}
Para ver más ejemplos de espacios vectoriales pueden revisar, por ejemplo, \textit{Linear Algebra} de Friedberg (págs. 8-11.) o \textit{Linear Algebra: A Modern Introduction} de Poole (págs. 430-432), entre otros.

\subsubsection{Algunos teoremas de espacios vectoriales} \label{Teo:Espacios_vectoriales} 

\begin{teorema} {1.2.3.1}
Sean $\mathbf{x},\mathbf{y},\mathbf{z}$ vectores de $V$ tales que $\mathbf{x}+\mathbf{z}=\mathbf{y}+\mathbf{z}$, entonces $\mathbf{x}=\mathbf{y}$.

\begin{proof}
Ya que $\mathbf{z}$ es un vector de $V\implies$ $\exists\hspace{2mm} \mathbf{-z}\in V$ tal que $\mathbf{z} + (-\mathbf{z}) = \mathbf{0}$, por la propiedad de existencia de inversos aditivos de los espacios vectoriales. Sumando este elemento $-\mathbf{z}$ a cada lado de la igualdad inicial, tenemos que $$\mathbf{x}+\mathbf{z}=\mathbf{y}+\mathbf{z}\iff\mathbf{x}+\mathbf{z}+ (-\mathbf{z})=\mathbf{y}+\mathbf{z}+ (-\mathbf{z})\iff\mathbf{x}+(\mathbf{z}+ (-\mathbf{z}))=\mathbf{y}+(\mathbf{z}+ (-\mathbf{z}))\iff$$ $$ \mathbf{x}+\mathbf{0}=\mathbf{y}+\mathbf{0}\iff\mathbf{x}=\mathbf{y},$$

\noindent donde en la tercera igualdad se aplicó la propiedad asociativa de la suma vectorial, en la cuarta igualdad se aplicó la propiedad de existencia de los inversos aditivos y, en la última, se aplicó la propiedad de existencia de neutro aditivo.
\end{proof}
A este teorema se le conoce como \emph{Ley de cancelación para la suma vectorial} y con él se puede demostrar la unicidad del nuetro aditivo y de los inversos aditivos.
\end{teorema}

\begin{teorema} {1.2.3.2}
Sea $V$ sobre $K$ un espacio vectorial arbitrario con $\mathbf{v}\in V$ y $a\in K$, entonces se verifica que:

\begin{enumerate}
    \item $0\mathbf{v}=\mathbf{0}$
    \item $a\mathbf{0}=\mathbf{0}$
    \item $(-a)\mathbf{v}=-(a\mathbf{v})=a(-\mathbf{v})$
\end{enumerate}

\begin{proof}
\begin{enumerate}
    \item $0\mathbf{v}+0\mathbf{v}=(0+0)\mathbf{v}=0\mathbf{v}=0\mathbf{v}+\mathbf{0}\iff0\mathbf{v}=\mathbf{0}$, donde se aplicaron las propiedades de distributividad del producto de un vector por un escalar con respecto a la suma \emph{escalar}, existencia del neutro aditivo y la ley de cancelación de la suma vectorial (Teorema 1.2.3.1).
    \item $a\mathbf{0}+a\mathbf{0}=a(\mathbf{0}+\mathbf{0})=a\mathbf{0}=a\mathbf{0}+\mathbf{0}\iff a\mathbf{0}=\mathbf{0}$, donde se aplicaron las propiedades de distributividad del producto de un vector por un escalar con respecto a la adición \emph{vectorial}, existencia del neutro aditivo y la ley de cancelación de la suma vectorial.
    \item Por el primer inciso y por distributividad, $\mathbf{0} = (0)\mathbf{v} = (a+(-a))\mathbf{v}=a\mathbf{v}+(-a)\mathbf{v}$\footnote{La existencia de $-a\in K$ está asegurada ya que $K$ es un campo (ver sec. \ref{Def:Campo}).}, por lo cual $a\mathbf{v}+(-a)\mathbf{v}=\mathbf{0}$. Por otro lado, por la propiedad de cerradura del producto de un vector por un escalar $a\mathbf{v}\in V$ y, por la existencia de inversos aditivos, $\exists\hspace{1mm}-(a\mathbf{v})\in V$ tal que $a\mathbf{v}+[-(a\mathbf{v})]=\mathbf{0}$. Por lo tanto, tenemos que $\mathbf{0}=\mathbf{0}\iff a\mathbf{v}+(-a)\mathbf{v}=a\mathbf{v}+[-(a\mathbf{v})]\iff (-a)\mathbf{v}=-(a\mathbf{v})$ por la ley de cancelación. En particular, $(-1)\mathbf{v}=-\mathbf{v}$; por la propiedad de compatibilidad del producto de un vector por un escalar con el producto entre escalares, se sigue que $a(-\mathbf{v})=a[(-1)\mathbf{v}]=[a(-1)]\mathbf{v}=(-a)\mathbf{v}=-(a\mathbf{v})$.
\end{enumerate}
\end{proof}
\end{teorema}

\newpage
\subsection{Interpretación geométrica de las operaciones esenciales de los espacios vectoriales} \label{Subsec:Interpretación_geométrica_de_las_operaciones_de_los_espacios_vectoriales}

Como se mencionó en una nota al final de la sección \ref{Ejem:Espacios_vectoriales} \hspace{1.5mm}\textemdash de la cual retomaremos muchas ideas a continuación\textemdash, podemos desarrollar nuestra intuición sobre muchos temas del álgebra lineal trabajando en espacios vectoriales \emph{visualizables}, para luego extenderla a espacios vectoriales más generales. Por ende, ahora haremos hincapié en la interpretacción geométrica de las operaciones de suma vectorial y producto de un vector por un escalar en los espacios vectoriales reales $\mathbb{R}^2$ y $\mathbb{R}^3$, así como en el espacio vectorial complejo $\mathbb{C}$. Antes de empezar, necesitamos recordar la siguiente definición.

\vspace{1.5mm}

\begin{tcolorbox}
 \underline{Def.} Un \emph{par ordenado} (o \emph{$2$-tupla}) es un par de números $(a,b)$ en donde el orden de los números importa; mátematicamente, decimos que el par ordenado $(a,b)\neq (b,a)\iff b\neq a$\footnote{Observemos que esto implicaría que $(a,b)=(b,a)\iff b=a$, lo cual tiene sentido ya que, si ambos números son el mismo, es imposible distinguir el orden. Esta es una definición equivalente de par ordenado.}.
\end{tcolorbox}{}

\subsubsection{En el espacio vectorial real \texorpdfstring{$\mathbb{R}^2$}{TEXT}} \label{Ejem:En_R^2}

En geometría analítica aprendimos que, con la ayuda de un sistema de coordenadas, podemos formar una correspondencia uno a uno (o \emph{biunívoca}) entre los pares ordenados $(a,b)$ de entradas reales\footnote{Es decir, con $a,b\in\mathbb{R}$.} y los puntos del plano cartesiano $\mathbb{R}\times\mathbb{R}$. En particular, si tomamos el sistema de coordenadas cartesianas, entonces a cualquier par ordenado de entradas reales $(a,b)$ le corresponde un punto en el plano cartesiano $\mathbb{R}\times\mathbb{R}$ con coordenadas cartesianas $(a,b)$, y vice versa. De manera análoga, en álgebra lineal, cada vector $\begin{pmatrix}a&b\end{pmatrix} \in \mathbb{R}^2$ tiene una correspondencia biunívoca con una flecha en el plano cartesiano que tiene cola en el origen y punta en la coordenada cartesiana $(a,b)$ correspondiente. Ambas correspondencias se muestran en la Figura \ref{fig:Correspondencias_del_plano_cartesiano}.

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}[thick,scale=0.8, every node/.style={scale=1}]
        \draw[thick,<->] (-4,0) -- (4,0);
        \draw[thick,<->] (0,-4) -- (0,4);
        \draw[step=1cm,gray,very thin,dashed] (-3.9,-3.9) grid (3.9,3.9);
        \foreach \x in {1,2,3}
            \draw (\x cm,1pt) -- (\x cm,-1pt) node[anchor=north] {$\x$};
        \foreach \y in {1,2,3}
            \draw (1pt,\y cm) -- (-1pt,\y cm) node[anchor=east] {$\y$};
        \foreach \x in {-3,-2,-1}
            \draw (\x cm, 1pt) -- (\x cm, -1pt);
        \foreach \y in {-3,-2,-1}
            \draw (1pt,\y cm) -- (-1pt,\y cm);
        \filldraw[black] (0,0) circle (2pt) node[] at (-0.6,0.35) {\small{$(0,0)$}};
        \filldraw[NARANJA] (2,3) circle (2pt) node[anchor=south east] {$(2,3)$};
        \draw[ROJO,very thick,->] (0,0) -- (2,3) node[] at (3,2.5) {$\begin{pmatrix} 2 & 3 \end{pmatrix}$};
        \filldraw[VERDE] (-3.5,-2) circle (2pt) node[anchor=south east]{$(-3.5,-2)$};
        \draw[AZUL,very thick,->] (0,0) -- (-3.5,-2) node[] at (-2,-2.5) {$\begin{pmatrix} -3.5 & -2 \end{pmatrix}$};
    \end{tikzpicture}
    \caption{Ejemplo de representación de pares ordenados y vectores en el plano cartesiano. Los pares ordenados $(-3.5,-2)$ y $(2,3)$ se representan mediante puntos que corresponden precisamente a las coordenadas cartesianas $(-3.5,-2)$ y $(2,3)$, respectivamente, mientras que los vectores $\protect\begin{pmatrix} -3.5 & -2 \protect\end{pmatrix}$ y $\protect\begin{pmatrix} 2 & 3 \protect\end{pmatrix}$ son representados por flechas que tienen su cola en el origen del plano cartesiano y su punta en las coordenadas $(-3.5,-2)$ y $(2,3)$, respectivamente.} 
    \label{fig:Correspondencias_del_plano_cartesiano}
\end{figure}

\vspace{3mm}
\textbf{Suma vectorial}
\vspace{3mm}

    En este espacio, la suma vectorial se define como $\begin{pmatrix}a&b\end{pmatrix}+\begin{pmatrix}c&d\end{pmatrix}\equiv\begin{pmatrix}a+c&b+d\end{pmatrix}$. Podemos calcular, por ejemplo, la suma $\begin{pmatrix}2&1\end{pmatrix}+\begin{pmatrix}1&3\end{pmatrix}=\begin{pmatrix}3&4\end{pmatrix}$. Los tres vectores mencionados se muestran en la Figura \ref{fig:Suma_vectorial}.

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}[thick,scale=1.6, every node/.style={scale=1}]
        \draw[thick,->] (0,0) -- (4.5,0);
        \draw[thick,->] (0,0) -- (0,4.5);
        \draw[step=1cm,gray,thin,dashed] (0,0) grid (4.4,4.4);
        \foreach \x in {0,1,2,3,4}
            \draw (\x cm,1pt) -- (\x cm, -1pt) node[anchor=north] {$\x$};
        \foreach \y in {1,2,3,4}
            \draw (1pt,\y cm) -- (-1pt,\y cm) node[anchor=east] {$\y$};
        
            \draw[ROJO,very thick,->] (0,0) -- (2,1) node[] at (2.05,0.5){$\begin{pmatrix} 2 & 1 \end{pmatrix}$};
            \draw[AZUL,very thick,->] (0,0) -- (1,3) node[] at (0.55,3) {$\begin{pmatrix} 1 & 3 \end{pmatrix}$};
            \draw[violet,very thick,->] (0,0) -- (3,4) node[] at (3,4.2) {$\begin{pmatrix} 3 & 4 \end{pmatrix}$};
            \draw[] node[] at (4.2,4.2) {\textbf{a)}};
            \draw[black,thick,->]  (5,2) -- (5.5,2);
    \end{tikzpicture} \hspace{0.5cm} \begin{tikzpicture}[thick,scale=1.6, every node/.style={scale=1}]
        \draw[thick,->] (0,0) -- (4.5,0);
        \draw[thick,->] (0,0) -- (0,4.5);
        \draw[step=1cm,gray,thin,dashed] (0,0) grid (4.4,4.4);
        \foreach \x in {0,1,2,3,4}
            \draw (\x cm,1pt) -- (\x cm, -1pt) node[anchor=north] {$\x$};
        \foreach \y in {1,2,3,4}
            \draw (1pt,\y cm) -- (-1pt,\y cm) node[anchor=east] {$\y$};
            \draw[ROJO,very thick,->] (0,0) -- (2,1) node[] at (2.05,0.5){$\begin{pmatrix} 2 & 1 \end{pmatrix}$};
            \draw[AZUL,dashed,->] (2,1) -- (3,4) node[] at (3,2.4) {$\begin{pmatrix} 1 & 3 \end{pmatrix}$};
            \draw[AZUL,very thick,->] (0,0) -- (1,3) node[] at (0.55,3) {$\begin{pmatrix} 1 & 3 \end{pmatrix}$};
            \draw[ROJO,dashed,->] (1,3) -- (3,4) node[] at (1.7,3.8) {$\begin{pmatrix} 2 & 1 \end{pmatrix}$};
            \draw[violet,very thick,->] (0,0) -- (3,4) node[] at (3,4.2) {$\begin{pmatrix} 3 & 4 \end{pmatrix}$};
            \draw[] node[] at (4.2,4.2) {\textbf{b)}};
    \end{tikzpicture}
        \caption{Interpretación geométrica de la suma vectorial en el espacio vectorial real $\protect\mathbb{R}^2$. En la figura $\textbf{a)}$ se observan los vectores $\protect\begin{pmatrix} 1 & 3 \protect\end{pmatrix}$ y $\protect\begin{pmatrix} 2 & 1 \protect\end{pmatrix}$, así como el vector resultante de la suma de los dos anteriores, $\protect\begin{pmatrix} 3 & 4 \protect\end{pmatrix}$. En la figura $\textbf{b)}$ observamos la llamada \emph{Ley del paralelogramo} para la suma de dos vectores.}
    \label{fig:Suma_vectorial}
\end{figure}

    Observemos que, visualmente, esto corresponde a trazar uno de los vectores en el plano cartesiano y luego trazar el otro colocando la cola en la punta del vector anterior, como si ése fuese su origen. Nótese que no importa cuál vector trazamos primero y cuál después, lo cual concuerda con la conmutatividad de la suma vectorial (esta misma interpretación geométrica es válida para la suma de tres o más vectores de $\mathbb{R}^2$: basta irlos sumando de dos en dos vectores); a esto se le conoce como la \textit{Ley del paralelogramo}. En particular, $\forall\hspace{0.5mm} \mathbf{v} \in \mathbb{R}^2$, $\mathbf{0}+\mathbf{v}=\mathbf{v}$, lo cual concuerda con el hecho de que el vector $\mathbf{v}$ corresponda a una flecha con cola en el origen.

\vspace{3mm}
\textbf{Producto de un vector por un escalar}
\vspace{3mm}

        En este espacio, el producto de un vector por un escalar se define como $c\begin{pmatrix}a&b\end{pmatrix}\equiv\begin{pmatrix}ca&cb\end{pmatrix}$. Podemos calcular, por ejemplo, los productos $(\frac{1}{2})\begin{pmatrix}2&2\end{pmatrix}=\begin{pmatrix}1&1\end{pmatrix}$ y $(-1.2)\begin{pmatrix}1&3\end{pmatrix}=\begin{pmatrix}-1.2&-3.6\end{pmatrix}$. La representación gráfica de estas operaciones se muestra en la Figura \ref{fig:Producto_de_un_vector_por_un_escalar}.

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}[thick,scale=0.8, every node/.style={scale=1}]
        \draw[thick,<->] (-4,0) -- (4,0);
        \draw[thick,<->] (0,-4) -- (0,4);
        \draw[step=1cm,gray,thin,dashed] (-3.9,-3.9) grid (3.9,3.9);
        \foreach \x in {1,2,3,4}
            \draw (\x cm,1pt) -- (\x cm, -1pt) node[anchor=north] {$\x$};
        \foreach \y in {1,2,3,4}
            \draw (1pt,\y cm) -- (-1pt,\y cm) node[anchor=east] {$\y$};
            \draw[ROJO,very thick,->] (0,0) -- (2,2) node[anchor=west] {$\begin{pmatrix} 2 & 2 \end{pmatrix}$};
            \draw[AZUL,very thick,->,opacity=0.7] (0,0) -- (1,3) node[anchor=south west] {$\begin{pmatrix} 1 & 3 \end{pmatrix}$};
            \draw[black,thick,->]  (5,0) -- (5.7,0);
            \draw[] node[] at (3.5,3.5) {\textbf{a)}};
            \end{tikzpicture} \hspace{0.5mm} \begin{tikzpicture}[thick,scale=0.8, every node/.style={scale=1}]
        \draw[thick,<->] (-4,0) -- (4,0);
        \draw[thick,<->] (0,-4) -- (0,4);
        \draw[step=1cm,gray,thin,dashed] (-3.9,-3.9) grid (3.9,3.9);
        \foreach \x in {1,2,3,4}
            \draw (\x cm,1pt) -- (\x cm, -1pt) node[anchor=north] {$\x$};
        \foreach \y in {1,2,3,4}
            \draw (1pt,\y cm) -- (-1pt,\y cm) node[anchor=east] {$\y$};
            \draw[ROJO,very thick,opacity=0.7,->] (0,0) -- (1,1) node[] at (2.1,0.65) {$\begin{pmatrix} 1 & 1 \end{pmatrix}$};
            \draw[AZUL,very thick,->] (0,0) -- (-1.2,-3.6) node[] at (-3,-3.4) {$\begin{pmatrix} -1.2 & -3.6 \end{pmatrix}$};
            \draw[ROJO,thin,dashed,->] (0,0) -- (2,2); 
            \draw[AZUL,thin,dashed,->,opacity=0.7] (0,0) -- (1,3); 
            \draw[] node[] at (3.5,3.5) {\textbf{b)}};
    \end{tikzpicture}
    \caption{Interpretación geométrica del producto de un vector por un escalar en el espacio vectorial real $\mathbb{R}^2$. Comparando las figuras $\textbf{a)}$ y $\textbf{b)}$ observamos que, en caso de que se multiplique a un vector de $\mathbb{R}^2$ por un escalar de $\mathbb{R}$, es posible que la longtitud del vector cambie y que su sentido se invierta, pero su dirección no cambia.}
    \label{fig:Producto_de_un_vector_por_un_escalar}
\end{figure}


    Como podemos observar, el primer producto redujo la longitud del vector sin cambiar su sentido, mientras que el segundo producto aumentó la longitud del vector, a la vez que invirtió su sentido; sin embargo, en ambos casos, el producto de un vector por un escalar no cambió la \emph{dirección} de los vectores\textemdash es decir, los mantuvo en la misma \emph{línea}. En general, si el escalar $c\in\mathbb{R}$ que multiplica al vector tiene $|c|>1$, lo \emph{alarga}; si tiene $|c|<1$, lo acorta; finalmente, si tiene $|c|=1$, no cambia su longitud. Por este cambio de longitud es que al producto de un vector por un escalar también se le conoce por el nombre \emph{reescalamiento}. Además, si $c>0$, el vector mantiene su misma dirección y sentido (sigue en la misma línea y apunta hacia el mismo lado) mientras que, si $c<0$, el vector conserva su dirección pero se invierte su sentido (sigue en la misma línea pero apunta hacia el lado opuesto); si $c=0$ entonces el vector automáticamente se convierte en el vector nulo $\begin{pmatrix}0&0\end{pmatrix}$, como se demostró algebráicamente en el primer inciso del Teorema 1.2.3.2. Para visualizar las operaciones de adición vectorial y producto de un vector por un escalar de forma interactiva, recomiendo la sección \textbf{Vector Algebra and Geometry} de \url{https://textbooks.math.gatech.edu/ila/vectors.html}, así como la ilustración interactiva \url{http://immersivemath.com/ila/ch02_vectors/ch02.html#fig_vec_scaling}.

    Así, en general, si combinamos las operaciones de suma vectorial y producto de un vector por un escalar, visualmente lo que estaremos haciendo será \emph{combinar líneas} con diferentes longitudes, direcciones y sentidos en el plano cartesiano. 

\vspace{3mm}

Nota: El vector nulo $\mathbf{0}=\begin{pmatrix}0&0\end{pmatrix}$ (también llamado \emph{vector origen}) no tiene longitud, ya que es el único donde la cola y la punta de su flecha coinciden. Además, se dice que tampoco tiene dirección ni sentido\footnote{Alternativamente, se dice que tiene \emph{todas las direcciones} y \emph{todos los sentidos simultáneamente}: en la práctica, ambas interpretaciones son equivalentes, pero la primera puede ser más fácil de asimilar.}. Si asumimos que este vector no tiene longitud, dirección ni sentido, entonces queda claro por qué cualquier reescalamiento de este vector no lo modifica, como se demostró en el segundo inciso del Teorema 1.2.3.2.

\subsubsection{En el espacio vectorial real \texorpdfstring{$\mathbb{R}^3$}{TEXT}}

La suma vectorial y el producto de un vector por un escalar (o \emph{reescalamiento}) en el espacio vectorial real $\mathbb{R}^3$ tienen la misma interpretación geométrica que en $\mathbb{R}^2$, con una dimensión extra añadida. Esto es de esperarse, ya que las definiciones de estas operaciones y las correspondencias entre vectores y flechas que salen del origen a una coordenada específica son análogas en ambos espacios vectoriales.

\subsubsection{En el espacio vectorial complejo \texorpdfstring{$\mathbb{C}$}{TEXT}}

Como hemos visto, el plano cartesiano nos sirve para representar vectores con dos entradas reales. De manera similar, el \emph{plano complejo} \textemdash con un eje de números \emph{reales} (por convención, el horizontal) y otro eje perpendicular a él de números \emph{imaginarios}\footnote{Los números imaginarios son aquellos números complejos con la parte real igual a cero, i.e. $0+ib=ib\in\mathbb{C}$, donde $b$ es un número real. En otras palabras, son el resultado de multiplicar el número imaginario $i$ por cualquier número real.}\textemdash\hspace{0.5mm} nos sirve para representar vectores con una entrada compleja. Así, cada vector de una entrada compleja $\begin{pmatrix}a+ib\end{pmatrix}$ con $a,b\in\mathbb{R}$ tiene una correspondencia uno a uno con una flecha con cola en el origen del plano y flecha en la coordenada $(a,ib)$ del plano complejo, la cual corresponde a, desde el origen, moverse $a$ unidades sobre el eje real y $b$ unidades sobre el eje imaginario.

\vspace{3mm}
\textbf{Suma vectorial}
\vspace{3mm}

De la definición de suma vectorial $\begin{pmatrix}a+ib\end{pmatrix}+\begin{pmatrix}c+id\end{pmatrix}\equiv\begin{pmatrix}(a+c)+(b+d)i\end{pmatrix}$ se deduce que la suma vectorial entre vectores de $\mathbb{C}$ tiene la misma interpretación geométrica que aquella entre vectores de $\mathbb{R}^2$. Por ejemplo, si calculamos $\begin{pmatrix}1+2i\end{pmatrix}+\begin{pmatrix}3+2i\end{pmatrix}=\begin{pmatrix}4+4i\end{pmatrix}$, podemos representarlo visulamente en la Figura \ref{fig:Suma_vectorial_compleja}.

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}
        \draw[thick,->] (0,0) -- (5,0);
        \draw[thick,->] (0,0) -- (0,5);
        \draw[step=1cm,gray,thin,dashed] (0,0) grid (4.9,4.9);
        \foreach \x in {1,2,3,4}
            \draw (\x cm,1pt) -- (\x cm, -1pt) node[anchor=north] {$\x$};
        \draw (1pt, 1cm) -- (-1pt, 1 cm) node[anchor=east] {$i$};
        \foreach \y in {2,3,4}
            \draw (1pt,\y cm) -- (-1pt,\y cm) node[anchor=east] {$\y i$};
            \draw[AZUL,very thick,->] (0,0) -- (1,2) node[] at (0.8,2.6) {$\begin{pmatrix} 1 + 2i \end{pmatrix}$};
            \draw[ROJO,thin,dashed,->] (1,2) -- (4,4) node[] at (2.4,3.8) {$\begin{pmatrix} 3 + 2i \end{pmatrix}$};
            \draw[ROJO,very thick,->] (0,0) -- (3,2) node[] at (3.1,1.2) {$\begin{pmatrix} 3 + 2i \end{pmatrix}$};
            \draw[AZUL,thin,dashed,->] (3,2) -- (4,4) node[] at (4.2,2.6) {$\begin{pmatrix} 1 + 2i \end{pmatrix}$};
        \draw[violet,very thick,->] (0,0) -- (4,4) node[anchor=south west] {$\begin{pmatrix} 4 + 4i \end{pmatrix}$};
    \end{tikzpicture}
    \caption{Interpretación geométrica de la suma vectorial en el espacio vectorial complejo $\mathbb{C}$. Observamos que, al igual que en el caso del espacio vectorial real $\mathbb{R}^2$, se cumple la \emph{Ley del paralelogramo}.}
    \label{fig:Suma_vectorial_compleja}
\end{figure}



\vspace{3mm}
\textbf{Producto de un vector por un escalar}
\vspace{3mm}

Por definición, el producto de un vector por un escalar es $(q+ir)\begin{pmatrix}s+it\end{pmatrix}\equiv\begin{pmatrix}(qs-rt)+i(qt+rs)\end{pmatrix}$.

Notemos que, en particular, si la parte imaginaria del escalar es nula (i.e., si $r=0$), entonces el escalar es un número real y el producto resultante es $(q)\begin{pmatrix}s+it\end{pmatrix}\equiv\begin{pmatrix}(qs)+(qt)i\end{pmatrix}$, por lo cual geométricamente sólo se produce un reescalamiento totalmente análogo al discutido en la sección \ref{Ejem:En_R^2}.

En cambio, ahora observemos qué sucede si la parte real del escalar es nula y la parte imaginaria es igual a $1$ (i.e., si multiplicamos por el escalar $i$). Tomemos, por ejemplo, al vector $\begin{pmatrix}2+2i\end{pmatrix}$. Al hacer el producto de este vector por $i$ obtenemos $\begin{pmatrix}-2+2i\end{pmatrix}$. Si, en cambio, hacemos el producto de este mismo vector por el escalar $-i$, obtenemos como resultado $(-i)\begin{pmatrix}2+2i\end{pmatrix}=\begin{pmatrix}2-2i\end{pmatrix}$. Ambas operaciones se muestran de manera visual en la Figura \ref{fig:Producto_de_un_vector_complejo_por_i}.

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}
        \draw[thick,<->] (-3.5,0) -- (3.5,0);
        \draw[thick,<->] (0,-3.5) -- (0,3.5);
        \draw[step=1cm,gray,thin,dashed] (-3.4,-3.4) grid (3.4,3.4);
        \foreach \x in {1,2}
            \draw (\x cm,1pt) -- (\x cm, -1pt) node[anchor=north] {$\x$};
            \draw (1pt,1 cm) -- (-1pt,1 cm) node[anchor=east] {$i$};
            \draw (1pt,2 cm) -- (-1pt,2 cm) node[anchor=east] {$2i$};
            \draw[darkgray,very thick,opacity=0.8,->] (0,0) -- (2,2) node[anchor=south west] {$\begin{pmatrix} 2 & 2i \end{pmatrix}$};
            \draw[gray,very thick,opacity=0.9,->] (0,0) -- (-2,2) node[anchor=south east] {$\begin{pmatrix} -2 & 2i \end{pmatrix}$};
            \draw[darkgray,very thick,->] (0,0) -- (2,-2) node[anchor=north west] {$\begin{pmatrix} 2 & -2i \end{pmatrix}$};
    \end{tikzpicture}
    \caption{Interpretación geométrica del producto de un vector complejo por los números imaginarios $i$ y $-i$. En este caso, nuestro vector base es $\protect\begin{pmatrix} 2 & 2i \protect\end{pmatrix}$. El producto de este vector por el escalar $i$ resulta en el vector $\protect\begin{pmatrix} -2 & 2i \protect\end{pmatrix}$, lo cual puede ser interpretado geométricamente como una rotación discreta de $\frac{\pi}{2}$ radianes. Así observamos que, en cambio, el producto de nuestro vector base $\protect\begin{pmatrix} 2 & 2i \protect\end{pmatrix}$ por $-i$ se puede interpretar geométricamente como una rotación discreta de $-\frac{\pi}{2}$ radianes.}
    \label{fig:Producto_de_un_vector_complejo_por_i}
\end{figure}

   Aquí vemos que hacer el producto de un vector por el escalar $i$ \emph{equivale a hacer una rotación de $90^\circ$ ó $\frac{\pi}{2}$ radianes}. Análogamente, el producto de un vector por el escalar $-i$ equivale a hacer una rotación de $-90^\circ$ ó $-\frac{\pi}{2}$ radianes. Esto tiene sentido ya que $-i=-1(i)=i(-1)$ lo cual implica que, debido a la compatibilidad del producto de un vector por un escalar con el producto entre escalares, es lo mismo multiplicar un vector por $(-i)$ a multiplicarlo por $i$ y después por -1, o vice versa: el razonamiento geométrico correspondiente es que da lo mismo rotar un vector $-\frac{\pi}{2}$ radianes a rotarlo $\frac{\pi}{2}$ radianes y después invertir su sentido, o primero invertir su sentido y después rotarlo $\frac{\pi}{2}$ radianes. 

   ¿Y si multiplicamos un vector de $\mathbb{C}$ por un escalar $ai$ con $a\neq 0,1$? Ya que $ai\begin{pmatrix}b + ic\end{pmatrix}=\begin{pmatrix}-ac+i(ab)\end{pmatrix}=a\begin{pmatrix}-c+ib\end{pmatrix}=a(i\begin{pmatrix}b+ic\end{pmatrix})$ \textemdash es decir, por la compatibilidad entre productos\textemdash\hspace{0.5mm} podemos deducir que hacer el producto de un vector complejo por un número imaginario arbitrario $ai$ tendrá dos consecuencias: rotarlo de acuerdo a $i$ ($\frac{\pi}{2}$ radianes a contrarreloj) y reescalarlo de acuerdo al valor de $a$ (invirtiendo el sentido si $a<0$). En este último caso donde $a$ es negativo, ya que $ai=|a|(-i)=(-i)|a| \hspace{3mm}\forall\hspace{1.5mm} a<0$, también podríamos pensar que se rota al vector complejo de acuerdo a $-i$ ($\frac{\pi}{2}$ radianes en el sentido de las manecillas) y se reescala de acuerdo al valor absoluto de $a$: ambos razonamientos son equivalentes.

Dicho lo anterior, estamos listos para el caso más general: multiplicar un vector complejo $\begin{pmatrix}s+it\end{pmatrix}$ por un escalar complejo $q+ir$ con $q,r\neq0$ \emph{reescalará} el vector en el plano complejo y lo \emph{rotará} en el sentido correspondiente al signo de $r$. Es decir que, en los espacios vectoriales complejos, los escalares no sólamente pueden \emph{reescalar} vectores, sino que también los pueden \emph{rotar}\footnote{El asunto de las \emph{magnitudes específicas} de estos reescalamientos y rotaciones \textemdash el cual se complica para escalares complejos en general\textemdash\hspace{0.5mm} será precisado más adelante.}.

\newpage
\subsection{Subespacios vectoriales} \label{Subsec:Subespacios_vectoriales}

En ciertas formas \emph{específicas}, las cuales iremos detallando, los subespacios vectoriales son a los espacios vectoriales lo que los subconjuntos a los conjuntos. Por ejemplo: así como cualquier subconjunto es, en sí mismo, un conjunto, cualquier subespacio vectorial es, en sí mismo, un espacio vectorial.

\subsubsection{Definición de subespacio vectorial} \label{Def:Subespacio_vectorial}

\begin{tcolorbox}
\underline{Def.} Sea un conjunto $V$ sobre un campo $K$ un espacio vectorial. Un \textit{subespacio vectorial} de $V$ es un subconjunto $W\subset V$ sobre el campo $K$ con las operaciones de suma vectorial y producto de un vector por un escalar que cumple las propiedades siguientes:

\begin{center}
\begin{tabular}{lr}
    $\forall\hspace{1.5mm} \mathbf{w},\mathbf{x}\in W \hspace{3mm}\exists \hspace{1.5mm} \mathbf{w}+\mathbf{x}\in W$ & Cerradura de la adición \\ \\ \multirow{2}{0.4\textwidth}{$\forall\hspace{1.5mm} \mathbf{w}\in W, a\in K \hspace{3mm}\exists \hspace{1.5mm} a\mathbf{w}\in W$} & \multirow{2}{0.28\textwidth}{Cerradura del producto de un vector por un escalar} \\ \\ \\
    $\exists \hspace{1.5mm} \mathbf{0}\in W$ t.q. $\mathbf{w}+\mathbf{0}=\mathbf{w}\hspace{3mm}\forall\hspace{1.5mm} \mathbf{w} \in W$ & Elemento identidad de la adición (neutro aditivo). \\ \\
\end{tabular}
\end{center}

\hspace{2.5mm} En este caso, a $W$ se le conoce como un \textit{subespacio vectorial} de $V$ (nuevamente, simplificando la notación); sin embargo, $W$ también es, en sí mismo, un espacio vectorial.

\end{tcolorbox}{}

Observemos que:

\begin{itemize}
    \item Ya que cualquier subespacio vectorial es un espacio vectorial, entonces cualquier subespacio vectorial puede tener subespacios vectoriales subsecuentes\footnote{Esto es más o menos similar al hecho de que cualquier subconjunto puede tener subconjuntos subsecuentes.}.
    \item La definición de subespacio vectorial sólo incluye tres propiedades. Esto nos indica que, ya que $W$ es subconjunto de $V$ y ambos espacios vectoriales están definidos sobre el mismo campo $K$, si $W$ cumple explícitamente las tres propiedades mencionadas, las demás propiedades de un espacio vectorial se siguen trivialmente.
    \item Para todo espacio vectorial $V$, $V$ y $\{\mathbf{0}\}$ son subespacios vectoriales de $V$\footnote{Aquí se sobreentiende que los conjuntos $V$ y $\{\mathbf{0}\}$ están definidos como espacios vectoriales sobre el mismo campo $K$ y que $\{\mathbf{0}\}$ representa al espacio vectorial que sólo tiene al vector nulo de $V$ como vector.}.
\end{itemize}{}

\subsubsection{Ejemplos de subespacios vectoriales}

El conjunto de todos los pares ordenados $\{\begin{pmatrix} x_1&x_2\end{pmatrix}\mathop |\mathop x_1,x_2\in\mathbb{R}\mathop\land\mathop x_1=x_2\}$ es un subespacio vectorial en el espacio vectorial real $\mathbb{R}^2$ (o $\mathbb{R}\times \mathbb{R}$).

\vspace{3mm}

El conjunto $\mathbb{R}$ sobre el campo $\mathbb{R}$ es un subespacio vectorial del conjunto $\mathbb{C}$ sobre el mismo campo $\mathbb{R}$.

\vspace{3mm}

Sean $j,k\in\mathbb{N}$ t.q. $j<k$. El conjunto de polinomios de grado $j$ es un subespacio vectorial\footnote{De aquí en adelante, asumiremos que cualquier espacio vectorial $V$ está definido por un conjunto vectorial $V$ sobre el campo $\mathbb{R}$ (espacio vectorial real), a menos que se indique lo contrario.} del espacio vectorial formado por el conjunto de polinomios de grado $k$.

\vspace{3mm}

El conjunto de todas las funciones reales de clase $C^{\infty}$ es un subespacio vectorial del espacio vectorial formado por el conjunto de todas las funciones reales de clase $C^n$ (con $n\in\mathbb{N}$).

\vspace{3mm}

\subsubsection{Algunos teoremas de subespacios vectoriales} \label{Teo:Subespacios_vectoriales}

\begin{teorema} {1.4.3.1} Cualquier intersección de dos subespacios vectoriales de $V$ es un subespacio vectorial de $V$.

\begin{proof}
    Sea $V$ sobre $K$ un espacio vectorial y sea $C$ una colección de subespacios vectoriales de $V$ (definidos sobre el mismo campo $K$). Sea $W$ la intersección de los conjuntos vectoriales de $C$. Entonces, ya que cada subespacio vectorial en $C$ contiene al neutro aditivo de $V$, $\mathbf{0}\in W$. Además, sea $a\in K$ y sean $\mathbf{u},\mathbf{v}\in W$, entonces $\mathbf{u},\mathbf{v}$ están en todos los subespacios de la colección $C$, cada uno de los cuales es cerrado por la adición vectorial y por el producto de un vector por un escalar, de donde se sigue que $a\mathbf{u}, a\mathbf{v}$ y $\mathbf{u}+\mathbf{v}$ están en todos los subespacios, por lo cual también están en $W$. Por lo tanto, por la definición de la sección \ref{Def:Subespacio_vectorial}, $W$ es un subespacio vectorial de $V$.
\end{proof}
\end{teorema}

\begin{teorema} {1.4.3.2} Sea $Z$ un subespacio vectorial de $W$ y sea $W$, a su vez, subespacio vectorial de $V$. Entonces $Z$ es un subespacio vectorial de $V$.
\end{teorema}

\noindent La demostración del Teorema 1.4.3.2 se deja como ejercicio. Este último teorema nos muestra otra analogía válida entre subconjuntos y subespacios vectoriales, ya que si $A \subset B$ y $B\subset C \implies A\subset C$.

\subsubsection{Suma y suma directa de espacios vectoriales}

\begin{tcolorbox}
\underline{Def.} Sean $S_1$ y $S_2$ subespacios de un espacio vectorial $V$. Definimos a la \emph{suma de los subespacios vectoriales} $S_1$ y $S_2$ como el espacio vectorial definido por $S_1+S_2=\{\mathbf{x}+\mathbf{y}\mathop|\mathop \mathbf{x}\in S_1, \mathbf{y}\in S_2\}$\footnote{Aquí se sobreentiende que $S_1, S_2$ y $S_1+S_2$ están definidos sobre el mismo campo.}.

\vspace{3mm}

\underline{Def.} Si, además, se cumple que $S_1+S_2=V$ y $S_1 \cap S_2 = \{\mathbf{0}\}$, decimos que el espacio vectorial $V$ es la \emph{suma directa} de $S_1$ y $S_2$, lo cual denotamos como $S_1\oplus S_2=V$.
\end{tcolorbox}

La operación de suma entre subespacios vectoriales en realidad es una suma entre sus \emph{conjuntos vectoriales} \textemdash así como la intersección de dos espacios vectoriales es en realidad una intersección de los conjuntos vectoriales\textemdash; el conjunto resultante de la suma forma un espacio vectorial sobre el mismo campo que define a los subespacios. Observemos que la definición de suma vectorial pide que $S_1$ y $S_2$ sean \emph{subespacios} de un espacio vectorial $V$, y no sólo espacios vectoriales arbitrarios: esto asegura que su suma $S_1+S_2$ también sea un espacio vectorial.

Por otro lado, una suma directa de la forma $V_1\oplus V_2\oplus...\oplus V_n=W$ nos da la sensación de que, en cierto sentido, el espacio vectorial $W$ se puede \emph{descomponer} en sus subespacios $V_1, V_2,...,V_n$, dado que el único elemento común entre cualesquiera de estos dos subespacios es el neutro aditivo (vector nulo).

\vspace{3mm}

Para dar un ejemplo: sea $C$ el espacio vectorial de todas las funciones constantes $f(x) = c$ con $c\in\mathbb{R}$, y sea $D$ el de todas las funciones de la forma $f(x) = d x$ para algún $d\in\mathbb{R}$. Sea $P^n$ el espacio vectorial de todos los polinomios de grado $n$, es decir, de todas las funciones con regla de correspondencia $f(x) = c_0 x^1 + c_1 x^1 + ... + c_n x^n$ con $c_i\in\mathbb{R}$, entonces $C\oplus D = P^1$ (de hecho, nótese que $P^0=C$ por lo cual pudimos haber escrito $P^0\oplus D=P^1$ de manera equivalente).

\vspace{3mm}

Volveremos a esta idea de \emph{descomponer un espacio vectorial como una suma directa de sus subespacios vectoriales} más adelante en el curso. Antes de eso, debemos ver otro tipo de operación de los espacios vectoriales, la cual se realiza entre dos vectores y da como resultado un escalar.

\subsection{Ejercicios de repaso}

\subsubsection{Campos}

\begin{enumerate}
    \item Demuestra que el conjunto $\mathbb{C}$ junto con las operaciones de suma y multiplicación definidas en la sección \ref{Ejem:Campo_complejo} cumplen las últimas tres propiedades de la definición de campo de la sección \ref{Def:Campo} para finalmente demostrar que estas tres cosas juntas (el conjunto $\mathbb{C}$ y las dos operaciones mencionadas) forman un campo. 
\end{enumerate}

\subsubsection{Espacios vectoriales} \label{Ejer:Espacios_vectoriales}

\begin{enumerate}
%    \item En la definición de espacio vectorial de la sección \ref{Def:Espacio_vectorial} no se especifica que los resultados de las operaciones $\mathbf{u}+\mathbf{v}$ (en la propiedad de \textit{cerradura de la adición}) ni $a\mathbf{v}$ (en \textit{cerradura del producto de un vector por un escalar}) sean únicos. Tampoco se especifica que el elemento identidad de la adición ($\mathbf{0}$), el elemento identidad del producto de un vector por un escalar ($1$), ni los elementos inversos de la adición de cada vector $\mathbf{v}$ ($-\mathbf{v}$) sean únicos. Demuestra que todos los elementos mencionados anteriormente son únicos. 
    \item Sea $K$ un campo arbitrario. ¿Siempre puede definirse un espacio vectorial de $K$ (como conjunto vectorial) sobre sí mismo (como campo)? Si sí, demuéstralo. Si no, da un contraejemplo (Nota: repasa los axiomas de campo y ten cuidado con tu notación). 
    \item Explica por qué \textbf{no} puede existir un espacio vectorial con vectores reales en $\mathbb{R}^n$ sobre el campo $\mathbb{C}$, pero sí puede haber un espacio vectorial con vectores complejos en $\mathbb{C}^n$ sobre el campo $\mathbb{R}$. 
    \item Sea $K$ un campo arbitrario. Demuestra que $K^n$ sobre $K$ es un espacio vectorial, con $n\in\mathbb{N}$. 
\end{enumerate}

\subsubsection{Interpretacción geométrica de los espacios vectoriales}

\begin{enumerate}
        \item Define las variables $a,b,c,d,e\in\mathbb{R}$ de acuerdo a los últimos 5 dígitos de tu número de cuenta. Calcula algebráicamente al vector $\begin{pmatrix}a & b\end{pmatrix}+c\begin{pmatrix}d & e\end{pmatrix}$ y muestra gráficamente al vector resultante aplicando la Ley del paralelogramo. 
                \item Como vimos en la sección \ref{Subsec:Interpretación_geométrica_de_las_operaciones_de_los_espacios_vectoriales}, la suma de dos vectores $\mathbf{v},\mathbf{u}\in\mathbb{R}^2$ sobre $\mathbb{R}$ se puede representar geométricamente con ayuda de un paralelogramo en el plano cartesiano, en donde la diagonal del paralelogramo que sale del origen es igual al vector resultante de la suma $\mathbf{u}+\mathbf{v}$ ó $\mathbf{v}+\mathbf{u}$ (ver, e.g., la Figura \ref{fig:Suma_vectorial}). Muestra intuitivamente que si transportamos a la \emph{otra} diagonal del paralelogramo hasta el origen (lo cual se puede hacer de dos maneras distintas), obtenemos la representación del vector resultante de la resta $\mathbf{u}-\mathbf{v}$ ó $\mathbf{v}-\mathbf{u}$. 
                        \item Muestra intuitivamente que el punto de intersección de las diagonales del paralelogramo mencionado en el ejercicio anterior es igual a la punta del vector $\frac{1}{2}(\vec{u}+\vec{v})$. 
                                \item Elige los dos últimos dígitos de tu número de cuenta y llámalos $q$ y $r$, respectivamente. Al segundo y tercer dígito de tu número de cuenta, llámalos $s$ y $t$. Dibuja el vector complejo $\begin{pmatrix} q + ir \end{pmatrix}\in\mathbb{C}$ en el plano complejo. Explica qué le pasaría a este vector si realizáramos su producto por los escalares complejos $s+it$, $-s+it$, $s-it$ y $-s-it$. 
\end{enumerate}

\subsubsection{Subespacios vectoriales}

\begin{enumerate}
    \item En la página 17 del libro \textit{Linear Algebra} de Friedberg, la definición de un subespacio vectorial incluye una cuarta propiedad que deben cumplir (la de existencia de inversos aditivos), la cual no incluí en las notas del curso, ya que es redundante. Explica por qué se puede desechar esta cuarta propiedad sin afectar la definición de subespacio vectorial. 
    \item Demuestra que el conjunto $\{\begin{pmatrix} a&2a&3a \end{pmatrix}\in\mathbb{R}^3\mathop|\mathop a\in\mathbb{R}\}$ sobre el campo $\mathbb{R}$ es un subespacio vectorial de $\mathbb{R}^3$. ¿Qué representa este conjunto visualmente en $\mathbb{R}^3$? 
    \item Demuestra que el espacio vectorial de todas las funciones polinomiales de una variable real de grado $n$ es un subespacio vectorial del espacio vectorial de todas las funciones reales de una variable de clase $C^m$ para toda $n \geq m$. 
    \item Demuestra que el espacio vectorial de todas las funciones de una variable real que son derivables y tienen derivada continua (de clase $C^1$) sobre $\mathbb{R}$ es un subespacio vectorial del espacio vectorial de todas las funciones reales continuas (de clase $C^0$) de una variable sobre $\mathbb{R}$. ¿Cómo generalizarías este resultado a que las de clase $C^i$ formen un subespacio de las de clase $C^n$ con $i>n$ sobre el mismo campo $\mathbb{R}$? Argumenta. 
    \item Demuestra el Teorema 1.4.3.2 (ver sec. \ref{Teo:Subespacios_vectoriales}). 
\end{enumerate}


\newpage
\section{Producto escalar (punto), norma, interpretación geométrica del producto escalar: proyecciones y ortogonalidad, producto vectorial (cruz)* y triple producto escalar*} \label{Sec:2}

\subsection{Producto escalar (punto)} \label{Subsec:Producto_escalar}

Algunos espacios vectoriales están dotados de una operación entre vectores que da como resultado un escalar conocida como \emph{producto escalar}. En estos espacios, dicha operación tiene una gran riqueza geométrica, la cual nos permite tener nociones como ortogonalidad y magnitud \textemdash y generalizarlas a espacios vectoriales más abstractos, como veremos a lo largo de esta sección\footnote{En el último módulo del curso regresaremos a este tipo de espacios para descubrir más secretos del producto escalar.}.

\subsubsection{Definición de producto escalar (punto)} \label{Def:Producto_escalar}

\begin{tcolorbox}
\underline{Def.} Sea $V$ sobre $K$ un espacio vectorial, con $\mathbf{u},\mathbf{v},\mathbf{w}\in V$ y $a\in K$. El \textit{producto escalar} $\langle\cdot\mathop ,\cdot\rangle:V\times V\rightarrow K$ le asocia a cualquier par ordenado de vectores en $V$ un escalar en $K$, y cumple las tres propiedades siguientes:

\begin{tabular}{l}
    \\
    $\langle\mathbf{u}+\mathbf{w},\mathbf{v}\rangle = \langle\mathbf{u},\mathbf{v}\rangle+\langle\mathbf{w},\mathbf{v}\rangle,$ \\ \\ $\langle a\mathbf{u},\mathbf{v}\rangle = a\langle\mathbf{u},\mathbf{v}\rangle,$ \\ \\
    $\langle\mathbf{u},\mathbf{v}\rangle=\overline{\langle\mathbf{v},\mathbf{u}\rangle},$ \\ \\
\end{tabular}

donde la barra $\overline{\langle\mathbf{v},\mathbf{u}\rangle}$ denota el complejo conjugado de $\langle\mathbf{v},\mathbf{u}\rangle$\footnote{Recordemos que, en general, nuestro campo $K$ puede ser complejo; en ese caso, el producto escalar en $V$ le asignará a cada par ordenado de dos vectores de $V$ un escalar complejo $c\in\mathbb{C}$.}. Si, además, se cumple la propiedad

\begin{tabular}{l}
    \\
    $\forall\hspace{1.5mm}\mathbf{v}\in V, \hspace{1.5mm} \langle\mathbf{v},\mathbf{v}\rangle\geq0; \hspace{1.5mm} \langle\mathbf{v},\mathbf{v}\rangle = 0 \iff \mathbf{v}=\mathbf{0}$, \\ \\
\end{tabular}

se dice que el producto escalar es \textit{positivo definido}.

\vspace{3mm}
\hspace{2.5mm} Para vectores que son $n-$tuplas (como aquellos de $\mathbb{R}^n$ o $\mathbb{C}^n$), es común que el producto escalar de $\mathbf{u}$ y $\mathbf{v}$ se denote por $\mathbf{u}\cdot\mathbf{v}$, por lo cual también se le conoce como \textit{producto punto}; sin embargo, también existen notaciones alternativas como $(\mathbf{u}, \mathbf{v})$\footnote{La notación principal que utilizaremos para el producto escalar $\langle \cdot,\cdot\rangle$ es similar a la utilizada en Mecánica Cuántica $\braket{\phi|\psi}$ \textemdash en donde el campo siempre es complejo\textemdash, y esconde un profundo significado, el cual veremos más adelante.}.

\end{tcolorbox}{}

A partir de la definición, observemos lo siguiente:

\begin{itemize}
    \item Las primeras dos propiedades juntas nos dicen que $\langle\mathbf{u}+a\mathbf{w},\mathbf{v}\rangle = \langle\mathbf{u},\mathbf{v}\rangle+a\langle\mathbf{w},\mathbf{v}\rangle$. Para referirnos a esta propiedad específica en lenguaje matemático, decimos que el producto escalar es una operación \textit{lineal\footnote{Como debes sospechar, el concepto de operación \emph{lineal} es fundamental para el álgebra \emph{lineal}, y lo veremos con más detalle en las secciones de transformaciones y operadores lineales.} en la primera entrada}.
    \item La tercera propiedad nos dice que, si el campo es real, el producto escalar es una operación conmutativa, es decir, que si $K=\mathbb{R}\implies\langle\mathbf{u},\mathbf{v}\rangle=\langle\mathbf{v},\mathbf{u}\rangle$. En cambio, si el campo es complejo, en general, el producto escalar es no conmutativo.
    \item Las primeras tres propiedades juntas nos dicen que, si el espacio vectorial está definido sobre un campo complejo, entonces $\langle\mathbf{u},\mathbf{v}+a\mathbf{w}\rangle = \langle\mathbf{u},\mathbf{v}\rangle+\overline{a}\langle\mathbf{u},\mathbf{w}\rangle$, donde $\overline{a}$ denota el complejo conjugado del escalar complejo $a$. Esto nos dice que, si el campo es complejo, el producto escalar es una operación \textit{antilineal\footnote{Este concepto también se verá con mayor detenimiento más adelante.} en la segunda entrada}. En cambio, si el campo es real, entonces el producto escalar es lineal en ambas entradas.
    \item Para un producto escalar positivo definido, el único vector que puede tener como resultado el escalar $0$ al hacer producto escalar consigo mismo es el vector nulo $\mathbf{0}$ (el neutro aditivo que vimos en las secciones \ref{Def:Espacio_vectorial} y \ref{Def:Subespacio_vectorial}).
\end{itemize}


\subsubsection{Ejemplos de producto escalar en espacios vectoriales} \label{Ejem:Producto_escalar}

En $\mathbb{R}^2$ el producto escalar se define como $\begin{pmatrix}u_1\\u_2\end{pmatrix}\cdot\begin{pmatrix}v_1\\v_2\end{pmatrix} \equiv u_1v_1+u_2v_2$, donde hemos utilizado la notación de punto, ya que los vectores son 2-tuplas. En general, en el espacio vectorial real $\mathbb{R}^n$ el producto escalar se define como

$$\mathbf{u}\cdot\mathbf{v} = \begin{pmatrix}u_1&u_2&...&u_n\end{pmatrix}\cdot\begin{pmatrix}v_1&v_2&...&v_n\end{pmatrix} \equiv u_1v_1+u_2v_2+...+u_nv_n=\sum_{i=1}^n u_i v_i\footnote{Para simplificar la notación, en varias áreas de la física se elimina el signo de suma ($\Sigma$) cuando aparecen índices repetidos, siguiendo la convención de que éstos implícitamente indican una suma sobre el índice. Así, $\sum_{i=1}^n u_i v_i$ se puede escribir simplemente como $u_iv_i$. A esto se le conoce como \textit{notación de Einstein} o \textit{convención de suma de Einstein}.}.$$

Para cumplir \emph{todas} las propiedades descritas en la sección \ref{Def:Producto_escalar}, el producto escalar en el espacio vectorial complejo $\mathbb{C}^n$ se debe definir de una forma ligeramente distinta. Sean $\mathbf{a},\mathbf{b}\in\mathbb{C}^n$ vectores con $n$ entradas complejas, entonces el producto escalar se define como

$$\langle\mathbf{a},\mathbf{b}\rangle\equiv\mathbf{a}\cdot\mathbf{b}\equiv \sum_{i=1}^n a_i \overline{b_i},$$

\noindent es decir, se realiza un producto entre la $i$-ésima entrada de $\mathbf{a}$ y el \emph{complejo conjugado} de la $i$-ésima entrada de $\mathbf{b}$ para cada $i$, y luego se suman dichos productos.

\vspace{3mm}

Sea $C^0([a,b])$ el conjunto de todas las funciones de variable real continuas en el intervalo cerrado $[a,b]$ con integral finita, entonces podemos definir un producto escalar en el espacio vectorial del conjunto $C^0([a,b])$ sobre el campo $\mathbb{R}$ como

$$\langle f,g\rangle = \int_{a}^{b} f(x)g(x)dx.$$

\vspace{3mm}

Observemos que, como muestra el último ejemplo, se puede definir un producto escalar en muchos tipos de espacios vectoriales diferentes, y no sólo en aquellos que tienen como vectores a $n-$tuplas\footnote{Veremos otros tipos de espacios vectoriales donde se pueden definir productos escalares más adelante.}. Nótese además que, en cada caso, el resultado del producto escalar es un escalar del campo sobre el cual está definido el espacio vectorial.

\vspace{3mm}

Para ver más ejemplos de productos escalares pueden revisar \emph{Linear Algebra} de Friedberg (págs. 330-331), \textit{Linear Algebra: A Modern Introduction} de Poole\footnote{Ten en cuenta que algunos libros introducen el producto escalar (al cual pueden llamar producto punto) únicamente con espacios vectoriales reales, y lo generalizan a espacios vectoriales complejos en secciones posteriores.} (págs. 531-534), etc.


\subsubsection{Propiedades del producto escalar} \label{Prop:Producto_escalar}

Como resumen, a partir de la definición dada en la sección \ref{Def:Producto_escalar}, podemos ver que las principales propiedades del producto escalar son:

\begin{center}
    \begin{tabular}{lr}
        $\langle\mathbf{u},\mathbf{v}\rangle = \overline{\langle\mathbf{v},\mathbf{u}\rangle}$ & Conmutar vectores resulta en la conjugación del escalar \\
        $\langle a\mathbf{u}+\mathbf{v},\mathbf{w}\rangle = a\langle\mathbf{u},\mathbf{w}\rangle + \langle\mathbf{v},\mathbf{w}\rangle$ & Linealidad en la primera entrada\\
        $\langle\mathbf{u},\mathbf{w}+b\mathbf{z}\rangle = \langle\mathbf{u},\mathbf{w}\rangle + \overline{b}\langle\mathbf{u},\mathbf{z}\rangle$ & Antilinealidad en la segunda entrada\\
    \end{tabular}{}
\end{center}{}

En particular, el producto escalar definido \underline{sobre un espacio vectorial real} es una operación lineal en ambas entradas (o \emph{bilineal}), es decir, que $$\langle a_1\mathbf{u_1}+...+a_n\mathbf{u_n},\mathbf{v}\rangle=a_1\langle\mathbf{u_1},\mathbf{v}\rangle+...+a_n\langle\mathbf{u_n},\mathbf{v}\rangle\hspace{1mm}$$ \noindent y $$\langle\mathbf{u},b_1\mathbf{v_1}+...+b_n\mathbf{v_n}\rangle=b_1\langle\mathbf{u},\mathbf{v_1}\rangle+...+b_n\langle\mathbf{u},\mathbf{v_n}\rangle.$$

\newpage
\subsection{Norma} \label{Subsec:Norma}

Otro tipo de operación que se puede definir sobre un espacio vectorial es la \emph{norma}, que toma un sólo vector del espacio y devuelve un escalar del campo. La norma, en general, dota a los vectores de un espacio de un cierto sentido de \emph{magnitud} mediante el cual se puede comparar a distintos vectores. Como veremos en esta sección, si un espacio vectorial tiene un producto escalar positivo definido (dotando al espacio de las nociones de proyecciones y ortogonalidad que ya vimos en la sec. \ref{Subsec:Interpretación_geométrica_del_producto_escalar}), entonces se puede definir una norma sobre el espacio a partir de dicho producto escalar; sin embargo, no es necesario que una norma provenga de un producto escalar, y es posible que existan normas (y, por tanto, ciertas nociones de magnitud) en espacios vectoriales \emph{sin} producto escalar.

\subsubsection{Definición de norma} \label{Def:Norma}

\begin{tcolorbox}
\underline{Def.} Una \textit{norma} es una operación $||\cdot||:V\rightarrow K$ que toma sólo un vector y devuelve un escalar, y que cumple las siguientes propiedades:

\begin{center}
    \begin{tabular}{lr}
        $||\mathbf{u}+\mathbf{v}|| \leq ||\mathbf{u}|| + ||\mathbf{v}||$ & Satisface la desigualdad del triángulo \\ \\
        $||a\mathbf{u}|| = |a|\hspace{0.5mm}||\mathbf{u}||$ & Es escalable de forma absoluta \\ \\
        $||\mathbf{u}||=0\iff \mathbf{u}=\mathbf{0}$ & Distingue al vector nulo.
    \end{tabular}
\end{center}

\end{tcolorbox}{}

\vspace{3mm}
\noindent A partir de la definición anterior podemos demostrar que la norma es positivo definida; es decir que, además de que el único vector con norma igual a $0$ es el vector nulo, todos los vectores no nulos tienen norma positiva.

\begin{corolario} {}
    Sea $V$ un espacio vectorial con una norma $||\cdot ||$, entonces $\hspace{1.5mm}\forall\hspace{1.5mm} \mathbf{v}\in V$ con $\mathbf{v}\neq\mathbf{0}$ se cumple que $||\mathbf{v}||>0.$

\begin{proof}
    Por definición de norma tenemos que $||\mathbf{0}||=0$. Sea $\mathbf{v}\in V$ tal que $\mathbf{v}\neq\mathbf{0};$ ya que la norma distingue al vector nulo, entonces $||\mathbf{v}||\neq 0.$ Por definición de espacio vectorial, $\exists\hspace{1mm} \mathbf{-v}\in V$ tal que $\mathbf{v}+(-\mathbf{v})=\mathbf{0}\implies ||\mathbf{v}+(-\mathbf{v})||=||\mathbf{0}||=0$. Ya que la norma cumple la desigualdad del triángulo por definición, tenemos que $$0=||\mathbf{0}||=||\mathbf{v}+(-\mathbf{v})||\leq ||\mathbf{v}||+||\mathbf{-v}||\implies 0\leq ||\mathbf{v}||+||\mathbf{-v}||.$$

\noindent Además, por definición, la norma es escalable de forma absoluta, por lo cual $$0\leq ||\mathbf{v}||+||-\mathbf{v}||=||\mathbf{v}||+|-1|\hspace{0.5mm} ||\mathbf{v}||=||\mathbf{v}||+||\mathbf{v}||=2||\mathbf{v}||.$$

\noindent Ya que $||\mathbf{v}||\neq 0$, el resultado anterior $2||\mathbf{v}||\geq 0\implies ||\mathbf{v}||>0,$ como se quería demostrar.

\end{proof}

\end{corolario}

\subsubsection{Ejemplos de norma}

Tanto en el espacio vectorial real $\mathbb{R}$ como en el espacio vectorial complejo $\mathbb{C}$ se puede definir una norma como $$||x|| = |x|$$

\noindent para cualquier vector $x$ de dichos espacios, ya que el valor absoluto cumple trivialmente las propiedades de la sección \ref{Def:Norma} (lo cual quizá demostraste en tu curso de Cálculo I, por lo menos en el caso real). Como recordatorio, las definiciones del valor absoluto son $$|r| = +\sqrt{r^2} \hspace{3mm}\forall\hspace{0.5mm}r\in\mathbb{
R}; \hspace{3mm} |c| = +\sqrt{c\hspace{0.5mm}\overline{c}}\hspace{3mm}\forall\hspace{0.5mm}c\in\mathbb{C}.$$

\noindent A ésta se le conoce como la \emph{norma del valor absoluto}\footnote{En el caso de los números reales, el valor absoluto equivale a cambiar los signos de los números negativos por signos positivos y no hacerle nada a los número no negativos.}. En el caso real, esta norma se interpreta geométricamente como la distancia entre el origen de la recta real y el punto correspondiente al valor $r$ o, equivalentemente, como la longitud de la flecha que tiene cola en $0$ y punta en $r$; en el caso complejo, se interpreta como la distancia euclideana entre el origen del plano complejo y el punto correspondiente al valor complejo $c$ o, equivalentemente, como la longitud de la flecha que tiene cola en el origen del plano complejo y punta en $c$. 

\vspace{3mm}

Para los vectores que son $n-$tuplas, la norma más comunmente utilizada se define como $$||\mathbf{u}|| = +\sqrt{\langle\mathbf{u},\mathbf{u}\rangle} = +\sqrt{\mathbf{u}\cdot\mathbf{u}},$$ \noindent de acuerdo a las definiciones de productos escalares para $n-$tuplas dadas en la sección \ref{Ejem:Producto_escalar}. Esta norma se interpreta geométricamente como la longitud de la flecha que tiene cola en el origen del espacio vectorial y punta en la coordenada dada por las entradas del vector $\mathbf{u}$, lo cual es equivalente a la distancia euclideana entre estos dos puntos. Por ende, a esta norma se le conoce como \emph{norma euclideana}\footnote{Observemos que la norma del valor absoluto es simplemente un caso particular de la norma euclideana para los espacios vectoriales $\mathbb{R}$ y $\mathbb{C}$, tanto en su definición algebráica como en su interpretación geométrica.}. En particular, a los espacios vectoriales reales $\mathbb{R}^n$ con esta norma se les conoce como \emph{espacios vectoriales euclideanos} (o \emph{euclídeos})\footnote{Estos son los espacios vectoriales básicos que se utilizan en geometría analítica y cálculo diferencial e integral con funciones de una o más variables reales.}. Cuando la norma está asociada geométricamente a la longitud de la flecha que representa un vector, también se dice que está relacionada con la \emph{magnitud} de ese vector.

\vspace{3mm}

También, en un espacio vectorial real de funciones reales con integral finita en el intervalo $[a,b]$, podemos definir una norma a partir del producto escalar como $$||f|| = +\sqrt{\langle f,f\rangle} = +\sqrt{\int_a^b f(x)f(x) dx},$$ \noindent siguiendo el último ejemplo de la sección \ref{Ejem:Producto_escalar}. Es fácil demostrar que esta definición cumple con las propiedades de norma (ver sec. \ref{Def:Norma}; se siguen de las propiedades de la integral vistas en Cálculo II). 

\subsubsection{Desigualdad de Cauchy-Schwarz} \label{Teo:Cauchy-Schwarz} 

Como mencionamos al inicio de esta sección, el proceso anterior de obtener una norma a partir de un producto escalar se puede generalizar para cualquier espacio vectorial con un producto escalar positivo definido, sin importar si dichos espacios vectoriales son reales o complejos. En estos casos, se dice que la norma es \emph{inducida} por un producto escalar positivo definido. Las normas de este tipo serán las más recurrentes durante este curso. El primer paso necesario para demostrar este resultado general es la llamada \emph{desigualdad de Cauchy-Schwarz}, que enunciamos a continuación como teorema. El resto de la demostración se encuentra en los ejercicios de repaso al final de esta sección (ver sec. \ref{Ejer:Norma}).

\begin{teorema} {2.2.3.1} 

    Sea $\langle.\hspace{0.5mm},.\rangle:V\times V\rightarrow K$ un producto escalar positivo definido y sea $||\cdot ||:V\rightarrow K$ una función definida tal que $||\cdot||=+\sqrt{\langle\cdot,\cdot\rangle}$\footnote{Con la ayuda de este teorema, se puede demostrar que esta función $||\cdot ||$ cumple con todas las propiedades de una norma, dadas en la sección \ref{Def:Norma}. Ésta es la llamada \emph{norma euclideana generalizada}.}, entonces para todo $\mathbf{u},\mathbf{v}\in V$ se cumple que $|\langle\mathbf{u},\mathbf{v}\rangle|\leq ||\mathbf{u}||\hspace{0.5mm}||\mathbf{v}||$.

\begin{proof}
    Sean $\mathbf{u},\mathbf{v}\in V.$ Si $\mathbf{v}$ o $\mathbf{u}$ (o ambos) son iguales al vector nulo, la demostración se cumple trivialmente (ver el primer ejercicio de la subsec. \ref{Ejer:Producto_escalar}). Supongamos que $\mathbf{u},\mathbf{v}\neq\mathbf{0}$. 

    Sabemos que para cualquier $a\in K$ se cumple que $$0\leq ||\mathbf{u}-a\mathbf{v}||^2 = \langle\mathbf{u}-a\mathbf{v},\mathbf{u}-a\mathbf{v}\rangle= \langle\mathbf{u},\mathbf{u}-a\mathbf{v}\rangle-a\langle\mathbf{v},\mathbf{u}-a\mathbf{v}\rangle=$$ $$\langle\mathbf{u},\mathbf{u}\rangle-\overline{a}\langle\mathbf{u},\mathbf{v}\rangle-a(\langle\mathbf{v},\mathbf{u}\rangle-\overline{a}\langle\mathbf{v},\mathbf{v})\rangle=\langle\mathbf{u},\mathbf{u}\rangle-\overline{a}\langle\mathbf{u},\mathbf{v}\rangle-a\langle\mathbf{v},\mathbf{u}\rangle+a\overline{a}\langle\mathbf{v},\mathbf{v}\rangle.$$ \noindent En particular, tomando $a=\frac{\langle\mathbf{u},\mathbf{v}\rangle}{\langle\mathbf{v},\mathbf{v}\rangle}$ y sustituyendo en la desigualdad anterior, obtenemos que $$0\leq\langle\mathbf{u},\mathbf{u}\rangle-\frac{\langle\mathbf{v},\mathbf{u}\rangle}{\langle\mathbf{v},\mathbf{v}\rangle}\langle\mathbf{u},\mathbf{v}\rangle-\frac{\langle\mathbf{u},\mathbf{v}\rangle}{\langle\mathbf{v},\mathbf{v}\rangle}\langle\mathbf{v},\mathbf{u}\rangle+\frac{\langle\mathbf{u},\mathbf{v}\rangle}{\langle\mathbf{v},\mathbf{v}\rangle}\frac{\langle\mathbf{v},\mathbf{u}\rangle}{\langle\mathbf{v},\mathbf{v}\rangle}\langle\mathbf{v},\mathbf{v}\rangle=$$ $$\langle\mathbf{u},\mathbf{u}\rangle-2\frac{\langle\mathbf{u},\mathbf{v}\rangle\langle\mathbf{v},\mathbf{u}\rangle}{\langle\mathbf{v},\mathbf{v}\rangle}+\frac{\langle\mathbf{u},\mathbf{v}\rangle\langle\mathbf{v},\mathbf{u}\rangle}{\langle\mathbf{v},\mathbf{v}\rangle}=||\mathbf{u}||^2-\frac{|\langle\mathbf{u},\mathbf{v}\rangle|^2}{||\mathbf{v}||^2}\implies$$ $$0\leq ||\mathbf{u}||^2-\frac{|\langle\mathbf{u},\mathbf{v}\rangle|^2}{||\mathbf{v}||^2}\implies \frac{|\langle\mathbf{u},\mathbf{v}\rangle|^2}{||\mathbf{v}||^2}\leq ||\mathbf{u}||^2\implies|\langle\mathbf{u},\mathbf{v}\rangle|^2\leq ||\mathbf{u}||^2\hspace{0.5mm}||\mathbf{v}||^2\implies|\langle\mathbf{u},\mathbf{v}\rangle|\leq ||\mathbf{u}||\hspace{0.5mm}||\mathbf{v}||.$$

\end{proof}
\end{teorema}

\newpage
\subsection{Interpretación geométrica del producto escalar: proyecciones y ortogonalidad}\label{Subsec:Interpretación_geométrica_del_producto_escalar} 

\subsubsection{Producto escalar y proyecciones} \label{Subsec:Producto_escalar_y_proyecciones}

Como quizás aprendiste en tus cursos de Geometría Analítica y/o Mecánica Vectorial, el producto escalar entre vectores de $\mathbb{R}^2$ tiene una definición geométrica dada por $$\langle\mathbf{u},\mathbf{v}\rangle\equiv ||\mathbf{u}||\hspace{0.5mm}||\mathbf{v}||\cos{\theta}=||\mathbf{v}||\hspace{0.5mm} ||\mathbf{u}||\cos{\theta},$$ \noindent donde $\theta$ es el ángulo positivo más chico entre $\mathbf{u}$ y $\mathbf{v}$; esta definición es equivalente a la algebráica dada al principio de la sección \ref{Ejem:Producto_escalar}. El producto $||\mathbf{u}||\cos{\theta}$ es igual a la \emph{proyección escalar} del vector $\mathbf{u}$ sobre el vector $\mathbf{v}$. Si escribimos dicha proyección escalar como $P_{\mathbf{v}}(\mathbf{u})$, tenemos que

$$\langle\mathbf{u},\mathbf{v}\rangle=||\mathbf{v}||P_{\mathbf{v}}(\mathbf{u}) \implies P_{\mathbf{v}}(\mathbf{u})=\frac{\langle\mathbf{u},\mathbf{v}\rangle}{||\mathbf{v}||}.$$ 

\noindent Este es el escalar por el cual habría que multiplicar un vector de norma $1$ en la dirección y sentido de $\mathbf{v}$ para obtener el vector componente de $\mathbf{u}$ colineal a $\mathbf{v}$, también conocido como \emph{proyección vectorial}. Empleando esta notación, la proyección vectorial de $\mathbf{u}$ sobre $\mathbf{v}$ se puede escribir como $P_{\mathbf{v}}(\mathbf{u})\frac{\mathbf{v}}{||\mathbf{v}||} .$ En la figura \ref{fig:Proyecciones_y_ortogonalidad} se muestran ejemplos de proyecciones vectoriales entre vectores de $\mathbb{R}^2$.

\begin{figure}[h!]
    \begin{subfigure}[b]{0.3\textwidth}
    \centering
    \begin{tikzpicture}[thick,scale=1, every node/.style={scale=1}]
        \draw[thick,<->] (0,0) -- (4,0);
        \draw[thick,<->] (0,0) -- (0,4);
        \draw[step=1cm,gray,very thin,dashed] (0,0) grid (3.9,3.9);
        \foreach \x in {1,2,3}
            \draw (\x cm, 1pt) -- (\x cm, -1pt) node[anchor=north] {$\x$};
        \foreach \y in {1,2,3}
            \draw (1pt, \y cm) -- (-1pt, \y cm) node[anchor=east] {$\y$};
        \draw[ROJO,very thick,->] (0,0) -- (1.5,2.5) node[left, xshift=-0.2cm] {$\mathbf{u}$};
        \draw[ROJO,dashed, thin] (0,0) -- (2.4,4);
        \draw[AZUL,very thick,->] (0,0) -- (3.5,2) node[right, xshift=0.1cm] {$\mathbf{v}$};
        \draw[magenta,->] (0,0) -- (1.809,3.015) node[left, xshift=0.05cm, yshift=0.3cm] {$P_{\mathbf{u}}(\mathbf{v})\frac{\mathbf{u}}{||\mathbf{u}||}$};
        \draw[magenta,dotted,thin] (1.809,3.015) -- (3.5,2);
    \end{tikzpicture}
    \caption{}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0,3\textwidth}
    \centering
    \begin{tikzpicture}[thick,scale=1, every node/.style={scale=1}]
        \draw[thick,<->] (0,0) -- (4,0);
        \draw[thick,<->] (0,0) -- (0,4);
        \draw[step=1cm,gray,very thin,dashed] (0,0) grid (3.9,3.9);
        \foreach \x in {1,2,3}
            \draw (\x cm, 1pt) -- (\x cm, -1pt) node[anchor=north] {$\x$};
        \foreach \y in {1,2,3}
            \draw (1pt, \y cm) -- (-1pt, \y cm) node[anchor=east] {$\y$};
        \draw[ROJO,very thick,->] (0,0) -- (1.5,2.5) node[left, xshift=-0.2cm] {$\mathbf{u}$};
        \draw[ROJO,->] (0,0) -- (0.7059,1.1765) node[left,xshift=0.1cm,yshift=0.3cm] {$\frac{\mathbf{u}}{||\mathbf{u}||}$};
        \draw[AZUL,very thick,->] (0,0) -- (3.5,2) node[right, xshift=0.1cm] {$\mathbf{v}$};
        \draw[AZUL,->] (0,0) -- (1.1846,0.677) node[right,xshift=-0.2cm,yshift=-0.3cm] {$\frac{\mathbf{v}}{||\mathbf{v}||}$};
    \end{tikzpicture}
    \caption{}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0,3\textwidth}
    \centering
    \begin{tikzpicture}[thick,scale=1, every node/.style={scale=1}]
        \draw[thick,<->] (0,0) -- (4,0);
        \draw[thick,<->] (0,0) -- (0,4);
        \draw[step=1cm,gray,very thin,dashed] (0,0) grid (3.9,3.9);
        \foreach \x in {1,2,3}
            \draw (\x cm, 1pt) -- (\x cm, -1pt) node[anchor=north] {$\x$};
        \foreach \y in {1,2,3}
            \draw (1pt, \y cm) -- (-1pt, \y cm) node[anchor=east] {$\y$};
        \draw[ROJO,->,very thick] (0,0) -- (1.5,2.5) node[left, xshift=-0.2cm] {$\mathbf{u}$};
        \draw[AZUL,->,very thick] (0,0) -- (3.5,2) node[right, xshift=0.1cm] {$\mathbf{v}$};
        \draw[magenta,->] (0,0) -- (2.2077,1.2615) node[right, xshift=-0.15cm, yshift=-0.3cm] {$P_{\mathbf{v}}(\mathbf{u})\frac{\mathbf{v}}{||\mathbf{v}||}$};
        \draw[magenta,dotted,thin] (1.5,2.5) -- (2.2077,1.2615);
    \end{tikzpicture}
    \caption{}
    \end{subfigure}
    \caption{Proyecciones vectoriales entre dos vectores $\mathbf{u}$ y $\mathbf{v}$ de $\mathbb{R}^2$. En la figura (a) tenemos la proyección vectorial del vector $\mathbf{v}$ sobre el vector $\mathbf{u}$, es decir, la componente de $\mathbf{v}$ en el eje en el cual se encuentra $\mathbf{u}$. En (b) se muestran los vectores de norma $1$ que tienen la misma dirección y sentido que $\mathbf{u}$ y $\mathbf{v}$, respectivamente. En (c) se muestra la proyeccíon vectorial de $\mathbf{u}$ sobre $\mathbf{v}$. Nótese en cada caso que la proyección vectorial se obtiene reescalando el vector sobre el que se proyecta para que tenga norma $1$ y después multiplicando dicho vector por la proyección escalar correspondiente.}
    \label{fig:Proyecciones_y_ortogonalidad}
\end{figure}

Observemos que, ya que tanto $\cos\theta$ como las normas $||\mathbf{u}||$ y $||\mathbf{v}||$ son números reales, entonces $$\langle\mathbf{u},\mathbf{v}\rangle=||\mathbf{v}||P_{\mathbf{v}}(\mathbf{u})=||\mathbf{v}||\hspace{0.5mm}||\mathbf{u}||\cos\theta = ||\mathbf{u}||\hspace{0.5mm}||\mathbf{v}||\cos\theta=||\mathbf{u}||P_{\mathbf{u}}(\mathbf{v})=\langle\mathbf{v},\mathbf{u}\rangle.$$\noindent Esto no es más que la propiedad conmutativa del producto escalar en un espacio vectorial real \textemdash visto desde un punto de vista geométrico.

Ahora imaginemos que tenemos dos vectores $\mathbf{u},\mathbf{v}\in\mathbb{R}^2$ de la misma magnitud (i.e., $||\mathbf{u}||=||\mathbf{v}||$) trazados en el plano cartesiano. En este caso, la bisectriz del ángulo que los separa traza un eje de simetría. Si trazamos la proyección de cada uno de los vectores sobre el otro, veremos de forma más intuitiva que $$||\mathbf{u}||P_{\mathbf{u}}(\mathbf{v})=||\mathbf{v}||P_{\mathbf{v}}(\mathbf{u}),$$ con lo cual reafirmamos que el producto escalar conmuta. Supongamos ahora que reescalamos alguno de los vectores, digamos $\mathbf{u}$, al doble de su longitud \textemdash efectivamente duplicando así su norma\textemdash\hspace{0.5mm}, y llamamos a este nuevo vector $\mathbf{u}'$. Entonces, algebráicamente vemos que $$\langle\mathbf{u}',\mathbf{v}\rangle=||\mathbf{u'}||\hspace{0.5mm}||\mathbf{v}||\cos\theta=||2\mathbf{u}||\hspace{0.5mm}||\mathbf{v}||\cos\theta=2||\mathbf{u}||\hspace{0.5mm}||\mathbf{v}||\cos{\theta}=2||\mathbf{u}||P_{\mathbf{u}}(\mathbf{v}).$$ \noindent Geométricamente, podemos ver esto como que $\langle\mathbf{u}',\mathbf{v}\rangle=(2||\mathbf{u}||)P_{\mathbf{u}}(\mathbf{v})$ o, equivalentemente, que $\langle\mathbf{u}',\mathbf{v}\rangle=||\mathbf{u}||(2P_{\mathbf{u}}(\mathbf{v}))\footnote{Este mismo argumento se expone de forma animada en el siguiente video: \url{https://www.youtube.com/watch?v=LyGKycYT2v0&list=PL_w8oSr1JpVCZ5pKXHKz6PkjGCbPbSBYv&index=11&t=255s}.}.$ Observemos que la elección del vector reescalado fue arbitraria \textemdash de haber reescalado $\mathbf{v}$ por $2$ en vez de $\mathbf{u}$, hubiéramos obtenido el mismo resultado. La discusión anterior es simplemente la interpretación geométrica del hecho de que $\langle2\mathbf{u},\mathbf{v}\rangle=2\langle\mathbf{u},\mathbf{v}\rangle=\langle\mathbf{u},2\mathbf{v}\rangle$ en espacios vectoriales reales.

Similarmente, la interpretación geométrica del hecho de que $\langle\mathbf{u}+\mathbf{v},\mathbf{w}\rangle=\langle\mathbf{u},\mathbf{w}\rangle+\langle\mathbf{v},\mathbf{w}\rangle$ ó $\langle\mathbf{w},\mathbf{u}+\mathbf{v}\rangle=\langle\mathbf{w},\mathbf{u}\rangle+\langle\mathbf{w},\mathbf{v}\rangle$ se puede ver directamente de la Ley del paralelogramo. 

\vspace{3mm}

Sin embargo, al generalizar esta operación a cualquier tipo de espacios vectoriales nos encontramos con un problema: en general, el producto escalar no es conmutativo, por lo cual debemos decidir de qué manera precisa definiremos a esta operación. Por razones que veremos más adelante, la definición que resulta más conveniente es la siguiente:

\begin{tcolorbox}
    \underline{Def.} Sean $\mathbf{u},\mathbf{v}$ vectores de un espacio vectorial $V$ arbitrario tal que $\mathbf{v}$ es distinto al vector nulo. Definimos a la operación $P_{\mathbf{v}}(\hspace{0.5mm}\cdot\hspace{0.5mm} ):V\to K$, conocida como la \emph{proyección escalar del vector} $\mathbf{u}$ \emph{sobre el vector} $\mathbf{v}$, como \[
        P_{\mathbf{v}}(\mathbf{u}) \equiv \frac{\langle\mathbf{u},\mathbf{v}\rangle}{||\mathbf{v}||}
    .\] Además, definimos la \emph{proyección vectorial} de $\mathbf{u}$ sobre $\mathbf{v}$ como $P_{\mathbf{v}}(\mathbf{u}) \frac{\mathbf{v}}{||\mathbf{v}||}$.
 
\end{tcolorbox}

\subsubsection{Ortogonalidad}

Por otro lado, ya que $\cos(\frac{\pi}{2})=0$, podemos ver directamente que, si dos vectores en $\mathbb{R}^2$ son perpendiculares entre sí en el plano, su producto escalar será cero. Esto significaría que la proyección de cualquiera de estos vectores sobre el otro sería nula \textemdash lo cual, en el plano cartesiano, es sinónimo de que los vectores sean perpendiculares\textemdash \hspace{0.5mm}; más generalmente, a esta propiedad se le conoce como \emph{ortogonalidad}, y siempre se asocia con un producto escalar nulo por razones que veremos más adelante.

\vspace{2mm}

\begin{tcolorbox} \label{Def:Vectores_ortogonales_y_subconjunto_ortogonal}
    \underline{Def.} Se dice que dos vectores $\mathbf{u}, \mathbf{v}\in V$ son \emph{ortogonales} si su producto escalar es nulo (i.e., si la proyección de cualquiera de los vectores sobre el otro es cero). Matemáticamente, esto se escribe como $\mathbf{u}\perp\mathbf{v}\iff\langle\mathbf{u},\mathbf{v}\rangle=0$ \hspace{0.5mm} ó, equivalentemente, $\mathbf{u}\perp\mathbf{v}\iff P_{\mathbf{u}}(\mathbf{v})=0=P_{\mathbf{v}}(\mathbf{u})$\footnote{Observemos que esta definición también aplica para espacios vectoriales complejos, ya que $\overline{0}=0$.}.

    \vspace{3mm} 

    \underline{Def.} Se dice que un subconjunto $S\subset V$ es \emph{ortogonal} si cualquier vector de $S$ es ortogonal al resto de los vectores del conjunto, es decir, si $\forall\hspace{1.5mm} \mathbf{u}_i, \mathbf{u}_j\in S, \mathbf{u}_i\neq\mathbf{u}_j\implies \langle\mathbf{u}_i,\mathbf{u}_j\rangle=0.$
\end{tcolorbox}

Por ejemplo, en $\mathbb{R}^3, \begin{pmatrix} 2 & 3 & 1 \end{pmatrix}\perp\begin{pmatrix} -4 & 2 & 2 \end{pmatrix}$, ó $\begin{pmatrix} 2 & 3 & 1 \end{pmatrix}\perp\begin{pmatrix} -8 & 4 & 4 \end{pmatrix}$; además, el subconjunto $\{\begin{pmatrix} 5  & 0 & 0 \end{pmatrix}, \begin{pmatrix} 0 & 7 & 0 \end{pmatrix}, \begin{pmatrix} 0 & 0 & -4 \end{pmatrix}\}$ es ortogonal. En cambio, en el espacio de funciones polinomiales reales $P^2$ restringido al intervalo $[-1,1]$, donde definimos el producto escalar como $\langle f,g\rangle\equiv \int_{-1}^{1}f(x)g(x)\hspace{1mm} dx$, el conjunto $\{1,x,\frac{1}{2}(3x^2-1)\}$ es ortogonal (¿podrás comprobarlo?).

\subsubsection{Vectores unitarios}

Nótese por la definición dada en \ref{Def:Vectores_ortogonales_y_subconjunto_ortogonal} y las propiedades del producto escalar que, si dos vectores son ortogonales, entonces reescalar cualquiera de ellos no afecta a su ortogonalidad. Además, notemos que la definición geométrica del producto escalar para vectores en $\mathbb{R}^2$ se simplifica cuando la norma de los vectores es uno. De hecho, como veremos más adelante, este tipo de vectores en general pueden simplificar muchas expresiones de operaciones realizadas en espacios vectoriales, por lo cual les damos un nombre especial.

\vspace{2mm}

\begin{tcolorbox} \label{Def:Vector_unitario_y_subconjunto_ortonormal} 
    \underline{Def.} Decimos que un vector $\mathbf{v}\in V$ es \emph{unitario} cuando su norma es igual a uno, i.e., si y sólo si $||\mathbf{v}||=1.$ Los vectores unitarios suelen denotarse con una cuña encima de la letra, e.g. $\hat{i}, \hat{j}, \hat{k},$ etc. 
    
    \vspace{3mm}
    
    \underline{Def.} Decimos que un subconjunto $S\subset V$ es \emph{ortonormal} si es un conjunto ortogonal donde todos los vectores son unitarios. Es decir, $S$ es \emph{ortonormal} si y sólo si $\langle\mathbf{v}_i,\mathbf{v}_j\rangle=\delta_{ij}\hspace{3mm} \forall\hspace{1.5mm} \mathbf{v}_i,\mathbf{v}_j\in S,$ donde $\delta_{ij}$ es la delta de Kronecker de dos entradas.
\end{tcolorbox}

Por ejemplo, el vector $\begin{pmatrix} \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} \end{pmatrix}$ es unitario en $\mathbb{R}^2 (\text{ó}\hspace{1mm} \mathbb{C}^2)$, mientras que los vectores $\begin{pmatrix} \frac{-1}{\sqrt{2}} & \frac{i}{\sqrt{2}} \end{pmatrix}$ y $\begin{pmatrix} \frac{-i}{\sqrt{2}} & \frac{1}{\sqrt{2}} \end{pmatrix}$ son unitarios en $\mathbb{C}^2.$

Por último, notemos que si $\mathbf{v}\in V$ es un vector arbitrario no unitario distinto al vector nulo, entonces el vector $$\hat{\mathbf{v}}=\frac{\mathbf{v}}{||\mathbf{v}||}$$ \emph{es} unitario, i.e., tiene norma igual a uno. Por ello, a este proceso de multiplicar un vector no nulo y no unitario por el inverso multiplicativo de su norma se le conoce como \emph{normalización}. Además, ya que por la definición de norma $||c\mathbf{v}||=|c|\hspace{0.5mm} ||\mathbf{v}||$ entonces, si hacemos el producto de un vector unitario por cualquier escalar de valor absoluto igual a uno (i.e., con $|c|=1$), obtendremos nuevamente un vector unitario.

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}[thick,scale=0.8, every node/.style={scale=1}]
        \draw[thick,<->] (0,0) -- (6,0);
        \draw[thick,<->] (0,0) -- (0,6);
        \draw[step=1cm,gray,very thin,dashed] (0,0) grid (5.9,5.9);
        \foreach \x in {1,2,3,4,5}
            \draw (\x cm,1pt) -- (\x cm,-1pt) node[anchor=north] {$\x$};
        \foreach \y in {1,2,3,4,5}
            \draw (1pt,\y cm) -- (-1pt,\y cm) node[anchor=east] {$\y$};
        \draw[ROJO,thin,dashed,->] (0,0) -- (3,3) node[] at (3.5,3.5) {$\begin{pmatrix} 3 & 3 \end{pmatrix}$};
    \draw[ROJO,very thick,->] (0,0) -- (0.71,0.71) node[] at (3.4,0.6) {$\frac{1}{\sqrt{18}}\begin{pmatrix}3&3\end{pmatrix}=\begin{pmatrix} \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} \end{pmatrix}$};
    \end{tikzpicture}
        \caption{Ejemplo de \emph{normalización} del vector $\protect\begin{pmatrix}3&3\protect\end{pmatrix}\in\mathbb{R}^2$. Ya que la norma de este vector es igual a $\sqrt{3^2+3^2}=\sqrt{18},$ hacemos su producto por el escalar $\frac{1}{\sqrt{18}}$ \textemdash el inverso multiplicativo de $\sqrt{18}$ en el campo real\textemdash\hspace{0.5mm}  y obtenemos al vector resultante $\protect\begin{pmatrix}\frac{1}{\sqrt{2}}&\frac{1}{\sqrt{2}}\protect\end{pmatrix}$, que tiene el mismo sentido y dirección que $\protect\begin{pmatrix}3&3\protect\end{pmatrix}$, pero con norma igual a uno.}
    \label{fig:Normalización_en_el_plano_cartesiano}
\end{figure}

Observemos que podemos utilizar la notación usual de vector unitario para reescribir la proyección \emph{vectorial} del vector $\mathbf{u}$ sobre el vector $\mathbf{v}$ como \[
    P_{\mathbf{v}}(\mathbf{u}) \ \hat{\mathbf{v}}
,\] \noindent donde $P_{\mathbf{v}}(\mathbf{u})$ es la proyección \emph{escalar}  de $\mathbf{u}$ sobre $\mathbf{v}$.

\subsection{Producto vectorial (cruz)*} 

El producto vectorial es una operación de gran utilidad, principalmente en algunas ramas de la física clásica (en las cuales se trabaja con vectores en $\mathbb{R}^3$), y tiene una interpretación geométrica bastante rica, por lo cual se menciona brevemente en estas notas. En esta sección también se mencionan por primera vez los temas de matrices, determinantes y vectores canónicos, los cuales asumo que ya debes conocer, aunque más adelante en el curso los repasaremos.

\subsubsection{Definición del producto vectorial (cruz)} \label{Def:Producto_vectorial}

En $\mathbb{R}^3$ se puede definir una operación entre dos vectores que da como resultado un tercer vector, el cual es ortogonal a los dos anteriores. A esta operación se le conoce como \emph{producto vectorial}.

\begin{tcolorbox}
\underline{Def.} En $\mathbb{R}^3$, el producto vectorial $\times:V\times V \rightarrow V$ entre dos vectores $\mathbf{r}=(r_1,r_2,r_3)$ y $\mathbf{s}=(s_1,s_2,s_3)$ se define como

$$\mathbf{r}\times\mathbf{s} \equiv (r_2s_3-r_3s_2,r_3s_1-r_1s_3,r_1s_2-r_2s_1).$$

\noindent Dado el símbolo ($\times$) utilizado para denotar esta operación, también se le conoce como \emph{producto cruz}.
\end{tcolorbox}{}

Algebráicamente, el producto cruz $\mathbf{r}\times\mathbf{s}$ también puede ser visto como el determinante de una matriz de $3\times3$ como sigue:

$$\mathbf{r}\times\mathbf{s} = \text{det} \begin{vmatrix} \mathbf{i}&\mathbf{j}&\mathbf{k} \\ r_1&r_2&r_3 \\ s_1&s_2&s_3 \\
\end{vmatrix} = \mathbf{i}(r_2s_3-r_3s_2)+\mathbf{j}(r_3s_1-r_1s_3)+\mathbf{k}(r_1s_2-r_2s_1),$$

\noindent donde $\mathbf{i},\mathbf{j}$ y $\mathbf{k}$ son los vectores canónicos de $\mathbb{R}^3$ $(1,0,0)$, $(0,1,0)$ y $(0,0,1)$, respectivamente.

Geométricamente, si nombramos como $\theta$ el menor ángulo de separación positivo entre dos vectores $\mathbf{r}$ y $\mathbf{s}\in\mathbb{R}^3$, entonces podemos hacer la definición equivalente $\mathbf{r}\times\mathbf{s}=||\mathbf{r}|| \hspace{0.5mm}  ||\mathbf{s}||\sin\theta$. En este caso, la magnitud del vector resultante de hacer el producto vectorial $\mathbf{r}\times\mathbf{s}$ se interpreta como la magnitud del área del paralelogramo formado por los vectores $\mathbf{r}$ y $\mathbf{s}$. De aquí se sigue que el producto vectorial de dos vectores ortogonales sea igual al producto de sus normas, mientras que el producto cruz de dos vectores colineales sea $0$ (en particular, el producto cruz de cualquier vector consigo mismo es igual a $0$). Además, de ambas definiciones (algebráica y geométrica) se sigue que el producto vectorial de cualquier vector en $\mathbb{R}^3$ con el vector nulo ($\mathbf{0}$) sea $\mathbf{0}$.

\subsubsection{Propiedades del producto vectorial (cruz)} \label{Prop:Producto_vectorial}

Las siguientes propiedades son fáciles de demostrar para cualesquiera $\mathbf{r},\mathbf{s},\mathbf{t}\in\mathbb{R}^3$ a partir de la definición de la sección \ref{Def:Producto_vectorial} (¡inténtalo!):

\begin{center}
\begin{tabular}{lr}
    $\mathbf{s}\times\mathbf{r} = -\mathbf{r}\times\mathbf{s}$ & Anticonmutatividad del producto vectorial \\
    $(\mathbf{r}\times\mathbf{s})\times\mathbf{t}\neq\mathbf{r}\times(\mathbf{s}\times\mathbf{t})$ & No asociatividad del producto vectorial \\
    $\mathbf{r}\times(\mathbf{s}+\mathbf{t}) = \mathbf{r}\times\mathbf{s}+\mathbf{r}\times\mathbf{t}$ & Distributividad bajo la suma vectorial \\
    $(a\mathbf{r}\times\mathbf{s}) = a(\mathbf{r}\times\mathbf{s})=(\mathbf{r}\times a\mathbf{s})$ & Compatibilidad con el producto de un vector por un escalar\\
    $\mathbf{r}\times(\mathbf{s}\times\mathbf{t})+\mathbf{s}\times(\mathbf{t}\times\mathbf{r})+\mathbf{t}\times(\mathbf{r}\times\mathbf{s}) = \mathbf{0}$ & Identidad de Jacobi para el producto vectorial.
\end{tabular}
\end{center}



\subsection{Triple producto escalar*}

Al unir el producto escalar con el producto vectorial, se obtiene una operación llamada \emph{triple producto escalar} entre tres vectores, que da como resultado un escalar. La magnitud del triple producto escalar se interpreta geométricamente como la magnitud del volumen del paralelepípedo formado por los tres vectores.

\subsubsection{Definición de triple producto escalar}
\begin{tcolorbox}
\underline{Def.} El \emph{triple producto escalar} entre tres vectores $\mathbf{r},\mathbf{s},\mathbf{t}\in\mathbb{R}^3$ se define como

$$\mathbf{r}\cdot\mathbf{s}\times\mathbf{t} \equiv r_1(s_2t_3-s_3t_2) + r_2(s_3t_1-s_1t_3) + r_3(s_1t_2-s_2t_1),$$

\noindent es decir, primero se realiza el producto vectorial $\mathbf{s}\times\mathbf{t}$ y luego se realiza el producto escalar entre el vector resultante de esa operación y $\mathbf{r}$.
\end{tcolorbox}

Recordemos que el producto escalar se realiza entre dos vectores y da como resultado un escalar, mientras que el producto vectorial se realiza entre dos vectores pero da como resultado un vector. Por lo tanto, no hay ambigüedad en la expresión $\mathbf{r}\cdot\mathbf{s}\times\mathbf{t}$, ya que la única forma lógica de juntar estas dos operaciones es realizando primero el producto vectorial y después el producto escalar; por ende, escribir esta operación como $\mathbf{r}\cdot(\mathbf{s}\times\mathbf{t})$ sería redundante.

Al igual que el producto vectorial, el triple producto escalar también puede ser visto algebráicamente como el determinante de una matriz:

$$\mathbf{r}\cdot\mathbf{s}\times\mathbf{t} = \begin{vmatrix} r_1&r_2&r_3 \\ s_1&s_2&s_3 \\ t_1&t_2&t_3 \end{vmatrix}.$$

    Además, recordando que el producto escalar de dos vectores ortogonales es igual a $0$ y que el vector resultante de la operación $\mathbf{r}\times\mathbf{s}$ es ortogonal tanto a $\mathbf{r}$ como a $\mathbf{s}$, se sigue directamente que $\mathbf{r}\cdot\mathbf{r}\times\mathbf{s}=0=\mathbf{r}\cdot\mathbf{s}\times\mathbf{r}$.



\subsubsection{Propiedades del triple producto escalar} \label{Prop:Triple_producto_escalar}

\begin{center}
\begin{tabular}{lr}
    $\mathbf{r}\cdot\mathbf{s}\times\mathbf{t} = \mathbf{s}\cdot\mathbf{t}\times\mathbf{r} = \mathbf{t}\cdot\mathbf{r}\times\mathbf{s}$ & Conmutatividad bajo permutaciones cíclicas \\
    $\mathbf{r}\cdot\mathbf{s}\times\mathbf{t} = \mathbf{r}\times\mathbf{s}\cdot\mathbf{t}$ & Invariancia bajo intercambio de operadores \\
    $\mathbf{r}\cdot\mathbf{s}\times\mathbf{t} = -\mathbf{r}\cdot\mathbf{t}\times\mathbf{s} = -\mathbf{s}\cdot\mathbf{r}\times\mathbf{t} = -\mathbf{t}\cdot\mathbf{s}\times\mathbf{r}$ & Anticonmutatividad bajo intercambio de dos vectores \\
\end{tabular}
\end{center}

Nota: también se puede definir un \emph{triple producto vectorial} entre tres vectores que da como resultado un vector como $\mathbf{r}\times\mathbf{s}\times\mathbf{t}$. Éste es de gran utilidad para hacer demostraciones de algunas identidades de cálculo vectorial en $\mathbb{R}^3$.

\subsection{Ejercicios de repaso}

\subsubsection{Producto escalar} \label{Ejer:Producto_escalar} 

\begin{enumerate}
    \item Sea $V$ un espacio vectorial con producto escalar. Demuestra que para todo $\mathbf{v}\in V, \langle \mathbf{v},\mathbf{0}\rangle=\langle\mathbf{0},\mathbf{v}\rangle=0.$ Además, demuestra que si $\langle\mathbf{u},\mathbf{v}\rangle=\langle \mathbf{u},\mathbf{w}\rangle$ para todo $\mathbf{u}\in V$, entonces $\mathbf{v}=\mathbf{w}.$
    \item Explica por qué no se puede definir un producto escalar de la forma $\langle f,g\rangle=\int_a^b f(x)g(x)dx$ para funciones integrables $f$ y $g$ con integrales con valor infinito en $[a,b]$. 
    \item Sea $\mathbb{C}^2$ un espacio vectorial complejo con $\mathbf{q},\mathbf{r}\in\mathbb{C}^2$. Explica cuál(es) de las propiedades de la sección \ref{Prop:Producto_escalar} no se cumpliría(n) si definiéramos el producto escalar en este espacio vectorial ingenuamente como $\mathbf{q}\cdot\mathbf{r}=\sum_{i=1}^n q_ir_i$  con $n=2$ (nótese que esta misma definición de producto escalar nos generaría problemas al intentar usarla para definir una norma en este espacio). 
\end{enumerate}

\subsubsection{Norma} \label{Ejer:Norma}

\begin{enumerate}
    \item Calcula la norma de las funciones dadas en el primer ejercicio de la sección \ref{Ejer:Producto_escalar}. (Nota: utiliza la norma inducida por ese mismo producto escalar.) 
    \item ¿El conjunto $\{(x_1,x_2,...,x_n)\mathop|\mathop x_i\in\mathbb{R} \land ||(x_1,x_2,...,x_n)||\leq1\}$ puede formar un espacio vectorial sobre el campo real? Argumenta. 
    \item Argumenta e ilustra la interpretación geométrica de la desigualdad de Cauchy-Schwarz para dos vectores cualesquiera de $\mathbb{R}^2$ (Nota: ver sec. \ref{Teo:Cauchy-Schwarz}). 
    \item Demuestra la desigualdad del triángulo $||\mathbf{a}+\mathbf{b}|| \leq ||\mathbf{a}||+||\mathbf{b}||$ para cualesquiera dos vectores en un espacio $V$ con producto escalar positivo definido donde la norma $||\mathbf{a}||\equiv+\sqrt{(\mathbf{a},\mathbf{a})}\hspace{3mm}\forall\hspace{1.5mm} \mathbf{a}\in V$ (pista: usa la desigualdad de Cauchy-Schwarz)\footnote{Con esto habrás demostrado que a partir de cualquier producto escalar positivo definido $(\mathbf{a},\mathbf{b})$ se puede definir una norma como $||\mathbf{a}|| = +\sqrt{(\mathbf{a},\mathbf{a})}.$}. Argumenta e ilustra su interpretación geométrica para dos vectores cualesquiera de $\mathbb{R}^2$. 
\end{enumerate}

\subsubsection{Interpretación geométrica del producto escalar: proyecciones y ortogonalidad}
\begin{enumerate}
\item Sea $\mathbf{v}\in\mathbb{R}^2$ no-nulo. Prueba que el conjunto $\{\mathbf{u}\in\mathbb{R}^2\hspace{0.5mm} |\hspace{0.5mm} \langle\mathbf{u},\mathbf{v}\rangle =0\}$ es una recta que pasa por el origen y llévala a una expresión de la forma $ax+by=c.$ 
    \item Sean $\mathbf{v}=\begin{pmatrix} v_1 & v_2 \end{pmatrix}, \mathbf{u}=\begin{pmatrix}u_1 & u_2 \end{pmatrix}$ vectores del espacio real $\mathbb{R}^2.$ Deduce que $\mathbf{u}\cdot\mathbf{v}=u_1v_1+u_2v_2=||\mathbf{u}||\hspace{0.5mm}||\mathbf{v}||\cos(\theta)$, donde $\theta$ es el ángulo mínimo entre ambos vectores en el plano cartesiano. (Nota: supon que ambos vectores son no nulos y encuentra al escalar $k\in\mathbb{R}$ tal que $\langle\mathbf{u},\mathbf{u}-k\mathbf{v}\rangle=0$. ¿Por qué es especial este escalar?) 
    \item Define a los vectores $\mathbf{u}=\begin{pmatrix} a_1 & a_2 \end{pmatrix}, \mathbf{v}=\begin{pmatrix} a_3 & a_4\end{pmatrix}$ y $\mathbf{w}=\begin{pmatrix} a_5 & a_6\end{pmatrix}\in\mathbb{R}^2,$ donde $a_i$ corresponde al $i$-ésimo dígito de tu número de cuenta. Explica gométricamente el hecho de que $\langle\mathbf{u}+\mathbf{v},\mathbf{w}\rangle=\langle\mathbf{u},\mathbf{w}\rangle+\langle\mathbf{v},\mathbf{w}\rangle$, en términos de las proyecciones discutidas en la sección \ref{Subsec:Interpretación_geométrica_del_producto_escalar} (Nota: recuerda la Ley del paralelogramo). 
\end{enumerate}

\subsubsection{Producto vectorial (cruz)*}

\begin{enumerate}
    \item Demuestra las propiedades del producto vectorial de la sección \ref{Prop:Producto_vectorial}. Además, da una interpretación geométrica para la primera, tercera y cuarta propiedad enlistadas. (Nota: si quieres hacer estas demostraciones utilizando una notación condensada, te recomiendo investigar acerca del símbolo de Levi-Civita el cual, junto con la delta de Kronecker, facilitan la escritura de muchas demostraciones de cálculo vectorial, entre otras áreas de las matemáticas.) 
%    \item Da una definición de un producto cruz entre vectores de $\mathbb{C}^3$ tal que se mantengan las propiedades vistas en la sección \ref{Prop:Producto_vectorial}.
\end{enumerate}

\subsubsection{Triple producto escalar*}

\begin{enumerate}
    \item Demuestra las propiedades del triple producto escalar de la sección \ref{Prop:Triple_producto_escalar}. 
\end{enumerate}{}

\begin{tcolorbox}
\begin{center}
    \textbf{Nota aclaratoria: \emph{Sobre nombres y traducciones...}}
\end{center}

\hspace{2.5mm}Como seguramente habrás notado al leer los libros recomendados en la sección \ref{Bibliografía}, al producto escalar en inglés se le conoce como \emph{inner product}, y a los espacios vectoriales dotados de un producto escalar se les conoce como \emph{inner product spaces}. En español, al producto escalar también se le conoce como \emph{producto interior}; sin embargo, existe otro tipo de producto diferente al producto escalar al cual en inglés, desafortunadamente, le llaman \emph{interior product}.

\vspace{5mm}
\hspace{2.5mm}Esto significa que, en inglés, la convención es que \emph{scalar product} e \emph{interior product} sean operaciones diferentes mientras que, en español, la convención es que \emph{producto escalar} y \emph{producto interior} se refieran a la misma operación. Algunos textos en español utilizan \emph{producto interno} (en vez de producto interior) como sinónimo de \emph{producto escalar} para homologar los nombres con los utilizados en inglés pero, por ahora, las convenciones preponderantes en español e inglés no permiten una traducción directa.

\vspace{5mm}
\hspace{2.5mm}Por lo anterior, en estas notas decidí usar únicamente el nombre de \emph{producto escalar} para la operación entre dos vectores que da como resultado un escalar (la cual acostumbramos llamar \emph{producto punto} cuando esos vectores son $n$-tuplas), pero es importante que sepan que esta operación es equivalente al \emph{\underline{inner} product} de los textos en inglés.

\vspace{5mm}
\hspace{2.5mm} Para empeorar la situación, algunos textos en inglés se refieren a la operación de producto de un vector por un escalar como \emph{scalar multiplication} \textemdash por lo cual algunos textos en español pueden referirse a esta operación como \emph{multiplicación escalar} o, inclusive, \emph{producto escalar}\textemdash \hspace{0.5mm} mientras que otros textos utilizan el término \emph{scalar multiplication} para referirse a la multiplicación entre dos elementos del campo (escalares) que da como resultado otro elemento del campo (escalar). Por lo tanto, debemos recordar que el significado de estos términos depende del contexto en que se utilicen. 

\vspace{5mm}
\hspace{2.5mm} Finalmente, remarcamos que en la convención de este texto: 
    \begin{itemize}
        \item Los términos \emph{producto de un vector por un escalar} y \emph{reescalamiento} se emplean para referirnos a la operación realizada entre un vector del conjunto vectorial y un escalar del campo que da como resultado otro vector;
        \item el término \emph{producto escalar} se reserva para la operación realizada entre dos vectores que da como resultado un escalar;
        \item el término \emph{producto entre escalares} se utiliza para referirnos a la multiplicación entre dos escalares del campo que da como resultado otro escalar del campo.
    \end{itemize}





\end{tcolorbox}


\newpage
\section{Combinaciones lineales, subespacio generado y conjunto generador, dependencia e independencia lineal} \label{Sec:3}

\subsection{Combinaciones lineales} \label{Subsec:Combinaciones_lineales}

Como vimos en la sección \ref{Def:Espacio_vectorial}, las dos operaciones \emph{necesarias} para definir un espacio vectorial son la suma vectorial (realizada entre elementos del conjunto vectorial) y el producto de un vector por un escalar (realizada entre un elemento del conjunto vectorial y un elemento del campo), ambas dando como resultado un vector en el mismo espacio. La operación más general que podemos realizar a partir de dichas operaciones se define a continuación.

\begin{tcolorbox}
    \underline{Def.} Sea $V$ sobre $K$ un espacio vectorial y $L=\{\mathbf{v}_1, \mathbf{v}_2, ..., \mathbf{v}_n\}\subset V$ un conjunto con un número finito de vectores de ese espacio. Decimos que $\mathbf{u}$ es una \emph{combinación lineal} de los vectores de $L$ si y sólo si $$\mathbf{u} =c_1\mathbf{v}_1+c_2\mathbf{v}_2+...+c_n\mathbf{v}_n=\sum_{i=1}^n c_i\mathbf{v}_i,\hspace{3mm}c_i\in K, \mathbf{v}_i\in L, n\in \mathbb{N}.$$

    Observemos que, dadas las propiedades de cerradura de ambas operaciones vistas en la sección \ref{Def:Espacio_vectorial}, claramente $\mathbf{u} \in V$, es decir, \emph{el resultado de la combinación lineal de vectores de un espacio vectorial $V$ siempre es un vector del mismo espacio}. 
\end{tcolorbox}

Así, vemos que en un espacio vectorial $V$ arbitrario es posible expresar cualquiera de sus vectores como combinación lineal de otros vectores del mismo espacio. Por ejemplo, en $\mathbb{R}^2$ el vector $$\begin{pmatrix} 1 & 5 \end{pmatrix} = \begin{pmatrix} 1 & 0 \end{pmatrix} + 5\begin{pmatrix} 0 & 1 \end{pmatrix} = 2\begin{pmatrix} 1 & 1.5 \end{pmatrix} + (-0.5)\begin{pmatrix} 2 & -4 \end{pmatrix} $$ $$ = (-4)\begin{pmatrix} 0.5 & -3 \end{pmatrix} + 3\begin{pmatrix} 1 & 1 \end{pmatrix} + (-5)\begin{pmatrix} 0 & 2 \end{pmatrix} = ...$$ \noindent Observamos que, en cada caso, el valor de los coeficientes $c_i\in\mathbb{R}$ depende de los vectores $\mathbf{v}_i\in\mathbb{R}^2$ con los cuales se realiza la combinación lineal. Para dar otro ejemplo, en $P^2$, si definimos los vectores $f(x) = 7x^2 - 5x + 2, g(x) = x^2, h(x) = 9x, i(x)=7, j(x)=x^2 + x + 1$, podemos verificar que $$f(x) = 7g(x)-\frac{5}{9}h(x)+\frac{2}{7}i(x)=7j(x)-\frac{4}{3}h(x)+\frac{1}{7}i(x)=3j(x)+4g(x)+-\frac{8}{9}h(x)-\frac{1}{7}i(x)=...$$

Siguiendo la discusión de la sección \ref{Subsec:Interpretación_geométrica_de_las_operaciones_de_los_espacios_vectoriales}, donde se plantearon por separado las interpretaciones geométricas de las operaciones de suma entre vectores y producto de un vector por un escalar, podemos pensar que, si combinamos ambas operaciones, la operación resultante se puede interpretar como una combinación de flechas rectas reescaladas (y/o rotadas, si el espacio vectorial es complejo y el escalar tiene una parte imaginaria no nula), cada una con su propia longitud, dirección y sentido, a las cuales aplicamos la Ley del paralelogramo para obtener una nueva flecha (o línea) como resultado. Precisamente por ello es que a está operación general se le conoce como \emph{combinación lineal}.

\newpage
\subsection{Subspacio generado y conjunto generador} \label{Subsec:Espacio_generado_y_conjunto_generador}

Sea $V$ sobre $K$ un espacio vectorial y $\mathbf{v}_1, \mathbf{v}_2, ... , \mathbf{v}_n\in V$ vectores. Si tomamos un conjunto de estos vectores, digamos, $G=\{\mathbf{v}_1, \mathbf{v}_2\}$, podemos también definir el conjunto de todos los vectores que se pueden generar a través de combinaciones lineales de los vectores de $G$ como $\{a\mathbf{v}_1+b\mathbf{v}_2\hspace{0.5mm}|\hspace{0.5mm}a,b\in K\}$. Este nuevo conjunto cumple con todas las propiedades de un espacio vectorial, y este hecho es generalizable a cualquier conjunto $G$ con un número finito de elementos, lo cual motiva la definición siguiente. 

\begin{tcolorbox} \label{Def:Espacio_generado_y_conjunto_generador}

    \underline{Def.} Sea $V$ sobre $K$ un espacio vectorial y $G\subset V$. Entonces, definimos $$\langle G\rangle \equiv \{c_1\mathbf{v}_1+c_2\mathbf{v}_2+...+c_n\mathbf{v}_n\hspace{0.5mm}|\hspace{0.5mm}c_i\in K, \mathbf{v}_i\in G\}\subseteq V.$$ A $G$ se le conoce como el \emph{conjunto generador} y a $\langle G \rangle$, como el \emph{subespacio generado} por $G$ ya que, por definición, cumple con todas las propiedades de un espacio vectorial y además, por cerradura de $V$, cualquier vector generado por $G$ es elemento de $V$\footnote{Nótese por la definición que, dependiendo de cómo sea el conjunto $G$, es posible que $\langle G \rangle =V$.}. Por completez, definimos $\langle \emptyset \rangle = \{\mathbf{0}\}.$ Si $G$ consiste de un sólo vector $\mathbf{g}$, podemos escribir el generado de $G$ como $\langle \mathbf{g} \rangle$.

\end{tcolorbox}

Para dar algunos ejemplos, si elegimos cualquier vector $\mathbf{v}\in\mathbb{R}^2$, entonces el subespacio generado correspondiente $\langle \mathbf{v} \rangle = \{c\mathbf{v}\hspace{0.5mm}|\hspace{0.5mm}c\in \mathbb{R}\}$ se puede interpretar geométricamente en el plano cartesiano como el conjunto de todas las flechas posibles de obtener a partir de reescalamientos de $\mathbf{v}$. Por otro lado, si en $\mathbb{R}^3$ definimos a $N=\{\begin{pmatrix} 1 & 0 & 0 \end{pmatrix}, \begin{pmatrix} 0 & 1 & 0 \end{pmatrix}, \begin{pmatrix} 0 & 0 & 1 \end{pmatrix}\}$ entonces vemos que $$\langle N \rangle = \{c_1\begin{pmatrix} 1 & 0 & 0 \end{pmatrix} + c_2\begin{pmatrix} 0 & 1 & 0 \end{pmatrix} + c_3\begin{pmatrix} 0 & 0 & 1 \end{pmatrix}\hspace{0.5mm}|\hspace{0.5mm}c_1,c_2,c_3\in\mathbb{R}\},$$ pero esto es equivalente a la definición $\mathbb{R}^3 = \{\begin{pmatrix} c_1 & c_2 & c_3\end{pmatrix} \hspace{0.5mm}|\hspace{0.5mm}c_1,c_2,c_3\in\mathbb{R}\}$; es decir, en este caso \emph{el espacio generado por los vectores de $N$ es igual a $\mathbb{R}^3$}, i.e., $\langle N \rangle =\mathbb{R}^3$.

A continuación, veremos un teorema que será de gran importancia en las secciones posteriores.

\begin{teorema} {3.2.1} 
Sea $V$ un espacio vectorial, $S\subset V$ un conjunto de vectores de $V$ y $\mathbf{v}\in V$ un vector arbitrario. Si $S'=S\cup\{\mathbf{v}\}$, entonces $\langle S \rangle = \langle S' \rangle \iff v\in\langle S \rangle.$

\begin{proof}
Ya que $\mathbf{v}\in S'$ entonces trivialmente se cumple que $\mathbf{v}\in\langle S'\rangle;$ por lo tanto, si $\mathbf{v}\notin \langle S \rangle \implies \langle S \rangle \neq \langle S' \rangle.$ Por otro lado, si $\mathbf{v}\in\langle S \rangle$ entonces $S'\subset\langle S \rangle$, lo cual implica que $\langle S' \rangle \subset \langle S \rangle.$ Además, ya que $S\subset S'$, entonces trivialmente se cumple que $\langle S \rangle \subset \langle S' \rangle.$ En conclusión, $\langle S \rangle =\langle S' \rangle.$
\end{proof}
        
    Este teorema nos dice que agregar un vector a un conjunto generador no necesariamente cambiará el subespacio generado por ese conjunto generador. Para que este cambio realmente suceda, el vector añadido debe ser en algún sentido \emph{ajeno} a los del conjunto generador original. En la siguiente sección, veremos algunas definiciones necesarias para precisar esta idea.

\end{teorema}

\newpage
\subsection{Dependencia e independencia lineal} \label{Subsec:Dependencia_e_independencia_lineal}

Como se vio en la sección \ref{Subsec:Combinaciones_lineales}, un vector puede ser expresado como diferentes combinaciones lineales de otros vectores del mismo espacio. En particular, el vector nulo $\mathbf{0}$ de cualquier espacio vectorial $V$ puede ser obtenido a través de la \emph{combinación lineal trivial} de cualesquiera $n$ vectores del espacio: sólo basta con que todos los coeficientes sean cero, i.e. $$0\mathbf{v}_1+0\mathbf{v}_2+...+0\mathbf{v}_n=\mathbf{0}, \hspace{3mm}\forall\hspace{1.5mm}\mathbf{v}_1 \mathbf{v}_2, ...,\mathbf{v}_n \in V.$$ Sin embargo, también pueden existir combinaciones lineales entre $n$ vectores de un espacio vectorial $V$ que den como resultado $\mathbf{0}$ pero sean no triviales (es decir, tengan coeficientes distintos de cero), e.g., en $\mathbb{R}^2, 7\begin{pmatrix} 1 & 1 \end{pmatrix}+5\begin{pmatrix} -1 & 1 \end{pmatrix}+2\begin{pmatrix} -1 & -6 \end{pmatrix}=\begin{pmatrix} 0 & 0 \end{pmatrix}=\mathbf{0}$. Una consecuencia de este hecho es que podamos despejar a cualquiera de los vectores y expresarlo como combinación lineal de los demás; por ejemplo, $\begin{pmatrix} 1 & 1 \end{pmatrix}=-\frac{5}{7}\begin{pmatrix} -1 & 1 \end{pmatrix}-\frac{2}{7}\begin{pmatrix} -1 & -6 \end{pmatrix}$, ó $\begin{pmatrix} -1 & -6 \end{pmatrix} = -\frac{7}{2}\begin{pmatrix} 1 & 1 \end{pmatrix}-\frac{5}{2}\begin{pmatrix} -1 & 1 \end{pmatrix},$ etc. Este importante hecho motiva la siguiente definición.

\subsubsection{Definición de dependencia e independencia lineal} \label{Def:Dependencia_e_independencia_lineal}

\begin{tcolorbox}

    \underline{Def.} Sea $V$ un espacio vectorial y $\mathbf{v}_1,\mathbf{v}_2,...,\mathbf{v}_n\in V$. Decimos que los vectores $\mathbf{v}_1,\mathbf{v}_2, ...,\mathbf{v}_n$ son \emph{linealmente independientes} entre sí si la única combinación lineal de ellos que da como resultado el vector nulo es la trivial (i.e., en la cual todos los coeficientes son cero). Matemáticamente, $$\mathbf{v}_1,\mathbf{v}_2, ..., \mathbf{v}_n \hspace{1.5mm} \text{son} \hspace{1.5mm} l.i. \iff c_1\mathbf{v}_1+c_2\mathbf{v}_2+...+c_n\mathbf{v}_n=\mathbf{0}\implies c_1,c_2, ...,c_n=0.$$

    En cambio, decimos que $\mathbf{v}_1,\mathbf{v}_2, ..., \mathbf{v}_n$ son \emph{linealmente dependientes} si existe al menos una combinación lineal no trivial que dé como resultado el vector nulo o, equivalentemente, si cualquiera de los vectores $\mathbf{v}_i$ puede ser expresado como una combinación lineal de los demás\footnote{El vector nulo se excluye de estas definiciones ya que, de lo contrario, cumpliría ambas trivialmente para cualquier conjunto arbitrario de vectores dado.}.

    Si todos los vectores de un conjunto $S$ son linealmente dependientes (independientes) entre sí, se dice que el conjunto $S$ es linealmente dependiente (independiente).

\end{tcolorbox}

Por ejemplo, en $\mathbb{R}^2$ los vectores $\mathbf{u}_1=\begin{pmatrix} 1 & 5 \end{pmatrix}$ y $\mathbf{u}_2=\begin{pmatrix} -3 & -15 \end{pmatrix}$ son linealmente dependientes, ya que $\mathbf{u}_2=-3\mathbf{u}_1$, por lo cual $3\mathbf{u}_1+\mathbf{u}_2=\mathbf{0}$; por otro lado, los vectores $\mathbf{v}_1=\begin{pmatrix} 1 & 2 \end{pmatrix}$ y $\mathbf{v}_2=\begin{pmatrix} 1 & 3 \end{pmatrix}$ son linealmente independientes, ya que no existe un número $c\in\mathbb{R}$ tal que $\mathbf{v}_1=c\mathbf{v}_2.$. Notemos que, como nuestros vectores en este caso son $2-$tuplas, las ecuaciones $\mathbf{u}_2=-3\mathbf{u}_1$ y $\mathbf{v}_1=c\mathbf{v}_2$ son en realidad la notación compactada de un \emph{sistema de ecuaciones}, con una ecuación por cada entrada del vector. En particular, la ecuación $\mathbf{v}_1=c\mathbf{v}_2$ puede ser reescrita como $$\begin{pmatrix} 1 & 2 \end{pmatrix}=c\begin{pmatrix} 1 & 3 \end{pmatrix}\iff 1=c1 \hspace{1.5mm} \land\hspace{1.5mm} 2=c3,$$ de donde vemos directamente que no existe solución para $c$, por lo cual estos vectores son linealmente independientes.

    En general, cuando expresamos combinaciones lineales del tipo $\mathbf{v}=c_1\mathbf{u}_1+...+c_n\mathbf{u}_n$ en donde los vectores son dados pero los coeficientes son desconocidos, éstos útlimos se vuelven las \emph{incógnitas} del \emph{sistema de ecuaciones algebráicas} dado por la ecuación $\mathbf{v}=c_1\mathbf{u}_1+...+c_n\mathbf{u}_2$. El número de ecuaciones del sistema dependerá de la naturaleza de los vectores, mientras que el número de incógnitas es igual al número de coeficientes desconocidos. Por lo tanto, la pregunta de si un vector es linealmente independiente o dependiente de otro(s) se reduce a la de si el sistema de ecuaciones asociado a la combinación lineal de ellos tiene solución o no.

Para ver más ejemplos de conjuntos de vectores linealmente dependientes e independientes pueden consultar, e.g., el Friedberg (págs. 36-38), el Lang introductorio (págs. 104-109), etc. 

\subsubsection{Interpretación geométrica de la dependencia e independencia lineal}

Como ya mencionamos, si un vector $\mathbf{v}_n\in V$ es linealmente \emph{dependiente} de otros vectores $\mathbf{v}_1, \mathbf{v}_2, ..., \textbf{v}_m\in V$, entonces puede ser expresado como combinación lineal de esos vectores. Geométricamente, en los espacios vectoriales reales $\mathbb{R}^2$ y $\mathbb{R}^3$ esto quiere decir que es posible reescalar y combinar (mediante la Ley del paralelogramo) las flechas de los vectores $\mathbf{v}_1, \mathbf{v}_2, ..., \mathbf{v}_m$ y obtener, como resultado final, a $\mathbf{v}_n$. Adicionalmente, en el espacio vectorial complejo $\mathbb{C}$, también se podrían estar rotando los vectores $\mathbf{v}_1, \mathbf{v}_2, ..., \mathbf{v}_m$ \textemdash además de reescalarlos y combinarlos\textemdash \hspace{0.5mm} para formar, finalmente, a $\mathbf{v}_n$. Si son linealmente \emph{independientes}, entonces lo anterior no es posible.

\subsubsection{Algunos teoremas sobre dependencia e independencia lineal} \label{Teo:Dependencia_e_independencia_lineal} 

\begin{teorema} {3.3.3.1} 

    Sea $V$ un espacio vectorial y $\mathbf{v_1}, \mathbf{v_2}, ..., \mathbf{v}_n$ vectores linealmente independientes de $V$. Sean $c_1, c_2, ..., c_n\in K$ y $d_1, d_2, ..., d_n\in K$ tales que $$c_1\mathbf{v}_1+c_2\mathbf{v}_2+...+c_n\mathbf{v}_n=d_1\mathbf{v}_1+d_2\mathbf{v}_2+...+d_n\mathbf{v}_n,$$ entonces se tiene que $c_i=d_1\hspace{3mm}\forall\hspace{1.5mm}i\in\{1,2, ..., n\}$.

    \begin{proof}

        $$c_1\mathbf{v_1}+c_2\mathbf{v}_2+...+c_n\mathbf{v}_n=d_1\mathbf{v}_1+d_2\mathbf{v}_2+...+d_n\mathbf{v}_n\iff(c_1-d_1)\mathbf{v}_1+(c_2-d_2)\mathbf{v}_2+...+(c_n-d_n)\mathbf{v}_n=\mathbf{0}.$$ Pero, ya que por hipótesis estos vectores son linealmente independientes, entonces por definición $$c_i-d_i=0\hspace{3mm}\forall\hspace{1.5mm}i\in\{1,2, ...,n\}\iff c_i=d_i\hspace{3mm}\forall\hspace{1.5mm}i\in\{1,2, ...,n\}.$$

    \end{proof}

    Este teorema significa que si un vector es resultado de una combinación lineal de vectores linealmente independientes, entonces esa combinación lineal es \emph{la única} que da como resultado a ese vector. Es decir, que si $\mathbf{u}=c_1\mathbf{v}_1+c_2\mathbf{v}_2+...+c_n\mathbf{v}_n$ y $\{\mathbf{v}_1,\mathbf{v}_2, ...,\mathbf{v}_n\}\hspace{1.5mm}\text{es}\hspace{1.5mm} l.i.$ entonces no existe otra combinación de escalaras y vectores $c_i \mathbf{v}_i$ tal que la suma de todos ellos dé $\mathbf{u}$.

\end{teorema}

\begin{teorema} {3.3.3.2} 

    Sea $V$ un espacio vectorial y $S_1,S_2$ subespacios tales que $S_1\subseteq S_2\subseteq V$. Si $S_2$ es linealmente independiente, entonces $S_1$ también es linealmente independiente.

\begin{proof}

    Sean $S_1=\{\mathbf{v}_1,\mathbf{v}_2, ..., \mathbf{v}_k\}$ y $S_2=\{\mathbf{v}_1,\mathbf{v}_2, ..., \mathbf{v}_n\}$ con $k\leq n$. Ya que por hipótesis $S_2$ es $l.i.$, por definición se cumple que $$c_1\mathbf{v}_1+c_2\mathbf{v}_2+...+c_n\mathbf{v}_n=\mathbf{0}\iff c_i=0\hspace{3mm}\forall\hspace{1.5mm}  i\in\{1,2, ..., n\}.$$ Si $k=n$ entonces $S_1$ también es l.i. trivialmente. Supongamos que $k<n$. Entonces, por el Teorema 1.2.3.2 (ver sec. \ref{Teo:Espacios_vectoriales}), $0\mathbf{v}_{k+1}+...+0\mathbf{v}_n=\mathbf{0}$ por lo cual lo anterior implica que $$c_1\mathbf{v}_1+c_2\mathbf{v}_2+...+c_k\mathbf{v}_k=\mathbf{0}\iff c_i=0\hspace{3mm}\forall\hspace{1.5mm}  i\in\{1,2, ...,k\}.$$ Por lo tanto, por definición, $S_1$ también es $l.i$.

\end{proof}

Este teorema nos dice que si removemos un vector de un conjunto linealmente independiente, el conjunto resultante también es linealmente independiente.

\end{teorema}

\begin{teorema} {3.3.3.3}

Sea $V$ un espacio vectorial sobre un campo $K$ y $L\subset V$ un conjunto con $n$ elementos linealmente independientes entre sí. Entonces, para cualquier $\mathbf{v}\in V$, el conjunto $L'\equiv L\cup \{\mathbf{v}\}$ es $l.i. \iff \mathbf{v}\notin \langle L \rangle$.

\begin{proof}
Sea $L=\{\mathbf{u}_1, ... , \mathbf{u}_n\}.$ Supongamos que $\mathbf{v}\in\langle L \rangle$, entonces existen coeficientes $c_i\in K$ tales que $\mathbf{v}=c_1\mathbf{u}_1+...+c_n\mathbf{v}_n.$ Despejando esta ecuación, obtenemos que $c_1\mathbf{u}_1+...+c_n\mathbf{u}_n+(-1)\mathbf{v}=\mathbf{0},$ es decir, que existe una combinación lineal no trivial entre los vectores de $L'$ que dan como resultado al vector nulo, por lo cual $L'$ es un conjunto linealmente dependiente.

Por otro lado, supongamos que $L'$ es linealmente dependiente. Entonces, existe una combinación lineal no trivial de los vectores de $L'$ que resulta en el vector nulo, i.e., $c_1\mathbf{u}_2+...+c_n\mathbf{u}_n+b\mathbf{v}=\mathbf{0}$ con al menos un coeficiente distinto de cero. En este caso, el coeficiente $b\neq 0$: si $b$ fuera igual a cero, la ecuación restante sería $c_1\mathbf{u}_1+...+c_n\mathbf{u}_n=\mathbf{0}$; ya que $L$ es linealmente independiente, entonces todos los vectores $c_i$ deben ser iguales a cero pero, ya que estamos suponiendo que también $b=0$, entonces el conjunto $L'$ también sería linealmente independiente, contradiciendo la hipótesis. Así, sabiendo que $b\neq 0$ podemos despejar la ecuación $c_1\mathbf{u}_1+...+c_n\mathbf{u}_n+b\mathbf{v}=\mathbf{0}$ y obtener que $\mathbf{v}=\frac{-c_1}{b}\mathbf{u}_1+...+\frac{-c_n}{b}\mathbf{u}_n$, lo cual implica que $\mathbf{v}\in\langle L \rangle.$

    Por contraposición, tenemos que $L'\equiv L\cup\{\mathbf{v}\}\hspace{1.5mm}\text{es}\hspace{1.5mm} l.i.\iff \mathbf{v}\notin \langle L \rangle.$

\end{proof}

    La demostración de este teorema básicamente nos dice que si tenemos un conjunto linealmente independiente y agregamos a un vector de su subespacio generado a este conjunto, entonces se vuelve linealmente dependiente. En contraposición, concluimos que en cualquier conjunto linealmente dependiente existe una especie de \emph{redundancia} entre sus elementos, ya que se puede remover a cualquiera de ellos sin alterar el subespacio generado por este conjunto. En cambio, remover un vector de un conjunto linealmente independiente \emph{sí} altera el subespacio generado por ese conjunto.
\end{teorema}


\subsection{Ejercicios de repaso}

\subsubsection{Combinaciones lineales}

\begin{enumerate}
    \item Demuestra que si tenemos tres vectores $\mathbf{u}, \mathbf{v}$ y $\mathbf{w}$ no nulos y ortogonales entre sí, entonces no puede haber una combinación lineal de dos de ellos que dé como resultado el tercero (Nota: repasa la definición de ortogonalidad y las propiedades del producto escalar vistas en la sec. \ref{Sec:2}.) 
    \item Da un ejemplo de la demostración anterior en $\mathbb{R}^3$, e interpreta este resultado geométricamente (Nota: recuerda la relación entre el producto escalar y las proyecciones.) 
\end{enumerate}

\subsubsection{Subespacio generado y conjunto generador}

\begin{enumerate}
    \item Sea $a\in\mathbb{R}$ un vector arbitrario del espacio vectorial real $\mathbb{R}$. Demuestra que este espacio vectorial sólo tiene subespacios vectoriales triviales. ¿Quiénes son los valores $a\in\mathbb{R}$ que generan a estos subespacios, en cada caso? 
    \hypertarget{Ejer:3.4.2.2}{} \item Sea $G=\{\begin{pmatrix} 3 & 0 & 3 \end{pmatrix}, \begin{pmatrix} -\frac{1}{2} & 0 & \frac{1}{2} \end{pmatrix}\}\subset\mathbb{R}^3.$ Escribe a $\langle G \rangle$ algebráicamente y descríbelo geométricamente. ¿A qué espacio vectorial real que conoces se parece? 
    \item Sea $\mathbf{c}$ un vector arbitrario del espacio vectorial complejo $\mathbb{C}$. Da una interpretación geométrica para $\langle \mathbf{c} \rangle$ (Nota: recuerda que, en este caso, $K=\mathbb{C}$; además, te sugiero que primero escribas a $\langle \mathbf{c}\rangle$ algebráicamente y, a partir de ahí, busques interpretarlo geométricamente.). 
    \item Sea $F=\{x^0,x^1,x^2,...,x^n\}$ con $n\in\mathbb{N}$ un conjunto de funciones reales de una variable real. ¿Quién es, entonces, $\langle F \rangle$? 
\end{enumerate}

\subsubsection{Dependencia e independencia lineal}

\begin{enumerate}
    \item Sea $n_i$ el $i$-ésimo dígito de tu número de cuenta. Determina si los vectores $\begin{pmatrix} n_1 + i(n_2) \end{pmatrix}$ y $\begin{pmatrix} n_3 + i(n_4) \end{pmatrix} \in \mathbb{C}$ son linealmente dependientes o independientes y muéstralo gráficamente en el plano complejo. 
    \item Repite el mismo ejercicio para los vectores reales $\begin{pmatrix} n_1 & n_4 & n_7 \end{pmatrix}, \begin{pmatrix} n_2 & n_5 & n_8 \end{pmatrix}$ y $\begin{pmatrix} n_3 & n_6 & n_9 \end{pmatrix}\in\mathbb{R}^3.$ ¿Cuál es el conjunto linealmente dependiente más grande que puedes armar con estos vectores, sin considerar los subespacios generados por éstos? ¿Qué hay del conjunto linealmente independiente más grande? 
    \item Sea $V$ un espacio vectorial y $S_1\subseteq S_2\subseteq V$. Demuestra que si $S_1$ es linealmente dependiente, entonces $S_2$ es linealmente dependiente. 
    \item Sean las funciones $p_1(x) = 3x^2-x, p_2(x) = x^3+5, p_3(x)=4x+1, p_4(x)=4x^3+4x+16$ vectores de $P^4$, el espacio vectorial real de funciones polinomiales de grado $n\le 4.$ ¿Cuáles son los tres conjuntos de tres vectores linealmente independientes que podemos formar a partir de estos cuatro vectores?  
    \item Da un ejemplo de un conjunto de tres vectores en $\mathbb{R}^3$ linealmente independientes y ortogonales entre sí, donde todas las entradas de dichos vectores sean distintas de cero.
    \item Describe con tus palabras y con los conceptos vistos en clase el procedimiento que seguiste para resolver el ejercicio 3.4.3.5.
    
\end{enumerate}


\newpage
\section{Bases, dimensión, ortogonalización y ortonormalización} \label{Sec:4}

\subsection{Bases}

Como hemos visto en secciones anteriores, cualquier vector de un espacio vectorial se puede expresar como combinación lineal de otros vectores de ese mismo espacio\footnote{De lo contrario, se violarían las propiedades de cerradura vistas en la sec. \ref{Def:Espacio_vectorial}.}. Cuando trabajamos en un espacio vectorial $V$, resulta conveniente tener un conjunto de vectores $B\subset V$ con el cual se pueda expresar a \emph{cualquier vector del espacio vectorial} $V$ de forma \emph{única} \textemdash lo cual se logra, precisamente, a través de una combinación lineal única de los vectores del conjunto $B$. Tomando en cuenta el Teorema 3.3.3.1 (sec. \ref{Teo:Dependencia_e_independencia_lineal}), vemos que los conjuntos linealmente independientes son buenos candidatos para lograr que las expresiones mediante combinaciones lineales sean \emph{únicas}, por lo cual pediremos que $B$ sea linealmente independiente; además, ya que queremos ser capaces de expresar a \emph{cualquier} vector arbitrario de $V$ como combinación lineal \emph{única} de los vectores de $B$, sería necesario que el conjunto linealmente independiente $B$ generara a \emph{todos} los vectores de $V$. A cualquier conjunto que cumpla ambas propiedades se le conoce como una \emph{base} para el espacio vectorial en cuestión.

\subsubsection{Definición de base} \label{Def:Base}

\begin{tcolorbox}

    \underline{Def.} Una base de un espacio vectorial $V$ es un conjunto de vectores linealmente independientes que generan a todo el espacio vectorial $V$. En lenguaje matemático, $$B\subset V \hspace{1.5mm}\text{es una base de}\hspace{1.5mm} V \iff B\hspace{1.5mm}\text{es}\hspace{1.5mm} l.i.  \hspace{1.5mm}\text{y}\hspace{1.5mm} \langle B \rangle=V.$$

\end{tcolorbox}

Nótese por la definición que, ya que muchos conjuntos de vectores distintos pueden ser linealmente independientes y generar a un mismo espacio vectorial, un espacio vectorial puede tener muchas bases distintas. Esto implica que cualquier vector arbitrario de un espacio vectorial puede ser expresado a través de diferentes combinaciones lineales (correspondientes a distintas bases del espacio, y únicas para cada base). Dicho de otra forma, dado un espacio vectorial con más de una base, cualquier vector de ese espacio puede ser \emph{representado en las distintas bases} de ese espacio\footnote{El tema de las \emph{representaciones} es de gran interés en algunas ramas de las matemáticas y sus aplicaciones son de suma importancia en varias áreas de la física. En este curso, lo veremos sobre todo en las secciones de representación matricial de una transformación lineal, representación de una matriz en distintas bases y representaciones de un operador lineal en distintos espacios vectoriales.}.

\subsubsection{Ejemplos de bases} \label{Ejem:Bases}

El conjunto $\{1\}$ es una base para el espacio vectorial complejo $\mathbb{C}.$ De hecho, si cambiamos a $1$ en el conjunto anterior por cualquier número complejo no nulo, también tendremos una base para el espacio complejo $\mathbb{C}$ (¿a qué propiedades se debe esto?). 
\vspace{3mm}

Los conjuntos $\{\begin{pmatrix} 2 & 0 \end{pmatrix}, \begin{pmatrix} 0, & -3 \end{pmatrix}\}, \{\begin{pmatrix} 3 & 3 \end{pmatrix}, \begin{pmatrix} -3 & 3 \end{pmatrix}\}$ y $\{\begin{pmatrix} 1 & 0 \end{pmatrix},\begin{pmatrix} 0 & 1 \end{pmatrix}\}$ son bases de $\mathbb{R}^2$.
\vspace{3mm}

Cualquier conjunto de la forma $\{c_n x^n\hspace{0.5mm}|\hspace{0.5mm}n\in\mathbb{N}\cup\{0\}, c_n\in \mathbb{R}\}$ es una base del espacio vectorial de las funciones polinomiales de grado $n$.

\subsubsection{Teorema de reemplazamiento} \label{Subsubsec:Teo_de_reemplazamiento}

A continuación, veremos un importante teorema que nos ayudará a construir bases más adelante.

\begin{teorema} {4.1.1}
    Sea $V$ un espacio vectorial generado por un conjunto $G$ con $n$ vectores, y sea $L$ un subconjunto linealmente independiente de $V$ con $m$ vectores. Entonces $m\le n$ y existe un subconjunto $H\subseteq G$ que contiene $n-m$ vectores tal que $L\cup H$ genera a $V$.

    \begin{proof}
        Esta demostración se hará por inducción.

    \vspace{3mm} 
    \textbf{Base inductiva}
    Sea $m=0$, entonces $L=\emptyset$. Si tomamos $H=G$ obtenemos el resultado deseado.

    \vspace{3mm} 
    \textbf{Hipótesis de inducción}
    Supongamos que la hipótesis del teorema se cumple para $L=\{\mathbf{v}_1,...,\mathbf{v}_m\}$ con $m>0$.

    \vspace{3mm} 
    \textbf{Paso inductivo}
    Ahora debemos demostrar que, bajo la hipótesis de inducción (donde el teorema se cumple para alguna $m>0$), el teorema se debe cumplir para $m+1$.\vspace{3mm}
    
     Sea $L=\{\mathbf{v}_1,...,\mathbf{v}_{m+1}\}$ un subconjunto linealmente independiente de $V$. Por el Teorema 3.3.3.2, el conjunto $\{\mathbf{v}_1,...,\mathbf{v}_m\}$ es l.i., por lo cual podemos aplicar la hipótesis de inducción y concluir que $m\le n$ y que existe un subconjunto $\{\mathbf{u}_1,...,\mathbf{u}_{n-m}\}\subset G$ tal que $\{\mathbf{v}_1,...,\mathbf{v}_m\}\cup\{\mathbf{u}_1,...,\mathbf{u}_{n-m}\}$ genera a $V$. Por lo tanto, existen escalares $a_1,...,a_m,b_1,...,b_{n-m}$ tales que \[
        a_1\mathbf{v}_1+...+a_m\mathbf{v}_m+b_1\mathbf{u}_1+...+b_{n-m}\mathbf{u}_{n-m}=\mathbf{v}_{m+1}.
    .\] 

    Observemos que, ya que $L$ es linealmente independiente, $n-m>0\implies n>m\implies n\ge m+1$. Además, alguna $b_i$ debe ser distinta de cero, por lo cual podemos despejarla (de lo contrario, estaríamos contradiciendo la hipótesis de inducción, que nos asegura que $\{\mathbf{v}_1,...,\mathbf{v}_m\}$ es l.i.). Suponiendo, por ejemplo, que $b_1\neq 0$, tenemos que \[
        \mathbf{u}_1=\frac{-a_1}{b_1}\mathbf{v}_1+...+\frac{-a_m}{b_1}\mathbf{v}_m+\frac{-b_2}{b_1}\mathbf{u}_2+...+\frac{-b_{n-m}}{b_1}\mathbf{u}_{n-m}
    .\] 

    Por lo cual $\mathbf{u}_1$ puede ser expresado como combinación lineal de los vectores $\mathbf{v}_1,...,\mathbf{v}_m,\mathbf{u}_2,...,\mathbf{u}_{n-m}.$ Sea $H=\{\mathbf{u}_2,...,\mathbf{u}_{n-m}\}$, entonces $L\cup H=\{\mathbf{v}_1,...,\mathbf{v}_{m+1},\mathbf{u}_2,...,\mathbf{u}_{n-m}\}$ y trivialmente tenemos que $\mathbf{v}_1,...,\mathbf{v}_m,\mathbf{u}_2,...,\mathbf{u}_{n-m}\in\langle L\cup H\rangle$ \textemdash lo cual también implica que $\mathbf{u}_1\in\langle L\cup H\rangle$. Por lo tanto, tenemos que $\{\mathbf{v}_1,...,\mathbf{v}_m,\mathbf{u}_1,...,\mathbf{u}_{n-m}\}\subseteq\langle L\cup H\rangle.$

    Recordando que por hipótesis de inducción $\{\mathbf{v}_1,...,\mathbf{v}_m,\mathbf{u}_1,...,\mathbf{u}_{n-m}\}$ genera a $V$, entonces el hecho de que esté contenido en $L\cup H$ implica necesariamente que $\langle L\cup H\rangle=V.$ Finalmente, ya que $H$ es un subconjunto de $G$ con $(n-m)-1=n-(m+1)$ vectores, el teorema se cumple para $m+1$, terminando así nuestra demostración.

    \end{proof}
\end{teorema}

El teorema anterior se conoce como el teorema de \emph{reemplazamiento} ya que, partiendo de un conjunto linealmente independiente $L$ y otro conjunto $H$ que juntos cumplen $\langle L\cup H \rangle=V$ (sin que $L\cup H$ sea necesariamente l.i.), lo que estamos haciendo con cada paso consecutivo de la inducción es reemplazar a los vectores de $H$ por vectores que podemos añadir a $L$ tal que este conjunto siga siendo linealmente independiente y se siga cumpliendo que la unión de ambos genere a $V$. De esta forma, $L$ es un conjunto linealmente independiente que va creciendo y que cada vez necesita a menos vectores de $H$ para poder, a través de la unión generar a $V$. ¿Qué pasará cuando $L$ sea un conjunto linealmente independiente que no necesita a ningún vector de $H$ para generar a $V$\footnote{Recuerda para qué dijimos que nos serviría este teorema.}?. 

\newpage
\subsection{Dimensión} \label{Subsec:Dimensión}

Como quizá notaste en los ejemplos de la sección anterior, pareciera que todas las bases de un mismo espacio vectorial tienen el mismo número de elementos. A continuación, demostraremos este hecho.

\begin{teorema} {4.2.1} 

    Sean $B=\{\mathbf{b}_1,\mathbf{b}_2, ..., \mathbf{b}_n\}$ y $B'=\{\mathbf{b'}_1,\mathbf{b'}_2, ..., \mathbf{b'}_{n'}\}$ bases de $V$, entonces $n=n'$.

\begin{proof}

    Supongamos que $n'>n$. Ya que $B'\subset V$ y $\langle B \rangle =V\implies B'\subset\langle B \rangle,$ por lo cual podemos expresar cualquier vector de $B'$ como combinación lineal de los de $B$. Entonces, 

    $$\mathbf{b'}_1=c_{11}\mathbf{b}_1+c_{12}\mathbf{b}_2+...+c_{1n}\mathbf{b}_{n},$$

    $$...$$

    $$\mathbf{b'}_{n'}=c_{n'1}\mathbf{b}_1+c_{n'2}\mathbf{b}_2+...+c_{n'n}\mathbf{b}_n,$$ \noindent donde $c_{ij}\in K$.

    Sea $\mathbf{z}\in V$ un vector arbitrario. Como $B'$ es base de $V\implies \mathbf{z}=d_1\mathbf{b'}_1+d_2\mathbf{b'}_2+...+d_{n'}\mathbf{b}_{n'}$. Sustituyendo con las ecuaciones obtenemos que $$\mathbf{z}=d_1(c_{11}\mathbf{b}_1+...+c_{1n}\mathbf{b}_n)+...+d_{n'}(c_{n'1}\mathbf{b}_1+...+c_{n'n}\mathbf{b}_n)=(d_1 c_{11}+...+d_{n'} c_{n'1})\mathbf{b}_1+...+(d_1 c_{1n}+...+d_{n'} c_{n'n})\mathbf{b}_n.$$ \noindent En particular, si $\mathbf{z}=\mathbf{0}$, ya que por hipótesis $B$ es linealmente independiente, obtenemos

    $$d_1 c_{11}+...+d_{n'}c_{n'1}=0,$$


    $$...$$ 

    $$d_1 c_{1n}+...+d_{n'} c_{n'n}=0.$$

    Sin embargo, ya que al inicio de la demostración supusimos que $n'>n$, entonces el sistema de ecuaciones anterior tiene más incógnitas que ecuaciones y, por ende, una solución no trivial para $(d_1, d_2, ..., d_{n'})$. Esto contradice el hecho de que $B$ sea una linealmente independiente, por lo cual tampoco podría ser una base. Análogamente, si $n>n'$ se llega a una contradicción similar. Por lo tanto, por tricotomía concluimos que, si $B$ y $B'$ son bases, $n=n'$.
\end{proof}

\end{teorema}

El hecho de que todas las bases de un mismo espacio vectorial tengan el mismo número de elementos motiva la siguiente definición.

\begin{tcolorbox}

    \underline{Def.} La \emph{dimensión} de un espacio vectorial $V$ es igual al número de elementos (i.e., la cardinalidad) de cualquiera de sus bases. Si cualquier base de $V$ tiene un número finito $n$ de elementos, decimos que $V$ es un \emph{espacio de dimensión finita} y escribimos esto como $\text{dim}(V)=n$; de lo contrario decimos que $V$ es un espacio de dimensión \emph{infinita}\footnote{En el resto de estas notas, supondremos que los espacios vectoriales mencionados tienen dimensión finita, a menos que se indique lo contrario.}.

\end{tcolorbox}

Observemos que esta definición \emph{algebráica} de dimensión difiere de las definiciones geométricas y físicas usuales de dimensión. Por ejemplo, a pesar de que el espacio vectorial complejo $\mathbb{C}$ se represente en el plano cartesiano \textemdash el cual tiene dimensión geométrica $2$\textemdash\hspace{0.5mm}, este espacio vectorial es de dimensión (algebráica) $1$, como vimos en los ejemplos de la sección \ref{Ejem:Bases}. Otra observación es que la dimensión de un espacio vectorial no sólo depende del conjunto vectorial, sino también del campo sobre el cual se define (por ejemplo, el espacio vectorial $\mathbb{C}$ sobre $\mathbb{R}$ no puede tener dimensión $1$: ¿podrías demostrarlo?).

\begin{teorema} {4.2.2}
    Sea $V$ un espacio vectorial de dimensión finita y $W$ un subespacio de $V$, entonces W tiene dimensión finita y $\text{dim}(W)\le \text{dim}(V).$ 

\begin{proof}

    Sea $\text{dim}(V)=n.$ Si $W=\{\mathbf{0}\} \implies \text{dim}(W)=0\le n.$ Consideremos ahora que $W$ contiene a un vector no nulo $\mathbf{x}_1$, entonces el conjunto $\{\mathbf{x}_1\}$ es linealmente independiente. Supongamos que seguimos agregando más vectores $\mathbf{x}_2,...,\mathbf{x}_k$ de $W$ al conjunto $\{\mathbf{x}_1\} $ de tal forma que $\{\mathbf{x}_1,\mathbf{x}_2,...,\mathbf{x}_k\}$ sea linealmente independiente. Ya que $\text{dim}(V)=n,$ entonces cualquier base de $V$ tiene $n$ elementos. Esto implica que ningún subconjunto de $V$ linealmente independiente puede tener más de $n$ elementos, por lo cual el proceso anterior debe detenerse para algún $k\le n.$ De acuerdo al Teorema 3.3.3.3, este conjunto genera a $W$, por lo cual forma una base de $W$, de donde concluimos que $\text{dim}(W)=k\le n.$

\end{proof}

\end{teorema}

En los teoremas anteriores demostramos que si $\text{dim}(V)=n$ entonces cualquer base de $V$ tiene precisamente $n$ elementos, y que cualquier subespacio vectorial tiene dimensión finita. Resulta, además, que en este caso cualquier conjunto de $n$ vectores linealmente independientes de $V$ es también una base para $V$\textemdash es decir, que también genera a todo el espacio $V$, como veremos en el siguiente teorema. 

\begin{teorema} {4.2.3}

    Sea $V$ un espacio vectorial. Si $\text{dim}(V)=n$ entonces cualquier conjunto de $n$ vectores linealmente independientes de $V$ es una base de $V$.

\begin{proof}

    Esta prueba se hará por contradicción. 

    Sea $V$ un espacio vectorial con $\text{dim}(V)=n, \hspace{1.5mm} n\in\mathbb{N}$ y $B=\{\mathbf{b}_1, ..., \mathbf{b}_n\}\subset V$ un conjunto de $n$ vectores de $V$ que son linealmente independientes entre sí.

    Supongamos que $\langle B \rangle \neq V$, es decir, que $\exists\hspace{1.5mm} \mathbf{b}_{n+1}\in V$ tal que éste no puede ser expresado como combinación lineal de los vectores de $B$. Por definición, entonces dicho vector es linealmente independiente de los vectores de $B$. Por lo tanto, podemos definir al conjunto $B'\equiv\{\mathbf{b}_1, ...,\mathbf{b}_n, \mathbf{b}_{n+1}\}$, que tiene $n+1$ elementos linealmente independientes entre sí. Supongamos que, ahora sí, $\langle B' \rangle = V$; en ese caso, por definición, $B'$ sería una base de $V$. Sin embargo, ya que $\text{dim}(V)=n$, por lo demostrado en el Teorema 4.2.1 hemos llegado a una contradicción, ya que cualquier base de $V$ debe tener exactamente $n$ elementos.
    
    Ya que la suposición $\langle B \rangle \neq V$ fue la que nos llevó a esta contradicción, tenemos que $\langle B \rangle = V$, por lo cual $B$ \textemdash un conjunto arbitrario de $n$ elementos linealmente independientes de $V$\textemdash \hspace{1mm} \emph{es} una base de $V$.

\end{proof}

\end{teorema}

\begin{corolario}{4.2}
    De los dos teoremas anteriores podemos concluir que si $W$ es un subespacio vectorial de $V$ y $\text{dim}(W)=\text{dim}(V)\implies W=V.$
\end{corolario}

Para terminar esta sección, veremos algunas formas en las cuales se pueden construir bases de un espacio vectorial de dimensión $n$ a partir de conjuntos linealmente independientes con menos de $n$ elementos, o de conjuntos linealmente dependientes que generan $V$ y tienen más de $n$ elementos.

\begin{teorema} {4.2.4}
    Sea $V$ un espacio vectorial de dimensión $n$, entonces cualquier conjunto finito linealmente dependiente que genera a $V$ puede reducirse hasta convertirse en una base de $V$.

\begin{proof}
    Sea $D$ un conjunto finito linealmente dependiente tal que $\langle D \rangle =V$. Ya que $D$ es linealmente dependiente, entonces existe un vector $\mathbf{v}$ que puede ser expresado como combinación lineal de los demás por lo cual, definiendo $D'=D\setminus\{\mathbf{v}\}$ tenemos que $\mathbf{v}\in\langle D' \rangle$, donde claramente $D'$ también es finito. Por el Teorema 3.2.1 de la sección \ref{Subsec:Espacio_generado_y_conjunto_generador} sabemos que $\langle D' \rangle =\langle D \rangle =V$. Si el conjunto generador $D'$ no es linealmente independiente, podemos seguir retirando vectores de la misma forma sin afectar su espacio generado hasta obtener un conjunto linealmente independiente que genera a $V$, es decir, una base para $V$.
\end{proof}

\end{teorema}

\begin{teorema} {4.2.5}
Sea $V$ un espacio vectorial de dimensión $n$, entonces cualquier conjunto linealmente independiente con menos de $n$ elementos no genera a $V$, pero puede extenderse hasta convertirse en una base de $V$.

\begin{proof}
Sea $L$ un conjunto linealmente independiente con $m$ elementos, donde $m<n$. Entonces $L$ no genera a $V$ ya que, de lo contrario, sería una base de $V$ y tendríamos dos bases de $V$ con diferente cardinalidad, lo cual contradice al Teorema 4.2.1. Sea $S$ un conjunto generador de $V$. Ya que $L$ no genera a $V$, debe haber algún vector en $\mathbf{v}\in S$ tal que $\mathbf{v}\notin \langle L \rangle$. Definimos ahora al conjunto $L'=L\cup \{\mathbf{v}\}$ el cual, por el Teorema 3.3.3.3 de la sección \ref{Teo:Dependencia_e_independencia_lineal}, es linealmente independiente. Si $L'$ no genera a $V$, podemos repetir el proceso hasta llegar a un conjunto linealmente independiente que genera a $V$, i.e., una base para $V$.
\end{proof}

\end{teorema}

Sabiendo que un mismo espacio vectorial puede tener muchas bases diferentes \textemdash todas con el mismo número de elementos\textemdash \hspace{1mm}, en la siguiente sección nos enfocaremos a ver algunos tipos de bases que resultan ser útiles comunmente, y a entender cómo podemos construirlas y usarlas.

\newpage
\subsection{Ortogonalización y ortonormalización} \label{Subsec:Ortogonalización_y_ortonormalización}

Recordemos de la sección \ref{Subsec:Interpretación_geométrica_del_producto_escalar} que dos vectores $\mathbf{u}$ y $\mathbf{v}$ son ortogonales si $\langle\mathbf{u},\mathbf{v}\rangle=0$. Supongamos que este no es el caso, i.e., que $\langle\mathbf{u},\mathbf{v}\rangle\neq 0$\footnote{Aquí implícitamente estamos asumiendo que $\mathbf{u}$ y $\mathbf{v}$ son vectores no nulos ya que, por definición, el producto escalar \emph{distingue} al vector nulo (ver sec. \ref{Def:Producto_escalar}).}. Existe una manera de modificar cualquiera de los vectores de tal forma que se vuelva ortogonal al otro: simplemente definimos $\mathbf{u'}=\mathbf{u}-\langle\mathbf{u},\mathbf{v}\rangle\frac{\mathbf{v}}{||\mathbf{v}||^2}$ y comprobamos que $$\langle\mathbf{u'},\mathbf{v}\rangle=\big\langle\mathbf{u}-\langle\mathbf{u},\mathbf{v}\rangle\frac{\mathbf{v}}{||\mathbf{v}||^2},\mathbf{v}\big\rangle=\langle\mathbf{u},\mathbf{v}\rangle-\big\langle\frac{\langle\mathbf{u},\mathbf{v}\rangle}{||\mathbf{v}||^2}\mathbf{v},\mathbf{v}\big\rangle=\langle\mathbf{u},\mathbf{v}\rangle-\frac{\langle\mathbf{u},\mathbf{v}\rangle}{\langle\mathbf{v},\mathbf{v}\rangle}\langle\mathbf{v},\mathbf{v}\rangle=\langle\mathbf{u},\mathbf{v}\rangle-\langle\mathbf{u},\mathbf{v}\rangle\cdot 1=0.$$ 

A esto se le conoce como un proceso de \emph{ortogonalización}. Observemos que, si $\mathbf{v}$ es un vector unitario (i.e., si $||\mathbf{v}||=1$) entonces la definición de $\mathbf{u'}$ se reduce a $\mathbf{u'}=\mathbf{u}-\langle\mathbf{u},\mathbf{v}\rangle\mathbf{v}$, simplificando el proceso. Por razones que irán quedando más claras con la experiencia, a menudo es conveniente trabajar con bases de vectores que sean ortogonales entre sí y, en casos específicos, que además sean unitarios, por lo cual damos las siguientes definiciones..

\subsubsection{Definiciones}

\begin{tcolorbox}

    \underline{Def.} Sea $O=\{\mathbf{o}_1, \mathbf{o}_2, ..., \mathbf{o}_n\}$ una base de un espacio vectorial $V$. Decimos que $O$ es una \emph{base ortogonal} si cada uno de sus vectores es ortogonal a todos los demás, i.e., si $\langle\mathbf{o}_i,\mathbf{o}_j\rangle=0\hspace{3mm}\forall\hspace{1.5mm} \mathbf{o}_i, \mathbf{o}_j\in O, \hspace{1.5mm} i\neq j$.

\vspace{3mm}

    \underline{Def.} Sea $N=\{\mathbf{n}_1, \mathbf{n}_2, ..., \mathbf{n}_n\}$ una base de un espacio vectorial $V$. Decimos que $N$ es una base \emph{ortonormal} si es una base ortogonal y, además, todos sus vectores son unitarios, i.e., si $\langle\mathbf{n}_i,\mathbf{n}_j\rangle=\delta_{ij}\hspace{3mm}\forall\hspace{1.5mm} \mathbf{n}_i,\mathbf{n}_j\in N,$ donde $\delta_{ij}$ es la \emph{delta de Kronecker} de dos índices\footnote{En general, la delta de Kronecker de $n$ índices se define como $\delta_{ab...n}=1$ si $a=b=...=n$ y $\delta_{ab...n}=0$ en cualquier otro caso.}.

\end{tcolorbox}

Antes de ver cómo podemos construir bases ortogonales y ortonormales, el siguiente teorema y corolario nos ayudarán a comenzar a entender su utilidad.

\begin{teorema} {4.3.1.1}
    Sea $V$ sobre $K$ un espacio vectorial con producto escalar y $S=\{\mathbf{v}_1, ..., \mathbf{v}_n\}$ un subconjunto ortogonal de $V$ con $\mathbf{v}_i\neq\mathbf{0}\hspace{3mm}\forall\hspace{1.5mm} 1\leq i\leq n.$ Si $\mathbf{u}\in \langle S \rangle \implies$ $$\mathbf{u} = \sum_{i=1}^n \frac{\langle\mathbf{u},\mathbf{v}_i\rangle}{||\mathbf{v}_i||^2}\mathbf{v}_i=\sum_{i=1}^n P_{\mathbf{v}_i}(\mathbf{u})\frac{\mathbf{v}_i}{||\mathbf{v}_i||}.$$

\begin{proof}
    Ya que $\mathbf{u}\in\langle S \rangle \implies \mathbf{u}=c_1\mathbf{v}_1+...+c_n\mathbf{v}_n=\sum_{i=1}^n c_i\mathbf{v}_i$ para algunos coeficientes $c_i\in K$. Para ver precisamente quiénes son esos coeficientes $c_i$, observemos que para $1\leq j\leq n$ $$\langle\mathbf{u},\mathbf{v}_j)=\big \langle \sum_{i=1}^n c_i\mathbf{v}_i, \mathbf{v}_j \big \rangle.$$ \noindent Aplicando las propiedades del producto escalar y la definición de conjunto ortogonal, tenemos que $$\langle\mathbf{u},\mathbf{v}_j\rangle=\sum_{i=1}^n \langle c_i\mathbf{v}_i, \mathbf{v}_j\rangle= \sum_{i=1}^n c_i\langle\mathbf{v}_i,\mathbf{v}_j\rangle=c_j\langle\mathbf{v}_j,\mathbf{v}_j\rangle=c_j||\mathbf{v}_j||^2\iff c_j||\mathbf{v}_j||^2=\langle\mathbf{u},\mathbf{v}_j\rangle\iff c_j=\frac{\langle\mathbf{u},\mathbf{v}_j\rangle}{||\mathbf{v}_j||^2}.$$ Sustituyendo, tenemos que \[
        \mathbf{u}=\sum_{i=1}^n c_i \mathbf{v}_i=\sum_{i=1}^n \frac{\langle\mathbf{u},\mathbf{v}_i\rangle}{||\mathbf{v}_i||^2}\mathbf{v}=\sum_{i=1}^n P_{\mathbf{v}_i}(\mathbf{u})\frac{\mathbf{v}_i}{||\mathbf{v}_i||}
    ,\] \noindent como se quería demostrar originalmente. En particular, observamos que $c_i= \frac{P_{\mathbf{v}_i}(\mathbf{u})}{||\mathbf{v}_i||}.$
\end{proof}

Habíamos visto con anterioridad que cualquier vector puede ser expresado como una combinación lineal única de elementos de su base; sin embargo, no habíamos entrado en detalles sobre cómo obtener los coeficientes necesarios para esto más allá de plantear y resolver un sistema de ecuaciones. Si aplicamos el teorema anterior a una base ortogonal $O$ de un espacio vectorial $V$, entonces $\forall\hspace{1.5mm} \mathbf{u}\in V $ tenemos una \emph{receta} para obtener directamente los coeficientes necesarios para expresar a $\mathbf{v}$ como combinación lineal de los vectores de $O$: de ahí viene la utilidad de las bases ortogonales.

    Nótese que, en particular, si el conjunto $S$ es \emph{ortonormal}, entonces $c_j=\langle\mathbf{u},\mathbf{v}_j\rangle$ en la demostración anterior y el resultado que obtuvimos se reduce a \[
        \mathbf{u} = \sum_{i=1}^n \langle\mathbf{u},\mathbf{v}_i\rangle\mathbf{v}_i
    .\] \noindent Por lo tanto, si aplicamos el Teorema 4.3.1.1 a una base ortonormal $N$, los coeficientes mencionados en el párrafo anterior son simplemente el producto escalar del vector $\mathbf{u}$ con cada vector de la base ortonormal.

\end{teorema}

\begin{corolario} {4.3.1}
Sea $V$ sobre $K$ un espacio vectorial con producto interior y $S$ un subconjunto ortogonal con vectores no nulos, entonces $S$ es linealmente independiente.

\begin{proof}
    Supongamos que $S=\{\mathbf{v}_1, ..., \mathbf{v}_n\}.$ Fijamos nuestra atención en la combinación lineal $\sum_{i=1}^n c_i\mathbf{v}_i=\mathbf{0}$, con $c_i\in K$. Aplicando el Teorema 4.3.1.1 con $\mathbf{u}=\mathbf{0}$ tenemos que $c_i=\frac{\langle\mathbf{0},\mathbf{v}_i\rangle}{||\mathbf{v}_i||^2}=0\hspace{3mm}\forall\hspace{1.5mm}  c_i\implies S$ es linealmente independiente.
\end{proof}

Observemos que este corolario es una genearlización del primer ejerrcicio de repaso de la sección 3. Además, este corolario y el teorema del cual se desprende nos dicen que los conjuntos ortogonales son buenos candidatos para bases, ya que son linealmente independientes y el cálculo de los coeficientes $c_i$ es sumamente sencillo. En particular, aplicando este corolario a los conjuntos ortogonales vemos que, si $V$ es un espacio vectorial de dimensión $n$, entonces cualquier conjunto ortogonal de $n$ vectores es una base de $V$ (ver teorema 4.2.3). 
\end{corolario}

Así como a partir de cualquier conjunto linealmente independiente de un espacio vectorial $V$ se puede construir una base para $V$ se puede, además, realizar un proceso de \emph{ortogonalización} u \emph{ortonormalización} de esa misma base. La demostración de este hecho\textemdash que también nos deletrea el proceso a seguir para lograrlo\textemdash \hspace{0.5mm} se conoce como el Teorema de Gram-Schmidt.

\subsubsection{Teorema de Gram-Schmidt} \label{Teo:Gram-Schmidt}

\begin{teorema} {4.3.2.1 (Gram-Schmidt)}
    Sea $V$ sobre $K$ un espacio vectorial y $S=\{\mathbf{u}_1, ..., \mathbf{u}_n\}$ un subconjunto linealmente independiente de $V$. Si definimos al conjunto $S'=\{\mathbf{v}_1, ..., \mathbf{v}_n\}$ de tal forma que $\mathbf{v}_1=\mathbf{u}_1$ y $$\mathbf{v}_k=\mathbf{u}_k-\sum_{j=1}^{k-1}\frac{\langle\mathbf{u}_k,\mathbf{v}_j\rangle}{||\mathbf{v}_j||^2} \mathbf{v}_j=\mathbf{u}_k-\sum_{j=1}^{k-1} P_{\mathbf{v}_j}(\mathbf{u}_k) \frac{\mathbf{v}_j}{||\mathbf{v}_j||},\hspace{3mm 2\leq k\leq n},$$ entonces $S'$ es un subconjunto ortogonal de $V$ tal que $\langle S' \rangle = \langle S \rangle.$

\begin{proof}
    Esta demostración se hará por inducción sobre $n$ y, para realizarla, definimos a $S_k\equiv\{\mathbf{u}_1, ..., \mathbf{u}_k\}$ para $k=1,2, ..., n.$

    \vspace{3mm}
\textbf{Base inductiva}
Si $n=1$, entonces el teorema se demuestra trivialmente, ya que $S'=S$, por lo cual $\langle S' \rangle =\langle S \rangle$ y, además,  $S'$ sería un subconjunto ortogonal de $V$ por vacuidad.

    \vspace{3mm}
\textbf{Hipótesis de inducción}
Supongamos que el conjunto $S'_{k-1}=\{\mathbf{v}_1, ..., \mathbf{v}_{k-1}\}$ ha sido construido siguiendo el proceso descrito en el planteamiento del teorema y que cumple las propiedades deseadas, es decir, que es un conjunto ortogonal tal que $\langle S'_{k-1} \rangle = \langle S_{k-1} \rangle,$ donde $S_{k-1}$ es linealmente independiente.

    \vspace{3mm}
\textbf{Paso inductivo}
    Sea $S_k$ un conjunto linealmente independiente y $S'_k=\{\mathbf{v}_1, ..., \mathbf{v}_{k-1}, \mathbf{v}_k\}$ tal que $$\mathbf{v}_k=\mathbf{u}_k-\sum_{j=1}^{k-1}\frac{\langle\mathbf{u}_k,\mathbf{v}_j\rangle}{||\mathbf{v}_j||^2}\mathbf{v}_j.$$

    Si $\mathbf{v}_k=\mathbf{0}$ entonces la ecuación anterior implicaría que $\mathbf{u}_k\in\langle S'_{k-1} \rangle = \langle S_{k-1} \rangle,$ lo cual contradice el hecho de que $S_k$ es un conjunto linealmente independiente, por lo cual forzozamente $\mathbf{v}_k\neq\mathbf{0}.$ 

    Para $1\leq i\leq k-1$ se sigue que $$\langle\mathbf{v}_k,\mathbf{v}_i\rangle=\big \langle\mathbf{u}_k-\sum_{j=1}^{k-1}\frac{\langle\mathbf{u}_k,\mathbf{v}_j\rangle}{||\mathbf{v}_j||^2}\mathbf{v}_j,\mathbf{v}_i \big \rangle=\langle\mathbf{u}_k,\mathbf{v}_i\rangle-\sum_{j=1}^{k-1}\frac{\langle\mathbf{u}_k,\mathbf{v}_j\rangle}{||\mathbf{v}_j||^2}\langle\mathbf{v}_j,\mathbf{v}_i\rangle=\langle\mathbf{u}_k, \mathbf{v}_i\rangle-\frac{\langle\mathbf{u}_k,\mathbf{v}_i\rangle}{||\mathbf{v}_i||^2}||\mathbf{v}_i||^2=\mathbf{0},$$ ya que en nuestra hipótesis inductiva supusimos que $S'_{k-1}$ es ortogonal. Por ende, $S'_k$ es un conjunto ortogonal de vectores no nulos.

Finalmente, por construcción $S'_k\subseteq \langle S_k \rangle$, lo cual también implica que $\langle S'_k \rangle \subseteq \langle S_k \rangle;$ análogamente, por la forma en que construimos $S'_k$, $S_k\subseteq \langle S'_k \rangle$, lo cual implica que $\langle S_k \rangle \subseteq \langle S'_k \rangle.$ Juntando ambos resultados, concluimos que $\langle S_k \rangle = \langle S'_k \rangle ,$ como queríamos demostrar.

\end{proof}
    Aplicando este teorema a bases, nos dice que a partir de una base arbitraria $B$ de un espacio vectorial $V$, se puede construir una base ortogonal $O$ para ese mismo espacio $V$: sólo hace falta escoger algún vector $\mathbf{b}_1\in B$ con el cual definir $\mathbf{o}_1\equiv\mathbf{b}_1$ y después seguir el procedimiento descrito en el teorema. 

    Además, observemos que se podría modificar ligeramente el procedimiento seguido en el teorema anterior definiendo al primer vector de la nueva base como un vector unitario. Es más, supongamos que definimos un nuevo conjunto generador $N$ con $\mathbf{n}_1=\frac{\mathbf{b}_1}{||\mathbf{b}_1||}$ y después, cada vez que obtenemos un nuevo vector ortogonal a los anteriores siguiendo el proceso del Teorema 4.3.2.1, lo normalizamos antes de añadirlo a $N$: en este caso, el conjunto generador resultante será una base \emph{ortonormal} de $V$. Es decir, podríamos hacer un Teorema de Gram-Scmidt \emph{modificado} de tal forma que a partir de cualquier subconjunto linealmente independiente podamos generar un subconjunto ortonormal que genere al mismo subespacio que el anterior. Aplicado a bases, estaríamos asegurando que a partir de cualquier base $B$ de un espacio vectorial $V$, se puede construir una base ortonormal para $V$, además de detallar el proceso mediante el cual se contstruye dicha base ortonormal.  

\end{teorema}

%\subsubsection{Ejemplos de ortogonalización y ortonormalización} 

\subsection{Ejercicios de repaso}

\subsubsection{Bases}
\begin{enumerate}
    \item Sea $K$ un campo arbitrario. En el ejercicio $1.5.2.4$ de la sección \ref{Ejer:Espacios_vectoriales} demostraste que $K\times K\times ...\times K=K^n, n\in\mathbb{N}$ era un espacio vectorial sobre $K$. Sea $\mathbf{e}_1=\begin{pmatrix} 1 & 0 & 0 & 0 & ... & 0 \end{pmatrix}, \mathbf{e}_2=\begin{pmatrix} 0 & 1 & 0 & 0 & ... & 0 \end{pmatrix}, ..., \mathbf{e}_n = \begin{pmatrix} 0 & 0 & 0 & 0 & ... & 1 \end{pmatrix}$. Demuestra que $\{\mathbf{e}_1, \mathbf{e}_2, ..., \mathbf{e}_n\}$ es una base para $K^n$ sobre $K$. En particular, demuestra que es una base ortonormal. A esta base se le conoce como la \emph{base canónica} del espacio vectorial $K^n$ sobre $K$.  
    \item Sea $d_i$ el $i$-ésimo dígito de tu número de cuenta. Considera el conjunto $\{\begin{pmatrix} d_1&d_2&d_3 \end{pmatrix}, \begin{pmatrix} d_4&d_5&d_6 \end{pmatrix},\\ \begin{pmatrix} d_7&d_8&d_9 \end{pmatrix}\}$. ¿Forma una base para $\mathbb{R}^3$? Si sí lo es, demuéstralo. Si no, demuestra por qué no es base de $\mathbb{R}^3$ y realiza las modificaciones necesarias para obtener una base. 
    \item Sea $B$ la base que obtuviste en el ejercicio anterior. Expresa a los vectores $\begin{pmatrix} d_9&d_8&d_7 \end{pmatrix} , \begin{pmatrix} d_6&d_5&d_4 \end{pmatrix} , \\ \begin{pmatrix} d_3&d_2&d_1 \end{pmatrix} \in \mathbb{R}^3$ como combinaciones lineales de los vectores de $B$. 
\end{enumerate}

\subsubsection{Dimensión}
\begin{enumerate}
    \item Sea $V$ un espacio vectorial de dimensión $n$ y $D$ un conjunto linealmente dependiente tal que $\langle D \rangle =V$. Demuestra que $D$ tiene más de $n$ elementos. 
    \item Demuestra que cualquier base del espacio vectorial $\mathbb{C}$ sobre $\mathbb{R}$ debe tener más de un elemento y que, por lo tanto, $\text{dim}((\mathbb{C},\mathbb{C}))\neq \text{dim}((\mathbb{C},\mathbb{R})).$ 
    \item Da un ejemplo distinto a $\mathbb{C}$ en donde la dimensión (algebráica) de un espacio vectorial no corresponda con la dimensión (geométrica) de su representación geométrica. Además, da un ejemplo distinto al del ejercicio anterior en donde la dimensión de un espacio vectorial cambie según el campo sobre el cual está definido. 
\end{enumerate}


\subsubsection{Ortogonalización y ortonormalización}
\begin{enumerate}
    \item Sea $B$ la base que obtuviste del ejercicio 4.4.1.2. Aplica el Teorema de Gram-Schmidt y construye una base ortogonal de $\mathbb{R}^3$ a partir de $B$. 
    \item Aplica el Teorema de Gram-Schmidt \emph{modificado} y construye una base ortonormal a partir de $B$. 
\item Sea $P_0(x)=1, P_1(x)=x, P_2(x)=\frac{3x^2-1}{2}, P_3(x)=\frac{5x^3-3x}{2}.$ Demuestra que el conjunto $\{P_0,P_1,P_2,P_3\}$ es una base del espacio vectorial $P^3$ y di si es ortogonal y/o ortonormal en el intervalo $[-1,1]$ (Nota: tendrás que utilizar la definición de producto escalar para funciones vista en la sec. \ref{Ejem:Producto_escalar}). 
\end{enumerate}

\section{Transformaciones lineales y sus representaciones matriciales} \label{Sec:Transformaciones lineales y sus representaciones matriciales} 

\section{Composición de transformaciones lineales y multiplicación de matrices} \label{Sec: Composición de transformaciones lineales y multiplicación de matrices}

\section{Invertibilidad e isomorfismos} \label{Sec:Invertibilidad e isomorfismos}

\section{Matriz de cambio de base} \label{Sec:Matriz de cambio de base}

\section{Eigenvectores y eigenvalores} \label{Sec:Eigenvectores y eigenvalores} 

\section{Diagonalizabilidad} \label{Sec:Diagonalizabilidad} 

\newpage
\section{Introducción a sistemas lineales de ecuaciones diferenciales} \label{Sec: Sistemas lineales de ecuaciones diferenciales ordinarias} 

Esta pequeña sección de las notas tiene como finalidad introducir algunos de los conceptos que utilizaremos durante la segunda mitad del tercer módulo del curso, para el cual seguiremos principalmente el libro \emph{Linear Algebra: A Modern Introduction} de Poole (págs. 340-348).

\subsection{Sistemas lineales de ecuaciones algebráicas}

Cuando hablamos de un \emph{sistema de ecuaciones}, generalmente nos referimos a un grupo de ecuaciones que deben cumplirse simultáneamente, por ejemplo, $$x_1 + x_2 = 1,$$ $$x_1 - x_2 = 5.$$ Para este sistema de ecuaciones, decimos que tenemos dos \emph{incógnitas}, $x_1$ y $x_2$, que queremos encontrar.

\vspace{3mm}
Como sabemos, podemos formar sistemas con más ecuaciones y/o incógnitas. En general, podemos expresar un sistema de $m$ ecuaciones y $n$ incógnitas como $$f_1(x_1,x_2,...,x_n)=0$$ $$f_2(x_1,x_2,...,x_n)=0$$ $$...$$ $$f_m(x_1,x_2,...,x_n)=0,$$ donde $f_j$, con $1\leq j\leq m$, es una función arbitraria de las incógnitas $x_i$, donde $1\leq i\leq n$. Decimos que tenemos una \emph{solución} al sistema cuando encontramos los valores de las incógnitas $x_i$ para los cuales todas las ecuaciones se verifican simultáneamente. Si reordenamos las ecuaciones del sistema que aparece al inicio de esta sección, podemos ver que en ese caso $f_1(x_1,x_2)=x_1+x_2-1$ y $f_2(x_1,x_2)=x_1-x_2-5$, y que la solución al sistema es $x_1=3, x_2=-2$. Observemos que $m=2$ y $n=2$ en este caso; precisamente por esto decimos que es un sistema de dos ecuaciones con dos incógnitas.

\vspace{3mm}
Por lo regular, trabajamos con sistemas de ecuaciones en donde las funciones $f_j$ son \emph{polinomios} en las variables $x_i$; a este tipo de ecuaciones se les llama \emph{algebráicas} y, por lo tanto, en esos casos decimos que tenemos un \emph{sistema de ecuaciones algebráicas}, como el ejemplo del inicio de esta sección.

\vspace{3mm}
Cuando en una ecuación algebráica las incógnitas no aparecen elevadas a una potencia mayor a uno, decimos que la ecuación es \emph{lineal}. Análogamente, cuando en un sistema de ecuaciones algebráicas las incógnitas no aparecen elevadas a una potencia mayor a uno \textemdash o, equivalentemente, cuando todos los polinomios $f_j$ son de grado uno\textemdash\hspace{1mm} tenemos un sistema de ecuaciones algebráicas lineales; generalmente, en estos casos decimos que tenemos un \emph{sistema lineal de ecuaciones algebráicas}.

\vspace{3mm}
Lo que caracteriza a los sistemas de ecuaciones algebráicas (de los cuales los sistemas lineales de ecuaciones algebráicas son un caso particular) es que sus soluciones pueden ser números enteros, racionales, irracionales o hasta complejos, pero siempre son números; es decir, las \emph{incógnitas} de este tipo de sistemas de ecuaciones \textemdash y, por tanto, sus soluciones\textemdash\hspace{1mm} son \emph{números}. Esta es la principal diferencia entre este tipo de sistemas y los sistemas de ecuaciones \emph{diferenciales}, que veremos más adelante. Antes de eso, tenemos que entender la diferencia entre las ecuaciones algebráicas y las ecuaciones diferenciales, lo cual haremos en la siguiente sección.

\newpage
\subsection{Introducción a ecuaciones diferenciales (ordinarias)} \label{Subsec:Introducción a ecuaciones diferenciales ordinarias}

A diferencia de una ecuación algebráica, en una ecuación \emph{diferencial} la incógnita de la ecuación \textemdash y, por lo tanto, su solución\textemdash es una \emph{función}. Por ejemplo, la ecuación $$\dot{x} + x = 0,$$ donde $\dot{x}$ representa la derivada de la función $x$ con respecto a alguna variable independiente \textemdash usualmente llamada $t$, aunque esto es sólo una etiqueta\footnote{Con \emph{etiqueta} nos referimos a que, de haber postulado que la función $x$ dependía de una variable independiente $s$, entonces la solución a la ecuación diferencial sería $x(s)=e^{-s},$ lo cual no cambia en absoluto el sentido de la ecuación ni de su solución. A este tipo de variables ``etiqueta'' acostumbramos llamarles \emph{variables mudas}.}\textemdash, tiene como solución a la función $x(t)=e^{-t}$, y dicha solución es válida para todo $t\in\mathbb{R}$ ya que en todo este intervalo se verifica que\footnote{Observemos que pudimos haber planteado esta ecuación diferencial de manera equivalente como $\dot{x}=-x$, y la solución sería la misma, ya que $\frac{d}{dt}e^{-t}=-e^{-t}$ para todo $t\in\mathbb{R}$.} $$\frac{d }{dt}e^{-t} + e^{-t} = \bigg(\frac{d}{d(-t)}e^{-t}\bigg)\bigg(\frac{d }{dt}(-t)\bigg) + e^{-t} = \big(e^{-t}\big)\big(-1\big)+e^{-t} = -e^{-t}+e^{-t}= 0.$$

\vspace{3mm}
En general, las incógnitas de las ecuaciones diferenciales pueden ser funciones de más de una variable, y en la ecuación pueden aparecer dos o más de sus derivadas parciales; a este tipo de ecuaciones diferenciales les llamamos ecuaciones diferenciales \emph{parciales}. En cambio, cuando nuestra función incógnita depende de una sola variable, decimos que tenemos una ecuación diferencial \emph{ordinaria}; éstas últimas son con las cuales trabajaremos. A continuación, formalizamos algunas definiciones que utilizaremos.

\vspace{3mm}
\begin{tcolorbox}
\underline{Def.} Una \emph{ecuación diferencial ordinaria} es una relación entre una función incógnita $x(t)$, sus derivadas, y una variable independiente $t$. En general, podemos expresar una ecuación diferencial ordinaria como $$f\big(t,x(t),\dot{x}(t),\ddot{x}(t),...\hspace{0.5mm},x^{(n)}(t)\big)=0,$$ donde $f$ es una función arbitraria y $x^{(n)}(t)$ denota la $n$-ésima derivada de la función incógnita $x$ con respecto a su variable independiente $t$. Decimos que una función $x(t)$ es una \emph{solución} a la ecuación diferencial anterior en un intervalo $I$ si tanto $x(t)$ como sus derivadas cumplen dicha ecuación para todo $t\in I$.
\end{tcolorbox}

\vspace{2mm}
Podemos clasificar algunos tipos de ecuaciones diferenciales ordinarias, como sigue\footnote{Observen la analogía con la forma en que clasificamos ecuaciones algebráicas (polinomiales), pero también las diferencias. Las definiciones presentadas serán relevantes para nuestros fines.}:

\vspace{3mm}
\begin{tcolorbox}
\underline{Def.} Decimos que una ecuación diferencial ordinaria es:
\begin{itemize}
    \item \emph{de orden} $n$ si la derivada de mayor grado que aparece en la ecuación es de orden $n$;
    \item \emph{lineal} si se puede escribir de la forma $$a_n(t)x^{(n)}(t)+a_{n-1}(t)x^{(n-1)}(t)+...+a_1(t)\dot{x}(t)+a_0(t)x(t)+b(t)=0$$ para funciones $a_1(t),a_2(t),...\hspace{0.5mm},a_n(t),b(t)$ dadas (que pueden, en particular, ser constantes)\footnote{En particular, si $b(t)$ es la constante cero, decimos que la ecuación es lineal \textit{homogénea}.};
    \item \emph{autónoma} si en la ecuación no aparece la variable independiente $t$.
\end{itemize}
\end{tcolorbox}

\newpage
\subsection{Ecuaciones diferenciales (ordinarias) con condiciones inciales y sistemas lineales de ecuaciones diferenciales} \label{Subsec:Ecuaciones diferenciales (ordinarias) con condiciones iniciales y sistemas lineales de ecuaciones diferenciales}

Con regularidad, nos encontramos con ecuaciones diferenciales que tienen más de una solución, e inclusive pueden tener una infinidad de ellas. Por ejemplo, regresando a la ecuación $$\dot{x} + x = 0,$$ podemos verificar que la función $x(t)=C e^{-t}$ es una solución en todo el intervalo $\mathbb{R}$ para cualquier $C\in\mathbb{R}$. Decimos entonces que $x(t)=Ce^{-t}$ es una \emph{solución general} a la ecuación diferencial anterior\footnote{Por dar otro ejemplo, podemos decir que la ecuación $\ddot{x}+x=0$ tiene como solución general a $x(t)=A\sin(t)+B\cos(t)$ en todo el intervalo $\mathbb{R}$ para cualesquiera valores $A,B\in\mathbb{R}$ (¡compruébalo!).}. En esta solución general, el valor $C$ representa un \emph{grado de libertad} de la solución, el cual puede removerse agregando una \emph{restricción} a la ecuación diferencial, la cual viene en forma de una \emph{condición inicial}.

\vspace{3mm}
Supongamos que nos piden resolver la ecuación diferencial anterior bajo la condición de que al evaluar la función en $t=0$ obtenemos un cierto valor $C_0\in\mathbb{R}$. Es decir que la solución $x(t)$, además de cumplir la ecuación diferencial, debe cumplir que $x(t)|_{t=0}=C_0.$ Partiendo de nuestra solución general, resolvemos ahora la última ecuación mostrada como sigue $$x(t)|_{t=0}=C_0\implies C e^{-t}|_{t=0}=C_0\implies C e^{0}=C_0\implies C(1)=C_0\implies C=C_0.$$ Por lo tanto, la solución a la ecuación diferencial $\dot{x}+x=0$ con condición inicial $x(0)=C_0$, donde $C_0\in\mathbb{R}$ es un valor determinado, es $x(t)=C_0 e^{-t}$. Ya que en este caso $x(t)$ sigue cumpliendo la ecuación diferencial original pero ya no tiene grados de libertad, decimos que es una \emph{solución particular} a la ecuación diferencial $\dot{x}+x=0$ sujeta a la \emph{condición inicial} $x(0)=C_0,$ con $C_0\in\mathbb{R}$.

\vspace{3mm}
\begin{tcolorbox}
\underline{Def.} Decimos que una función $x(t)$ es una \emph{solución particular} a una ecuación diferencial ordinaria de grado uno si, además de cumplir con dicha ecuación, cumple con la \emph{condición inicial} $$x(t_0)=C_0,$$ donde $C_0$ es un valor fijo.
\end{tcolorbox}

De forma análoga a como podemos formar sistemas de ecuaciones a partir de ecuaciones algebráicas, también podemos formarlos a partir de ecuaciones diferenciales. Por ejemplo, el sistema de ecuaciones diferenciales $$\dot{x_1} - x_2 = 0,$$ $$x_1 + \dot{x_2} = 0,$$ tiene como solución general a $x_1(t)=A\sin(t), x_2(t)=A\cos(t)$, válida en todo el intervalo $\mathbb{R}$ para cualquier $A\in\mathbb{R}$. Observemos que, en este sistema, ninguna de las incógnitas apareció elevada a una potencia mayor a uno. Por lo tanto, este sistema es un ejemplo de un \emph{sistema lineal de ecuaciones diferenciales ordinarias}.

\vspace{3mm}
\begin{tcolorbox}
\underline{Def.} Decimos que un sistema de ecuaciones diferenciales es un \emph{sistema lineal de ecuaciones diferenciales ordinarias} si todas las ecuaciones diferenciales ordinarias del sistema son lineales en todas sus incógnitas. Usualmente, se les abrevia como \emph{sistemas lineales de ecuaciones diferenciales}.
\end{tcolorbox}
\vspace{3mm}

Después de esta introducción, puedes continuar leyendo las págs. 273-274 del libro \emph{Linear Algebra} de Friedberg y 340-342 del libro \emph{Linear Algebra: A Modern Introduction} de Poole para ver cómo podemos aplicar herramientas de álgebra lineal (en particular, diagonalización) para resolver este tipo de sistemas. Si deseas repasar el material presente antes de continuar, te sugiero resolver los ejercicios al final de esta sección. Después, deberás resolver los ejercicios $59, 61$ y $63$ de la sección 4.6 del Poole.

\subsection{Ejercicios de repaso}

\subsubsection{Sistemas lineales de ecuaciones algebráicas}
\begin{enumerate}
    \item Realiza un cuadro comparativo con las definiciones de sistema de ecuaciones, sistema de ecuaciones algebráicas y sistema lineal de ecuaciones algebráicas. 
    \item Escribe o argumenta por qué no puedes escribir un ejemplo de:
    \begin{enumerate}[label=\alph*)]
        \item una ecuación \emph{trascendental} (i.e., no algebráica);
        \item un sistema no lineal de ecuaciones algebráicas;
        \item un sistema lineal de ecuaciones no algebráicas.
    \end{enumerate}
    \item Demuestra que en una ecuación lineal homogénea con más de una incógnita el conjunto de soluciones forma un espacio vectorial. Después, generaliza el resultado para demostrar que en un sistema lineal de ecuaciones algebráicas homogéneas con menos ecuaciones que incógnitas el conjunto de soluciones forma un espacio vectorial.
\end{enumerate}

\subsubsection{Introducción a ecuaciones diferenciales (ordinarias)}
\begin{enumerate}
    \item Encuentra la solución general a la ecuación $\dot{x}-x=0$ en todo el intervalo $\mathbb{R}$.
    \item Sea $C^{\infty}(\mathbb{R})$ el conjunto de todas las funciones reales de variable real con derivadas de cualquier orden y sea $\frac{d}{dt}:C^{\infty}(\mathbb{R})\to C^{\infty}(\mathbb{R})$ el operador lineal \emph{derivada}. Supongamos que $x(t)$ es un eigenvector de $\frac{d}{dt}$ con eigenvalor $\lambda,$ ¿entonces quién es $x(t)$?
    \item Muestra que tanto la ecuación diferencial del ejemplo dado al inicio de la sección \ref{Subsec:Ecuaciones diferenciales (ordinarias) con condiciones iniciales} como su solución general son un caso particular del ejercicio anterior.
\end{enumerate}

\subsubsection{Ecuaciones diferenciales (ordinarias) con condiciones iniciales}
\begin{enumerate}
    \item Encuentra la solución particular a la ecuación $\dot{x}+x=0$ con la condición inicial $x(0)=5$.
    \item Encuentra la solución particular a la ecuación $\dot{x}+x=0$ con la condición inicial $x(1)=5$.
    \item Muestra que cualquier solución particular (es decir, sin grados de libertad) de la ecuación $\ddot{x}=g$ debe tener dos condiciones iniciales: una para la función $x(t)$ y otra para $\dot{x}(t)$. ¿De qué ecuación se trata?
\end{enumerate}

\subsubsection{Sistemas lineales de ecuaciones diferenciales (ordinarias)}
\begin{enumerate}
    \item Di si el siguiente sistema de ecuaciones diferenciales ordinarias es lineal, autónomo, ambos o ninguno, y explica por qué: $$\dot{x_1}+3x_2 = 0,$$ $$\dot{x_2} - x_3 = 0,$$ $$x_1-6\dot{x_3} = 0.$$
\end{enumerate}

\newpage
\section{Modelación con sistemas lineales de ecuaciones diferenciales} \label{Sec:Modelación con sistemas lineales de ecuaciones diferenciales} 
Para esta sección, seguiremos el libro \emph{Linear Algebra: A Modern Introduction} de Poole, págs. 342-348. Deberás realizar los ejercicios 65, 66, 67, 69, 71, 73 y 75 de la sección 4.6 del Poole, así como el ejercicio 15 de la sección 5.2 del Friedberg.

\newpage

\section{Funcionales y espacio dual, complemento ortogonal y proyecciones ortogonales} \label{Sec:13} 

Durante este último módulo del curso, nuestro principal objeto de estudio serán los espacios vectoriales con productos escalares, así como los operadores lineales que actúan sobre dichos espacios. Descubriremos que, con ayuda del producto escalar, podemos llegar a tener una comprensión profundamente geométrica sobre cómo actúan ciertos operadores en estos espacios \textemdash más allá de una simple descripción algrebráica dada por reglas de correspondencia. Este resultado es uno de los más importantes de todo el curso, y se le conoce como el \emph{Teorema espectral}. Dado que haremos uso frecuente del producto escalar  $\langle\cdot ,\cdot\rangle$ \textemdash principalmente, a partir de la sección \ref{Subsec:Correspondencia entre bras y kets}\textemdash \ en lo que resta del curso utilizaremos la llamada \emph{notación de bra-ket}, también conocida como \emph{notación de Dirac}, por lo que se presenta a continuación.

\vspace{3mm}

\textbf{Notación de bra-ket (o de Dirac) para espacios vectoriales con producto escalar:}
\begin{tcolorbox}
    \centering
    \begin{tabular}{cc}
        \\
        $\ket{v}$ & vector o \emph{ket}  \\ \\
        $\braket{u|v}$ & producto escalar o \emph{bra-ket}  \\ \\
        $\bra{u}$ & vector dual (funcional) o \emph{bra} \\ \\
    \end{tabular}
\end{tcolorbox}

\noindent Los vectores duales (también conocidos como \emph{funcionales} serán definidos formalmente más adelante, con ayuda de esta notación. Una diferencia muy importante con respecto a la notación utilizada anteriormente es que en esta nueva notación, por razones que veremos más adelante, la entrada \emph{derecha} del producto escalar será lineal, mientras que la entrada \emph{izquierda} será antilineal. Es decir, que si $\ket{u},\ket{v}\in V$ y $\alpha\in K$, donde $(V,K)$ es un espacio vectorial, entonces en general \[
    \bra{u}\big (\ket{\alpha v}\big ) = \bra{u}\big(\alpha\ket{v}\big) = \alpha\braket{u|v}
\] \noindent y \[
\big (\bra{\alpha u}\big ) \ket{v} = \big( \overline{\alpha}\bra{u}\big)\ket{v}  = \overline{\alpha}\braket{u|v}.
\] De las ecuaciones anteriores podemos hacer las identificaciones $\ket{\alpha v}=\alpha\ket{v}$ y $\bra{\alpha u}= \overline{\alpha}\bra{u}$, las cuales utilizaremos ampliamente durante este módulo. 

Con esta notación podemos, por ejemplo, reescribir algunos resultados que ya conocemos sobre bases ortonormales de una manera mucho más sencilla, como se muestra a continuación\footnote{Asegúrate de entender bien esta notación y a qué resultados nos referimos antes de continuar leyendo el resto de las notas.}: sea $V$ un espacio vectorial de dimensión finita $n$ con base ortonormal $\beta=\{\ket{v_1},\ket{v_2},...,\ket{v_n}\},$ entonces para todo $\ket{v}\in V$ \[\ P_{\ket{v_i}}(\ket{v})=\braket{v_i|v},\] \[\ket{v}=\sum_{i=1}^n \braket{v_i|v}\ket{v_i},
\] \noindent y para todo operador lineal $T:V\to V$ tenemos que si \[A=[T]_{\beta} \implies A_{ij}=\braket{T(v_j)|v_i}
.\] 

\newpage
\subsection{Funcionales y espacio dual} \label{Subsec:Funcionales_y_espacio_dual} 

Como hemos visto anteriormente, si $K$ es un campo y $V$ y $W$ son dos espacios vectoriales sobre $K$, entonces $\mathcal{L}(V,W)$ \textemdash el conjunto de todas las transformaciones lineales de $V$ a $W$\textemdash\hspace{0.5mm} es un espacio vectorial sobre $K$. En particular, como vimos a inicios del curso, $(K,K)$ también es un espacio vectorial, por lo cual podemos fijar nuestra atención en el espacio vectorial $\mathcal{L}(V,K)$. En esta sección veremos el importante papel que juega este espacio vectorial; principalmente, nos enfocaremos en los espacios vectoriales con producto escalar.

\vspace{3mm}
\begin{tcolorbox}
    \underline{Def.} Sea $(V,K)$ un espacio vectorial. Decimos que $\textbf{f}$ es un \emph{funcional lineal}, o simplemente un \emph{funcional}, si $\textbf{f}\in\mathcal{L}(V,K)$.

\vspace{3mm}
    \underline{Def.} Al espacio vectorial $\mathcal{L}(V,K)$ lo llamamos el \emph{espacio dual} de $V$, denotado por $V^*$. Por ello, a sus elementos también se les conoce como \emph{vectores duales}. 
\end{tcolorbox}

\vspace{3mm}
\textbf{Ejemplos de funcionales} \label{Ejem:Funcionales}

\vspace{3mm}
Sea $V$ un espacio vectorial de dimensión finita, $\beta=(\mathbf{b}_1,\mathbf{b}_2,...,\mathbf{b}_n)$ una base ordenada de $V$ y $[\hspace{0.5mm} \cdot\hspace{0.5mm}]_\beta$ el \emph{mapeo de coordenadas en la representación de la base ordenada} $\beta$ dado por $$[\mathbf{v}]_\beta=\begin{pmatrix} c_1&c_2&...&c_n \end{pmatrix}$$ para todo $\mathbf{v}\in V$, donde los coeficientes $c_i$ son tales que $\mathbf{v}=c_i \mathbf{b}_i$ en notación de Einstein. Si definimos a $\textbf{f}_i(\mathbf{v})=c_i$ para toda $1\le i\le n$, entonces $\textbf{f}_i$ es un funcional sobre $V$ conocido como la \emph{función de la i-ésima coordenada en la representación de la base ordenada} $\beta$. 

\vspace{3mm}
En los espacios vectoriales de matrices cuadradas $M_{n\times n}(K)$, donde $K$ es un campo, la traza $tr(\cdot):M_{n\times n}\to K$ y el determinante $det(\cdot):M_{n\times n}\to K$ son funcionales.

\vspace{3mm}
Sea $(V,K)$ un espacio vectorial con producto escalar y $\ket{v}\in V$ un vector arbitrario, entonces la proyección escalar sobre $\ket{v}$ dada (en notación de Dirac) por $$P_{\ket{{v}}}(\ket{u}) = \frac{\braket{v|u}}{||\ket{v}||}$$ para todo $\ket{u}\in V$ es un funcional sobre $V$. 

\newpage
\subsection{Correspondencia entre bras y kets} \label{Subsec:Correspondencia entre bras y kets} 

Sea $(V,K)$ un espacio vectorial arbitrario con producto escalar $\braket{\cdot|\cdot}:V\times V\to K$ donde, siguiendo la notación de bra-ket, la primera entrada es antilineal. Si elegimos a un vector arbitrario $\ket{v}\in V$ y lo fijamos en la entrada antilineal del producto escalar, podemos definir un funcional $\braket{v|\cdot}:V\to K$, el cual se puede escribir más sencillamente como $\bra{v}$. Ya que en notación de bra-ket a los vectores $\ket{u}$ se les llama \emph{kets} y a los productos escalares $\braket{y|x}$, \emph{bra-kets} (proveniente del inglés \emph{brackets}), a este tipo de funcionales $\bra{v}$ se les conoce como \emph{bras}.

Dado que a partir de cualquier vector $\ket{v}\in V$ se puede definir un funcional $\bra{v}\in V^*$, existe una correspondencia natural entre \emph{bras} y \emph{kets}. Sin embargo, para entender bien esta correspondencia debemos observar un detalle crucial: si hacemos el producto de un ket $\ket{v}$ por un escalar $\alpha\in K$, obteniendo así el ket $\alpha\ket{v}$, entonces el bra correspondiente será $\overline{\alpha}\bra{v}$\footnote{En la notación que usábamos antes, esto se explica como que si tomamos al vector $\alpha\mathbf{v}$ y lo fijamos en la entrada antilineal del producto escalar, obtenemos el funcional $\langle\hspace{0.5mm}\cdot\hspace{0.5mm} , \alpha\mathbf{v}\rangle:V\to K$ que, como sabemos, es igual a $\overline{\alpha}\langle\hspace{0.5mm} \cdot\hspace{0.5mm} ,\mathbf{v}\rangle$.}. Es decir, la correspondencia entre bras (que son un tipo de vectores duales) y kets está dada por \[
    \alpha\ket{v} \leftrightarrow \overline{\alpha}\bra{v}
.\] 

De esta manera, si queremos hacer el producto escalar de un vector arbitrario $\alpha_1 \ket{u}$ con otro vector $\alpha_2 \ket{v}$, podemos simplemente unir el ket $\alpha_1 \ket{u}$ con el bra correspondiente al otro vector, que sería $\overline{\alpha_2}\ket{v}$, obteniendo \[
    \big(\overline{\alpha_2}\bra{v}\big)\big(\alpha_1\ket{u}\big) = \overline{\alpha_2}\alpha_1\braket{v|u}
.\] Una ventaja de esta notación es que \textemdash siempre y cuando recordemos bien la correspondencia entre bras y kets mostrada en el párrafo anterior\textemdash\hspace{0.5mm} ya no tenemos que preocuparnos por cómo sacar escalares del producto escalar.

Más generalmente, si $\{\ket{b_1},\ket{b_2},...,\ket{b_n}\}$ es una base de $V$, entonces la correspondencia entre bras de $V^*$ y kets de $V$ puede expresarse como \[
    \sum_{i=1}^n c_i \ket{b_i} \leftrightarrow \sum_{i=1}^n \overline{c_i}\bra{b_i}
.\]    

\newpage
\subsection{Complemento ortogonal} \label{Subsec:Complemento_ortogonal} 

\begin{tcolorbox}
    \underline{Def.} Sea $V$ un espacio vectorial con producto escalar y $W\subseteq V$ un subespacio vectorial de $V$. Definimos al \emph{complemento ortogonal} de $W$ como \[
        W^{\perp}=\{\ket{x}\in V\mid \braket{x|w}=0\hspace{2mm}\forall\hspace{0.5mm} \ket{w}\in W\} 
    .\] 
\end{tcolorbox}

\noindent De la definición anterior podemos hacer varias observaciones:
\begin{itemize}
    \item Ya que para cualquier espacio vectorial $V$ con producto escalar el vector nulo es ortogonal a todos los vectores del espacio, tenemos que $\{\ket{0}\}^{\perp}=V$ y $V^{\perp}=\{\ket{0}\}.$
    \item Por la observación anterior de la ortogonalidad del vector nulo y las propiedades lineales del producto escalar, para todo subespacio vectorial $W\subseteq V$, $W^{\perp}$ también es un subespacio vectorial de $V$.
    \item Para cualquier subespacio vectorial $W\subseteq$ de V, $W\cap W^{\perp}=\{\ket{0}\}.$
\end{itemize}

\begin{lema}\empty
    Sea $V$ un espacio vectorial con producto esclar sobre un campo $K$, $W$ un subespacio vectorial de $V$ y $\beta=\{\ket{v_1}, \ket{v_2},..., \ket{v_k}\}$ una base de $W$, entonces $\ket{z}\in W^{\perp}$ si y sólo si $\braket{z|v_i}=0$ para toda $\ket{v_i}\in\beta.$

    \begin{proof}
        Sea $\ket{w}\in W$ un vector arbitrario. Como $\beta$ es base de $W$, existen coeficientes $c_i\in K$ tales que \[
            \ket{w}=\sum_{i=1}^k c_i\ket{v_i}
        .\] \noindent Sea $\ket{z}\in W^{\perp}$, entonces tenemos que $\braket{z|w}=0,$ pero esto ocurre si y sólo si \[
        \bra{z}\big(\sum_{i=1}^k c_i\ket{v_i}\big)\iff \sum_{i=1}^k c_i \braket{z|v_i}=0\iff \braket{z|v_i}=0
    \] \noindent para todo $\ket{v_i}\in\beta$, ya que los coeficientes $c_i$ son arbitrarios. 
    \end{proof}

\end{lema}

\begin{teorema} {13.1.1}
    Sea $V$ un espacio vectorial con producto escalar y $W\subseteq V$ un subespacio vectorial de dimensión finita $k$, entonces para todo $\ket{y}\in V$ existen vectores únicos $\ket{u}\in W$ y $\ket{z}\in W^{\perp}$ tales que $\ket{y}=\ket{u}+\ket{z}.$ Además, si $\beta=\{\ket{v_1},\ket{v_2},...,\ket{v_k}\}$ es una base ortonormal de $W$, entonces $$\ket{u}=\sum_{i=1}^k \braket{v_i|y}\ket{v_i}.$$
    \begin{proof}
        Sea $\beta=\{\ket{v_1},\ket{v_2},...,\ket{v_k}\}$ una base ortonormal de $W$. Definimos \[
            \ket{u}=\sum_{i=1}^k\braket{v_i|y}\ket{v_i}\hspace{3mm}\text{y}\hspace{3mm}\ket{z}=\ket{y}-\ket{u}
        .\] \noindent Claramente $\ket{u}\in W$ y $\ket{y}=\ket{u}+\ket{z}.$ Además, observemos que \[
        \braket{z|u}=\big(\bra{y}-\bra{u}\big)\ket{u}=\braket{y|u}-\braket{u|u}=\bra{y}\bigg(\sum_{i=1}^k\braket{v_i|y}\ket{v_i}\bigg)-\bigg(\sum_{i=1}^k\braket{y|v_j}\bra{v_i}\bigg)\bigg(\sum_{j=1}^k \braket{v_j|y}\ket{v_j}\bigg)\] \[
    = \sum_{i=1}^k\braket{v_i|y}\braket{y|v_i}-\sum_{i=1}^k\sum_{j=1}^k\braket{y|v_i}\braket{v_j|y}\braket{v_i|v_j}=\sum_{i=1}^k\braket{v_i|y}\braket{y|v_i}-\sum_{i=1}^k\sum_{j=1}^k\braket{y|v_i}\braket{v_j|y}\delta_{ij}\] \[=\sum_{i=1}^k\braket{v_i|y}\braket{y|v_i}-\sum_{i=1}^k\braket{y|v_i}\braket{v_i|y}=0
,\] \noindent por lo que $\ket{z}\in W^{\perp}$. 

\vspace{3mm}
Para demostrar la unicidad de $\ket{u}$ y $\ket{z}$, supongamos que existen $\ket{u'}\in W$ y $\ket{z'}\in W^{\perp}$ tales que $\ket{y}=\ket{u'}+\ket{z'}$. Entonces, tenemos que \[
    \ket{y}=\ket{u'}+\ket{z'}=\ket{u}+\ket{z}\implies \ket{u}-\ket{u'}=\ket{z}-\ket{z'}
.\] \noindent Ya que el vector del lado izquierdo de la última ecuación es un elemento de $W$ y el del lado derecho, de $W^{\perp}$, tenemos que \[
\ket{u}-\ket{u'}=\ket{z}-\ket{z'}\in W\cap W^{\perp} \implies \ket{u}-\ket{u'}=\ket{0}=\ket{z}-\ket{z'}\implies \ket{u'}=\ket{u}\hspace{3mm} \text{y}\hspace{3mm} \ket{z'}=\ket{z}
.\] 
    \end{proof}
\end{teorema}

\begin{teorema} {13.1.2}
    Sea $S=\{\ket{v_1},\ket{v_2},...,\ket{v_k}\}$ un conjunto ortonormal de vectores de un espacio vectorial $V$ de dimensión finita $n$ con producto escalar. Entonces
    \begin{enumerate}[label=\alph*)]  
    \item $S$ se puede extender a una base ortonormal $\{\ket{v_1},\ket{v_2},...,\ket{v_k},\ket{v_{k+1}},...,\ket{v_n}\}$ de $V$.
    \item Usando la notación anterior, si $W=\langle S \rangle$, entonces $S_1=\{\ket{v_{k+1}},...,\ket{v_n}\}$ es base ortonormal de $W^{\perp}$. 
    \item Para cualquier subespacio $W \subseteq V, \text{dim}(V) = \text{dim}(W) + \text{dim}(W^{\perp}).$
    \end{enumerate}
    \begin{proof}
        \begin{enumerate}[label=\alph*)]
            \item Por el Teorema de reemplazamiento (ver sec. \ref{Subsubsec:Teo_de_reemplazamiento}), sabemos que podemos extender a $S$ para formar una base $S'=\{\ket{v_1},\ket{v_2},...,\ket{v_k},\ket{w_{k+1}},...,\ket{w_n}\}$ de $V$. Aplicando el Teorema de Gram-Schmidt, podemos obtener una base $\{\ket{v_1},\ket{v_2},...,\ket{v_k},\ket{v_{k+1}},...,\ket{v_n}\}$ de $V$.
            \item Observemos que, por la construcción de la base $\{\ket{v_1},\ket{v_2},...,\ket{v_k},\ket{v_{k+1}},...,\ket{v_n}\}$ de $V$, $\langle S_1 \rangle \subseteq W^{\perp}$ y $S_1$ es un conjunto linalmente independiente. Por último, observemos que para cualquier $\ket{v}\in V$, $$\ket{v}=\sum_{i=1}^n \braket{v_i|v}\ket{v_i}.$$ \noindent En particular, para todo $\ket{v}\in W^\perp$ tenemos que $\braket{v_i|v}=0$ para $1\le i\le k$. En este caso, \[
                \ket{v}=\sum_{i=k+1}^n\braket{v_i|v}\ket{v_i}=\sum_{i=k+1}^n c_i\ket{v_i}\implies \ket{v}\in \langle S_1 \rangle \implies W^\perp \subseteq \langle S_1 \rangle .\] \noindent Por otro lado, como $S_1 \subseteq W^\perp$ tenemos que $\langle S_1 \rangle \subseteq W^\perp$, por lo que $\langle S_1 \rangle = W^\perp.$ Por lo tanto, $S_1$ es base ortonormal de $W^\perp$. 
            \item Sea $W$ un subespacio vectorial de $V$. Como $V$ es de dimensión finita, entonces $W$ también lo es. Por ende, $W$ tiene una base ortonormal $\{\ket{v_1},\ket{v_2},...,\ket{v_k}\}.$ Por los incisos a) y b), tenemos que \[
                    \text{dim}(V) = n = k + (n-k) = \text{dim}(W) + \text{dim}(W^\perp).\]  
        \end{enumerate}
    \end{proof}
\end{teorema}

\newpage
\subsection{Proyecciones ortogonales***}


\begin{tcolorbox}
\begin{center}
    \textbf{Nota aclaratoria: \emph{Sobre bras, kets, y bra-kets...}}
\end{center}

%EXPLICAR LA DIFERENCIA SUTIL ENTRE UNIR UN BRA Y UN KET (APLICARLE UN FUNCIONAL A UN VECTOR) Y HACER EL PRODUCTO ESCALAR ENTRE DOS KETS (UTILIZANDO UN BRA-KET).

\hspace{3mm} Como hemos visto en esta sección, en un espacio vectorial $(V,K)$ los \emph{kets} $\ket{x},\ket{y},\ket{z}$, etc. son simplemente vectores de $V$, mientras que los \emph{bras} $\bra{x},\bra{y},\bra{z}$, etc. son funcionales sobre $V$, es decir, transformaciones lineales que van de $V$ a $K$.

\vspace{5mm}
\hspace{3mm} A pesar de que exista la correspondencia entre kets $\alpha\ket{x}\in V$ y bras $\overline{\alpha}\bra{x}\in V^*$ descrita en la sección \ref{Subsec:Correspondencia_entre_bras_y_kets}, debemos remarcar una diferencia sutil pero importante entre la operación de aplicarle un funcional a un vector y la operación de obtener el producto escalar entre dos vectores antes de pasar a la siguiente sección.

\vspace{5mm}
\hspace{3mm} Si $\ket{x}\in V$ y $\bra{y}\in V^*$ entonces, como $\bra{y}:V\to K$, tenemos que $$\bra{y}(\ket{x}) = \braket{y|x},$$ donde $\bra{y}$ es el vector dual correspondiente al vector $\ket{y}$. La observación crucial es esta: del lado izquierdo de la ecuación estamos aplicándole el funcional $\bra{y}$ al vector $\ket{x}$, mientras que del lado derecho estamos obteniendo el producto escalar entre el vector $\ket{x}$ (en la entrada lineal) y el vector $\ket{y}$ (en la entrada antilineal)\footnote{Inclusive l@s más atrevid@s entre ustedes se podrían animar a agregar una equivalencia más a la ecuación anterior: $\braket{y|x}=(\bra{y})\ket{x}$; es decir, podrían entretener la idea de que $\ket{x}$ sea ahora un \emph{funcional} que actúa sobre los vectores duales $\bra{y}\in V^*$ por la derecha; esto está relacionado con el hecho de que, si $V$ es un espacio de dimensión finita, entonces $(V^*)^*$ es isomorfo a $V$ \textemdash aunque no lo demostraremos en este curso.}; ambas operaciones involucran objetos distintos \textemdash la primera se realiza entre un funcional ($\bra{y}$) y un ket ($\ket{x}$), mientras que la segunda se realiza entre dos kets ($\ket{x}$ y $\ket{y}$)\textemdash\hspace{1mm} y, sin embargo, dan el mismo escalar como resultado.

\vspace{5mm}

\end{tcolorbox}


\subsection{Ejercicios de repaso} \label{}

\subsubsection{Funcionales y espacio dual} \label{Ejer:Funcionales_y_espacio_dual}
\begin{enumerate}
    \item
\end{enumerate}

\subsubsection{Correspondencia entre bras y kets} \label{Ejer:Correspondencia_entre_bras_y_kets}
\begin{enumerate}
    \item Sea $(V,K)$ un espacio vectorial con producto escalar. Demuestra que el conjunto de todos los bras correspondientes a los vectores de $V$ forma un subespacio vectorial de $V^*$.
    \item Demuestra que el subespacio de $V^*$ obtenido en el ejercicio anterior es isomorfo a $V$. En otras palabras, demuestra que la correspondencia entre bras y kets dada en la sección \ref{Subsec:Correspondencia_entre_bras_y_kets} es un isomorfismo.
\end{enumerate}

\subsubsection{Complemento ortogonal} \label{Ejer:Complemento_ortogonal}
\begin{enumerate}
    \item Demuestra que para todo subespacio vectorial $W$ de un espacio vectorial $V$ de dimensión finita $n$ y con producto escalar, $V$ es igual a una suma directa de $W$ y $W^{\perp}.$ ¿Por qué es importante que $V$ tenga dimensión finita? (Nota: tendrás que demostrar las observaciones hechas al principio de la sec. \ref{Subsec:Complemento_ortogonal}.)
\end{enumerate}

\subsubsection{Proyecciones ortogonales***} \label{Ejer:Proyecciones_ortogonales}
\begin{enumerate}
    \item
\end{enumerate}

\newpage
\section{Descomposición espectral y operadores adjuntos} \label{Sec:14} 

\subsection{Descomposición espectral (introducción)***}

Supongamos que tenemos un espacio vectorial $(V,K)$ de dimensión finita con producto escalar y un operador lineal diagonalizable $T:V\to V$ tal que sus eigenvectores forman una base \emph{ortogonal} de $V$; es decir, que se puede elegir a una base ortogonal de $V$ compuesta por eigenvectores de nuestro operador $T$. 

Empezando nuestra búsqueda por hipótesis más precisas que deban cumplir operadores lineales sobre espacios vectoriales de dimensión finita con producto escalar para que les podamos aplicar la descomposición espectral, nuestra primera pista se encuentra en los operadores adjuntos, que definiremos y estudiaremos en la siguiente sección.

\subsection{Operadores adjuntos}

Como vimos en la sección \ref{Subsec:Correspondencia_entre_bras_y_kets}, en cualquier espacio vectorial $(V,K)$ con producto escalar podemos tomar cualquier vector $\ket{y}\in V$ y definir un funcional $\textbf{g}:V\to K$ como \[
    \textbf{g}(\ket{x})=\braket{y|x}
,\] que podemos escribir simplemente como el bra $\bra{y}$.

Lo interesante de este tipo de funcionales es que, en espacios vectoriales de dimensión finita, todas las transformaciones lineales de $V$ a $K$ son de esta forma; es decir, podemos hacer una identificación entre las transformaciones lineales $\textbf{g}\in\mathcal{L}(V,K)$ y los bras $\bra{y}$, como demostraremos a continuación.

\begin{teorema} {14.1}
    Sea $(V,K)$ un espacio vectorial de dimensión finita con producto escalar y sea $\textbf{g}:V\to K$ una transformación lineal. Entonces existe un único $\bra{y}\in V^*$ tal que $\textbf{g}(\ket{x})=\braket{y|x}$ para todo $\ket{x}\in V$.

    \begin{proof}
        Sea $\beta=\{\ket{b_1},\ket{b_2},...,\ket{b_n}\}$ una base ortonormal de $V$ y sea \[
            \ket{y}=\sum_{i=1}^n \overline{\textbf{g}(\ket{b_i})}\ket{b_i} 
            .\] Entonces el vector dual correspondiente a $\ket{y}\in V$ es el bra $\bra{y}\in V^*$ dado por \[
            \bra{y}=\sum_{i=1}^n\textbf{g}(\ket{b_i})\bra{b_i}
        .\]  

        Por otro lado, definamos a $\textbf{h}:V\to K$ como $\textbf{h} (\ket{x})=\braket{y|x}$ para todo $\ket{x}\in V$. Claramente, $\textbf{h}$ es una transformación lineal. Observemos que para todos los elementos de la base $\beta$, sus imágenes bajo $\textbf{g}$ y $\textbf{h}$ coinciden; es decir, para todo $1\leq j\leq n$ tenemos que \[
            \textbf{h}(\ket{b_j})=\braket{y|b_j}=\bigg(\sum_{i=1}^n \textbf{g}(\ket{b_i})\bra{b_i}\bigg)\ket{b_j}=\sum_{i=1}^n \textbf{g}(\ket{b_i}) \braket{b_i|b_j}=\sum_{i=1}^n \textbf{g}(\ket{b_i})\delta_{ij}=\textbf{g}(\ket{b_j})
        .\] 

        Dado que $\textbf{g}$ y $\textbf{h}$ coinciden para todos los elementos de la base y son transformaciones lineales, entonces coinciden para todos los vectores $\ket{v}\in V$. Por ende, $\textbf{g}=\textbf{h}$. 

        Para demostrar la unicidad de $\bra{y}$, supongamos que $\textbf{g}(\ket{x})=\braket{y'|x}$ para todo $\ket{x}\in V$. Entonces tenemos que $$\braket{y|x}=\braket{y'|x} \implies \braket{y|x}-\braket{y'|x}=0 \implies \big(\bra{y}-\bra{y'}\big)\ket{x}=0$$ para todo $\ket{x}\in V$, pero esto se cumple si y sólo si $$\bra{y}-\bra{y'}=\bra{0}\implies \bra{y}=\bra{y'}.$$

    \end{proof}

\end{teorema}

Antes de continuar, recordamos la siguiente definición.

\hspace{5mm} 
\begin{tcolorbox}
    \underline{Def.} Decimos que una transformación lineal es un \emph{operador lineal} cuando su dominio y contradominio son el mismo espacio vectorial. Por lo tanto, si decimos que $T$ es un operador lineal \textemdash o, simplemente, operador\textemdash \ sobre $V$, nos referimos a una transformación lineal $T:V\to V$.
\end{tcolorbox}

\begin{teorema}{14.2}
    Sea $(V,K)$ un espacio vectorial de dimensión finita con producto escalar y sea $T:V\to V$ un operador lineal. Entonces existe una única función $T^*:V\to V$ tal que $\braket{y|T(x)}=\braket{T^*(y)|x}$ para toda $\ket{x},\ket{y}\in V$. Además, $T^*$ es lineal.
    
    \begin{proof}
        Sea $\ket{y}\in V$. Definimos a $\textbf{g}:V\to K$ como $\textbf{g}(\ket{x})=\braket{y|T(x)}$ para todo $\ket{x}\in V.$ Primero, verificamos que $\textbf{g}$ es lineal: sean $\ket{x_1},\ket{x_2}\in V$ y $c\in K$, entonces $$\textbf{g}(c\ket{x_1}+\ket{x_2})=\braket{y|T(cx_1+x_2)}=\braket{y|cT(x_1)+T(x_2)}=\braket{y|cT(x_1)}+\braket{y|T(x_2)}$$ $$=c\braket{y|T(x_1)}+\braket{y|T(x_2)}=c\textbf{g}(\ket{x_1})+\textbf{g}(\ket{x_2}),$$ por lo que $T$ es lineal.
        
        Luego, aplicamos el Teorema 14.1 para obtener un vector único $\ket{y'}\in V$ tal que $\textbf{g}(\ket{x})=\braket{y'|x}$ para todo $\ket{x}\in V.$ Observemos que, por definición de $\textbf{g}$, $\braket{y|T(x)}=\braket{y'|x}$ para todo $\ket{x}\in V$. Definiendo a $T^*:V\to V$ como $T^*(\ket{y})=\ket{y'}$ tenemos que $\braket{y|T(x)}=\braket{T^*(y)|x}$, como se deseaba. Ahora, debemos demostrar que $T^*$ es lineal: sean $\ket{y_1},\ket{y_2}\in V$ y $c\in K$, entonces para todo $\ket{x}\in V$ se cumple que $$\braket{T^*(cy_1+y_2)|x}=\braket{cy_1+y_2|T(x)}=\overline{c}\braket{y_1|T(x)}+\braket{y_2|T(x)}$$ $$=\overline{c}\braket{T^*(y_1)|x}+\braket{T^*(y_2)|x}=\braket{cT^*(y_1)+T^*(y_2)|x}$$ $$\implies\braket{T^*(cy_1+y_2)|x}-\braket{cT^*(y_1)+T(y_2)|x}=0$$ $$\implies \braket{T^*(cy_1+y_2)-(cT^*(y_1)+T^*(y_2))|x}=0.$$ Como esto vale para todo $\ket{x}\in V$, entonces esto implica que $$ \ket{T^*(cy_1+y_2)-(cT^*(y_1)+T(y_2))}=\ket{0}$$ $$\implies T^*(\ket{cy_1+y_2})=cT^*(\ket{y_1})+T^*(\ket{y_2}).$$
        
        Finalmente, para ver que $T^*$ es única: sea $U:V\to V$ un operador lineal tal que $\braket{y|T(x)}=\braket{U(y)|x}$ para todo $\ket{x},\ket{y}\in V$, entonces $\braket{T^*(y)|x}=\braket{U(y)|x}$ para todo $\ket{x},\ket{y}\in V$, por lo que $U=T^*$.
    \end{proof}
\end{teorema}

Como veremos dentro de poco, el operador $T^*$ descrito en el Teorema 14.2 es de gran importancia en los espacios vectoriales con producto escalar, por lo cual le damos un nombre especial.

\hspace{0.5mm}
\begin{tcolorbox} \label{Def:Operador_adjunto}
\underline{Def.} Sea $V$ un espacio vectorial de dimensión finita y $T^*:V\to V$ el operador lineal único que para todo $\ket{x},\ket{y}\in V$ satisface la relación $$\braket{y|T(x)}=\braket{T^*(y)|x},$$ donde $T:V\to V$ es un operador lineal. Entonces, decimos que $T^*$ es el \emph{operador adjunto} de $T$.

\vspace{5mm}
\hspace{3mm} En el caso en que $V$ sea un espacio vectorial de dimensión infinita y para un operador lineal $T:V\to V$ exista otro operador lineal $T^*:V\to V$ tal que la relación anterior se cumpla, entonces nuevamente diremos que $T^*$ es el operador adjunto de $T$ \textemdash sin embargo, en este tipo de espacios vectoriales la existencia de $T^*$ no está asegurada para todo operador lineal $T$.
\end{tcolorbox}
\vspace{5mm}

Ya que varios de los teoremas que demostraremos a continuación aplican tanto para espacios vectoriales de dimensión finita como aquellos de dimensión infinita, de ahora en adelante, cuando hagamos referencia a un operador adjunto en un espacio vectorial de dimensión infinita, asumiremos implícitamente su existencia.

A continuación, demostraremos que en espacios vectoriales de dimensión finita podemos obtener la representación matricial de un operador adjunto a partir de la representación matricial del operador original de una forma muy sencilla, siempre y cuando utilicemos una base ortonormal para representar ambas matrices.

% PODRÍA METER A_ij=<b_i,T(b_j)> con beta una base ortonormal como un Teorema anterior al actual Teorema 14.3

\begin{teorema} {14.3}
Sea $(V,K)$ un espacio vectorial de dimensión finita $n$ con producto escalar, $\beta=\{\ket{b_1},\ket{b_2},...,\ket{b_n}\}$ una base ortonormal de $V$ y $T:V\to V$ un operador lineal. Entonces $$[T^*]_\beta=\big([T]_\beta\big)^*,$$ donde $A^*$ se define mediante la relación $A^*_{ij}=\overline{A_{ji}}$ para toda $A\in M_{n\times n}(K)$.
\begin{proof}
Sean $A=[T]_\beta$ y $B=[T^*]_\beta$. Si partimos del sistema de ecuaciones que define a la representación matricial $A$ y hacemos producto escalar por la izquierda en ambos lados de cada una de las ecuaciones por un elemento $\ket{b_i}$ de la base y aplicamos ortonormalidad, obtenemos la relación $$A_{ij}=\braket{b_i,T(b_j)}.$$ 

Haciendo un procedimiento análogo con la representación matricial $B$, obtenemos que $$B_{ij}=\braket{b_i,T^*(b_j)},$$ de donde se sigue que $$B_{ij}=\overline{\braket{T^*(b_j),b_i}}=\overline{\braket{b_j|T(b_i)}}=\overline{A_{ji}},$$ lo cual implica que $B=A^*$, es decir, que $[T^*]_\beta=\big([T]_\beta\big)^*$.
\end{proof}
\end{teorema}

A la luz del Teorema 14.3, damos una nueva definición, que quizá ya esperes.

\vspace{5mm}
\begin{tcolorbox}
\underline{Def.} Sea $K$ un campo y $A\in M_{n\times n}(K)$. Decimos que $A^*\in M_{n\times n}(K)$, definida a través de la relación $$A^*_{ij}=\overline{A_{ji}},$$ es la \emph{matriz adjunta} de $A$.
\end{tcolorbox}
\vspace{5mm}

\noindent Recordando que para matrices cuadradas $A$ la matriz transpuesta $A^T$ se define mediante la relación $$A^T_{ij}=A_{ji},$$ podemos ver que la matriz adjunta $A^*$ se obtiene mediante la transposición de la matriz $A$ y la conjugación de sus entradas, sin importar el orden en que se realicen estas dos operaciones.

\subsection{Ejercicios de repaso}
\begin{enumerate}
    \item Sea $V$ un espacio vectorial de dimensión finita $n$ con producto escalar y $\beta=\{\ket{b_1},\ket{b_2},...,\ket{b_n}\}$ una base ortonormal de $V$. Demuestra que si $T:V\to V$ es lineal, entonces $([T]_\beta)_{ij}=\braket{b_i|T(b_j)}.$
    \item Sea $K$ un campo y $A\in M_{n\times n}(K)$. Demuestra que
    $A^*=\overline{A^T}=(\overline{A})^T$ y, por lo tanto, que $\text{det}(A^*)=\overline{\text{det}(A)}$.
    \item Sea $(V,K)$ un espacio vectorial con producto escalar y sean $T$ y $U$ operadores lineales sobre $V$ para los cuales existen operadores adjuntos $T^*$ y $U^*$, respectivamente. Demuestra las siguientes propiedades:
        \begin{enumerate}[label=\alph*)]
            \item $(T+U)^*=T^*+U^*$;
            \item $(cT)^*=\overline{c}T^*$ para todo $c\in K$;
            \item $(TU)^*=U^*T^*$;
            \item $(T^*)^*=T$;
            \item $I^*=I$.
        \end{enumerate}
    \item Sea $K$ un campo y sean $A,B\in M_{n\times n}(K)$. Demuestra las siguientes propiedades:
        \begin{enumerate}[label=\alph*)]
            \item $(A+B)^*=A^*+B^*$;
            \item $(cA)^*=\overline{c}A^*$ para todo $c\in K$;
            \item $(AB)^*=B^*A^*$;
            \item $(A^*)^*=A$;
            \item $I^*=I$.
        \end{enumerate}
\end{enumerate}

\newpage
\section{Operadores normales y autoadjuntos y teorema espectral} \label{Sec:15} 

\subsection{Operadores normales y autoadjuntos***}

\subsection{Teorema espectral}

Concluida nuestra búsqueda por hipótesis precisas necesarias para llevar a cabo la descomposición espectral, enunciamos a continuación un teorema que recopila y sintetiza muchos resultados que hemos demostrado a lo largo de este último módulo y que utiliza una inmensa cantidad de conceptos que hemos aprendido durante todo el curso. La demostración de este teorema se deja como ejercicio.

\begin{teorema} {15.algo (Teorema espectral)}
Sea $(V,K)$ un espacio vectorial de dimensión finita con producto escalar y $T:V\to V$ un operador lineal con eigenvalores distintos $\lambda_1,\lambda_2,...,\lambda_k$. Supongamos que $T$ es normal si $K=\mathbb{C}$ y que $T$ es autoadjunto si $K=\mathbb{R}$. Sea $W_i$ el eigenespacio de $T$ correspondiente al eigenvalor $\lambda_i$, con $1\leq i\leq k$, y sea $T_i$ la proyección ortogonal de $V$ sobre $W_i$. Entonces, se cumple que:
\begin{enumerate}[label=\alph*)]
    \item $V = W_1 \oplus W_2 \oplus ... \oplus W_k$;
    \item si $W_i'=\oplus_{j\neq i}W_j$, entonces $W_i^\perp=W_i'$;
    \item $T_i T_j = \delta_{ij}T_i$ para $1\leq i,j\leq k$;
    \item $I = T_1+T_2+...+T_k$;
    \item $T = \lambda_1 T_1 + \lambda_2 T_2 + ... + \lambda_k T_k$.
\end{enumerate}

%\begin{proof}
%\begin{enumerate}[label=\alph*)]
 %   \item Por los teoremas 6.16 y 6.17 sabemos que $T$ es diagonalizable. Entonces, para cada eigenespacio $W_i$ podemos elegir una base ordenada $\beta_i$ tal que la unión $\beta_1\cup\beta_2\cup...\cup\beta_k$ sea una base ordenada de $V$; en otras palabras, $W_1+W_2+...+W_k=V$. Por otra parte, ya que, por definición de eigenespacios, $W_i\cap W_j=\{\mathbf{0}\}$ si $i\neq j$, tenemos que $$V = W_1 \oplus W_2 \oplus ... \oplus W_k.$$
    %\item 
%\end{enumerate}
%\end{proof}

\end{teorema}

\subsection{Ejercicios de repaso}

\subsubsection{Operadores normales y autoadjuntos}
\begin{enumerate}
    \item
\end{enumerate}

\subsubsection{Teorema espectral}

\begin{enumerate}
    \item Demuestra el teorema espectral.
\end{enumerate}


\newpage
\section{Operadores ortogonales y unitarios***} \label{Sec:16} 

\subsection{Operadores ortogonales y unitarios}



\subsection{Ejercicios de repaso}

\subsubsection{Operadores lineales ortogonales y unitarios}


\subsubsection{Proyecciones ortogonales}





\end{document}
